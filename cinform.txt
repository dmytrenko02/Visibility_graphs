2276
010       Information, disturbance and Hamiltonian quantum feedback control
021       We consider separating the problem of designing Hamiltonian quantum feedback control algorithms into a measurement (estimation) strategy and a feedback (control) strategy, and consider optimizing desirable properties of each under the minimal constraint that the available strength of both is limited. 
020       This motivates concepts of information extraction and disturbance which are distinct from those usually considered in quantum information theory. Using these concepts we identify an information trade-off in quantum feedback control. 
007       rubr.quant-ph
007       name.Andrew C. Doherty
007       name.Kurt Jacobs
007       name.Gerard Jungman
040       2000.06.03 16:56
050       http://arxiv.org/abs/quant-ph/0006013
*** 3307
010       Information extraction and quantum state distortions in continuous   variable quantum teleportation
021       We analyze the loss of fidelity in continuous variable teleportation due to non-maximal entanglement. 
020       It is shown that the quantum state distortions correspond to the measurement back-action of a field amplitude measurement. Results for coherent states and for photon number states are presented. 
007       rubr.quant-ph
007       name.Holger F. Hofmann
007       name.Toshiki Ide
007       name.Takayoshi Kobayashi
007       name.Akira Furusawa
040       2001.10.22 06:21
050       http://arxiv.org/abs/quant-ph/0110127
*** 2520
010       The Quantum Universe
021       On the basis that the universe is a closed quantum system with no external observers, we propose a paradigm in which the universe jumps through a series of stages. 
020       Each stage is defined by a quantum state, an information content, and rules governing temporal evolution. Only some of these rules are currently understood; we can calculate answers to quantum questions, but we do not know why those questions have been asked in the first place. In this paradigm, time is synonymous with the quantum process of information extraction, rather than a label associated with a temporal dimension. We discuss the implications for cosmology. 
007       rubr.quant-ph
007       rubr.gr-qc
007       name.Jon Eakins
007       name.George Jaroszkiewicz
040       2002.03.05 17:21
050       http://arxiv.org/abs/quant-ph/0203020
*** 2628
010       Causality in quantum teleportation: information extraction and noise   effects in entanglement distribution
021       Quantum teleportation is possible because entanglement allows a definition of precise correlations between the non-commuting properties of a local system and corresponding non-commuting properties of a remote system. 
020       In this paper, the exact causality achieved by maximal entanglement is analyzed and the results are applied to the transfer of effects acting on the entanglement distribution channels to the teleported output state. In particular, it is shown how measurements performed on the entangled system distributed to the sender provide information on the teleported state while transferring the corresponding back-action to the teleported quantum state. 
007       rubr.quant-ph
007       name.Holger F. Hofmann
040       2002.05.15 06:44
050       http://arxiv.org/abs/quant-ph/0203128
*** 3178
010       Implementation of generalized quantum measurements: superadditive   quantum coding, accessible information extraction, and classical capacity   limit
021       Quantum information theory predicts that when the transmission resource is doubled in quantum channels, the amount of information transmitted can be increased more than twice by quantum channel coding technique, whereas the increase is at most twice in classical information theory. 
020       This remarkable feature, the superadditive quantum coding gain, can be implemented by appropriate choices of code words and corresponding quantum decoding which requires a collective quantum measurement. Recently, the first experimental demonstration was reported [Phys. Rev. Lett. 90, 167906 (2003)]. The purpose of this paper is to describe our experiment in detail. Particularly, a design strategy of quantum collective decoding in physical quantum circuits is emphasized. We also address the practical implication of the gain on communication performance by introducing the quantum-classical hybrid coding scheme. We show how the superadditive quantum coding gain, even in a small code length, can boost the communication performance of conventional coding technique. 
007       rubr.quant-ph
007       name.M. Takeoka
007       name.M. Fujiwara
007       name.J. Mizuno
007       name.M. Sasaki
040       2004.06.07 22:40
050       http://arxiv.org/abs/quant-ph/0306034
*** 1346
010       Data Management and Mining in Astrophysical Databases
021       We analyse the issues involved in the management and mining of astrophysical data. 
020       The traditional approach to data management in the astrophysical field is not able to keep up with the increasing size of the data gathered by modern detectors. An essential role in the astrophysical research will be assumed by automatic tools for information extraction from large datasets, i.e. data mining techniques, such as clustering and classification algorithms. This asks for an approach to data management based on data warehousing, emphasizing the efficiency and simplicity of data access; efficiency is obtained using multidimensional access methods and simplicity is achieved by properly handling metadata. Clustering and classification techniques, on large datasets, pose additional requirements: computational and memory scalability with respect to the data size, interpretability and objectivity of clustering or classification results. In this study we address some possible solutions. 
007       rubr.cs
007       rubr.cs.DB
007       rubr.astro-ph
007       rubr.physics.data-an
007       name.M. Frailis
007       name.A. De Angelis
007       name.V. Roberto
040       2003.07.16 12:49
050       http://arxiv.org/abs/cs/0307032
*** 3302
010       Measurement and Information Extraction in Complex Dynamics Quantum   Computation
021       We address the problem related to the extraction of the information in the simulation of complex dynamics quantum computation. 
020       Here we present an example where important information can be extracted efficiently by means of quantum simulations. We show how to extract efficiently the localization length, the mean square deviation and the system characteristic frequency. We show how this methods work on a dynamical model, the Sawtooth Map, that is characterized by very different dynamical regimes: from near integrable to fully developed chaos; it also exhibits quantum dynamical localization. 
007       rubr.quant-ph
007       name.G.Casati
007       name.S.Montangero
040       2003.07.23 16:54
050       http://arxiv.org/abs/quant-ph/0307165
*** 1186
010       Controlled hierarchical filtering: Model of neocortical sensory   processing
021       A model of sensory information processing is presented. 
020       The model assumes that learning of internal (hidden) generative models, which can predict the future and evaluate the precision of that prediction, is of central importance for information extraction. Furthermore, the model makes a bridge to goal-oriented systems and builds upon the structural similarity between the architecture of a robust controller and that of the hippocampal entorhinal loop. This generative control architecture is mapped to the neocortex and to the hippocampal entorhinal loop. Implicit memory phenomena; priming and prototype learning are emerging features of the model. Mathematical theorems ensure stability and attractive learning properties of the architecture. Connections to reinforcement learning are also established: both the control network, and the network with a hidden model converge to (near) optimal policy under suitable conditions. Falsifying predictions, including the role of the feedback connections between neocortical areas are made. 
007       rubr.cs
007       rubr.cs.NE
007       rubr.cs.AI
007       rubr.cs.LG
007       rubr.q-bio.NC
007       name.Andras Lorincz
040       2003.08.16 07:31
050       http://arxiv.org/abs/cs/0308025
*** 1261
010       Dictionary based methods for information extraction
021       In this paper we present a general method for information extraction that exploits the features of data compression techniques. 
020       We first define and focus our attention on the so-called "dictionary" of a sequence. Dictionaries are intrinsically interesting and a study of their features can be of great usefulness to investigate the properties of the sequences they have been extracted from (e.g. DNA strings). We then describe a procedure of string comparison between dictionary-created sequences (or "artificial texts") that gives very good results in several contexts. We finally present some results on self-consistent classification problems. 
007       rubr.cond-mat
007       rubr.cond-mat.stat-mech
007       rubr.cond-mat.other
007       rubr.cs.IR
007       rubr.q-bio.GN
007       rubr.q-bio.OT
007       name.A. Baronchelli
007       name.E. Caglioti
007       name.V. Loreto
007       name.E. Pizzi
040       2004.09.14 10:54
050       http://arxiv.org/abs/cond-mat/0402581
*** 1370
010       Perspects in astrophysical databases
021       Astrophysics has become a domain extremely rich of scientific data. 
020       Data mining tools are needed for information extraction from such large datasets. This asks for an approach to data management emphasizing the efficiency and simplicity of data access; efficiency is obtained using multidimensional access methods and simplicity is achieved by properly handling metadata. Moreover, clustering and classification techniques on large datasets pose additional requirements in terms of computation and memory scalability and interpretability of results. In this study we review some possible solutions. 
007       rubr.cs
007       rubr.cs.DB
007       rubr.astro-ph
007       name.M. Frailis
007       name.A. De Angelis
007       name.V. Roberto
040       2004.02.09 19:13
050       http://arxiv.org/abs/cs/0402016
*** 3029
010       Quantum computing and information extraction for a dynamical quantum   system
021       We discuss the simulation of a complex dynamical system, the so-called quantum sawtooth map model, on a quantum computer. 
020       We show that a quantum computer can be used to efficiently extract relevant physical information for this model. It is possible to simulate the dynamical localization of classical chaos and extract the localization length of the system with quadratic speed up with respect to any known classical computation. We can also compute with algebraic speed up the diffusion coefficient and the diffusion exponent both in the regimes of Brownian and anomalous diffusion. Finally, we show that it is possible to extract the fidelity of the quantum motion, which measures the stability of the system under perturbations, with exponential speed up. 
007       rubr.quant-ph
007       rubr.cond-mat.other
007       rubr.nlin.CD
007       name.Giuliano Benenti
007       name.Giulio Casati
007       name.Simone Montangero
040       2004.02.02 15:12
050       http://arxiv.org/abs/quant-ph/0402010
*** 956
010       Artificial Sequences and Complexity Measures
021       In this paper we exploit concepts of information theory to address the fundamental problem of identifying and defining the most suitable tools to extract, in a automatic and agnostic way, information from a generic string of characters. 
020       We introduce in particular a class of methods which use in a crucial way data compression techniques in order to define a measure of remoteness and distance between pairs of sequences of characters (e.g. texts) based on their relative information content. We also discuss in detail how specific features of data compression techniques could be used to introduce the notion of dictionary of a given sequence and of Artificial Text and we show how these new tools can be used for information extraction purposes. We point out the versatility and generality of our method that applies to any kind of corpora of character strings independently of the type of coding behind them. We consider as a case study linguistic motivated problems and we present results for automatic language recognition, authorship attribution and self consistent-classification. 
007       rubr.cond-mat
007       rubr.cond-mat.stat-mech
007       rubr.cs.CL
007       rubr.cs.IR
007       rubr.cs.IT
007       name.Andrea Baronchelli
007       name.Emanuele Caglioti
007       name.Vittorio Loreto
040       2006.01.25 14:13
050       http://arxiv.org/abs/cond-mat/0403233
*** 1399
010       Incorporating information from source simulations into searches for   gravitational-wave bursts
021       The detection of gravitational waves from astrophysical sources of gravitational waves is a realistic goal for the current generation of interferometric gravitational-wave detectors. 
020       Short duration bursts of gravitational waves from core-collapse supernovae or mergers of binary black holes may bring a wealth of astronomical and astrophysical information. The weakness of the waves and the rarity of the events urges the development of optimal methods to detect the waves. The waves from these sources are not generally known well enough to use matched filtering however; this drives the need to develop new ways to exploit source simulation information in both detections and information extraction. We present an algorithmic approach to using catalogs of gravitational-wave signals developed through numerical simulation, or otherwise, to enhance our ability to detect these waves. As more detailed simulations become available, it is straightforward to incorporate the new information into the search method. This approach may also be useful when trying to extract information from a gravitational-wave observation by allowing direct comparison between the observation and simulations. 
007       rubr.gr-qc
007       name.Patrick R Brady
007       name.Saikat Ray-Majumder
040       2004.05.06 21:00
050       http://arxiv.org/abs/gr-qc/0405036
*** 3247
010       Predicting protein functions with message passing algorithms
021       Motivation: In the last few years a growing interest in biology has been shifting towards the problem of optimal information extraction from the huge amount of data generated via large scale and high-throughput techniques. 
020       One of the most relevant issues has recently become that of correctly and reliably predicting the functions of observed but still functionally undetermined proteins starting from information coming from the network of co-observed proteins of known functions. <br />Method: The method proposed in this article is based on a message passing algorithm known as Belief Propagation, which takes as input the network of proteins physical interactions and a catalog of known proteins functions, and returns the probabilities for each unclassified protein of having one chosen function. The implementation of the algorithm allows for fast on-line analysis, and can be easily generalized to more complex graph topologies taking into account hyper-graphs, {\em i.e.} complexes of more than two interacting proteins. 
007       rubr.q-bio
007       rubr.q-bio.QM
007       rubr.cond-mat.dis-nn
007       name.M. Leone
007       name.A. Pagnani
040       2004.05.07 11:05
050       http://arxiv.org/abs/q-bio/0405007
*** 547
010       Data Mining in Gamma Astrophysics Experiments
021       Data mining techniques, including clustering and classification tasks, for the automatic information extraction from large datasets are increasingly demanded in several scientific fields. 
020       In particular, in the astrophysical field, large archives and digital sky surveys with dimensions of 10E12 bytes currently exist, while in the near future they will reach sizes of the order of 10E15. In this work we propose a multidimensional indexing method to efficiently query and mine large astrophysical datasets. A novelty detection algorithm, based on the Support Vector Clustering and using density and neighborhood information stored in the index structure, is proposed to find regions of interest in data characterized by isotropic noise. We show an application of this method for the detection of point sources from a gamma-ray photon list. 
007       rubr.astro-ph
007       name.M. Frailis
007       name.A. De Angelis
007       name.V. Roberto
040       2005.03.24 15:23
050       http://arxiv.org/abs/astro-ph/0503543
*** 3939
010       Optical storage of high density information beyond the diffraction limit   : a quantum study
021       We propose an optical read-out scheme allowing a demonstration of principle of information extraction below the diffraction limit. 
020       This technique, which could lead to improvement in data read-out density onto optical discs, is independent from the wavelength and numerical aperture of the reading apparatus, and involves a multi-pixel array detector. Furthermore, we show how to use non classical light in order to perform bit discrimination beyond the quantum noise limit. 
007       rubr.quant-ph
007       name.Vincent Delaubert
007       name.Nicolas Treps
007       name.Gao Bo
007       name.Claude Fabre
040       2005.12.19 08:03
050       http://arxiv.org/abs/quant-ph/0512152
*** 3445
010       Asymptotic adaptive bipartite entanglement distillation protocol
021       We present a new asymptotic bipartite entanglement distillation protocol that outperforms all existing asymptotic schemes. 
020       This protocol is based on the breeding protocol with the incorporation of two-way classical communication. Like breeding, the protocol starts with an infinite number of copies of a Bell-diagonal mixed state. Breeding can be carried out as successive stages of partial information extraction, yielding the same result: one bit of information is gained at the cost (measurement) of one pure Bell state pair (ebit). The basic principle of our protocol is at every stage to replace measurements on ebits by measurements on a finite number of copies, whenever there are two equiprobable outcomes. In that case, the entropy of the global state is reduced by more than one bit. Therefore, every such replacement results in an improvement of the protocol. We explain how our protocol is organized as to have as many replacements as possible. The yield is then calculated for Werner states. 
007       rubr.quant-ph
007       name.Erik Hostens
007       name.Jeroen Dehaene
007       name.Bart De Moor
040       2006.05.17 12:27
050       http://arxiv.org/abs/quant-ph/0602205
*** 3586
010       Measuring complexity with zippers
021       Physics concepts have often been borrowed and independently developed by other fields of science. 
020       In this perspective a significant example is that of entropy in Information Theory. The aim of this paper is to provide a short and pedagogical introduction to the use of data compression techniques for the estimate of entropy and other relevant quantities in Information Theory and Algorithmic Information Theory. We consider in particular the LZ77 algorithm as case study and discuss how a zipper can be used for information extraction. 
007       rubr.physics
007       rubr.physics.ed-ph
007       rubr.physics.comp-ph
007       name.Andrea Baronchelli
007       name.Emanuele Caglioti
007       name.Vittorio Loreto
040       2006.05.03 13:41
050       http://arxiv.org/abs/physics/0605031
*** 3909
010       Information Extraction from Decohering Systems via Indirect Continuous   Measurement
021       In this article we discuss a general information extraction scheme to gain knowledge of the state and the amount of decoherence based on indirect continuous measurement. 
020       The purpose of this information extraction is to determine the feedback in order to control decoherence based on an ``output equation", described by a bilinear form. An interacting bilinear form of control system is used instead of master equation to analyze the dynamic properties of the system. The analysis of continuous measurement on a decohering quantum system has not been extensively studied before. 
007       rubr.quant-ph
007       name.Narayan Ganesan
007       name.Tzyh-Jong Tarn
040       2006.12.06 05:36
050       http://arxiv.org/abs/quant-ph/0605044
*** 4978
010       Information-Disturbance Tradeoff in Quantum State Discrimination
021       When discriminating between two pure quantum states, there exists a quantitative tradeoff between the information retrieved by the measurement and the disturbance caused on the unknown state. 
020       We derive the optimal tradeoff and provide the corresponding quantum measurement. Such an optimal measurement smoothly interpolates between the two limiting cases of maximal information extraction and no measurement at all. 
007       rubr.quant-ph
007       name.Francesco Buscemi
007       name.Massimiliano F. Sacchi
040       2006.11.15 00:52
050       http://arxiv.org/abs/quant-ph/0610196
*** 4375
010       A Robust Linguistic Platform for Efficient and Domain specific Web   Content Analysis
021       Web semantic access in specific domains calls for specialized search engines with enhanced semantic querying and indexing capacities, which pertain both to information retrieval (IR) and to information extraction (IE). 
020       A rich linguistic analysis is required either to identify the relevant semantic units to index and weight them according to linguistic specific statistical distribution, or as the basis of an information extraction process. Recent developments make Natural Language Processing (NLP) techniques reliable enough to process large collections of documents and to enrich them with semantic annotations. This paper focuses on the design and the development of a text processing platform, Ogmios, which has been developed in the ALVIS project. The Ogmios platform exploits existing NLP modules and resources, which may be tuned to specific domains and produces linguistically annotated documents. We show how the three constraints of genericity, domain semantic awareness and performance can be handled all together. 
007       rubr.cs.AI
007       rubr.cs
007       name.Thierry Hamon
007       name.Adeline Nazarenko
007       name.Thierry Poibeau
007       name.Sophie Aubin
007       name.Julien Derivi&#xe8;re
040       2007.06.29 08:58
050       http://arxiv.org/abs/0706.4375
*** 694
010       Reconstruction of Protein-Protein Interaction Pathways by Mining   Subject-Verb-Objects Intermediates
021       The exponential increase in publication rate of new articles is limiting access of researchers to relevant literature. 
020       This has prompted the use of text mining tools to extract key biological information. Previous studies have reported extensive modification of existing generic text processors to process biological text. However, this requirement for modification had not been examined. In this study, we have constructed Muscorian, using MontyLingua, a generic text processor. It uses a two-layered generalization-specialization paradigm previously proposed where text was generically processed to a suitable intermediate format before domain-specific data extraction techniques are applied at the specialization layer. Evaluation using a corpus and experts indicated 86-90% precision and approximately 30% recall in extracting protein-protein interactions, which was comparable to previous studies using either specialized biological text processing tools or modified existing tools. Our study had also demonstrated the flexibility of the two-layered generalization-specialization paradigm by using the same generalization layer for two specialized information extraction tasks. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       rubr.cs.CL
007       rubr.cs.DL
007       name.Maurice HT Ling
007       name.Christophe Lefevre
007       name.Kevin R. Nicholas
007       name.Feng Lin
040       2007.08.06 01:22
050       http://arxiv.org/abs/0708.0694
*** 2889
010       An efficient reduction of ranking to classification
021       This paper describes an efficient reduction of the learning problem of ranking to binary classification. 
020       The reduction guarantees an average pairwise misranking regret of at most that of the binary classifier regret, improving a recent result of Balcan et al which only guarantees a factor of 2. Moreover, our reduction applies to a broader class of ranking loss functions, admits a simpler proof, and the expected running time complexity of our algorithm in terms of number of calls to a classifier or preference function is improved from $\Omega(n^2)$ to $O(n \log n)$. In addition, when the top $k$ ranked elements only are required ($k \ll n$), as in many applications in information extraction or search engines, the time complexity of our algorithm can be further reduced to $O(k \log k + n)$. Our reduction and algorithm are thus practical for realistic applications where the number of points to rank exceeds several thousands. Much of our results also extend beyond the bipartite case previously studied. <br />Our rediction is a randomized one. To complement our result, we also derive lower bounds on any deterministic reduction from binary (preference) classification to ranking, implying that our use of a randomized reduction is essentially necessary for the guarantees we provide. 
007       rubr.cs.LG
007       rubr.cs
007       rubr.cs.IR
007       rubr.cs.IR
007       name.Nir Ailon
007       name.Mehryar Mohri
040       2007.12.07 00:02
050       http://arxiv.org/abs/0710.2889
*** 1465
010       3-Way Composition of Weighted Finite-State Transducers
021       Composition of weighted transducers is a fundamental algorithm used in many applications, including for computing complex edit-distances between automata, or string kernels in machine learning, or to combine different components of a speech recognition, speech synthesis, or information extraction system. 
020       We present a generalization of the composition of weighted transducers, 3-way composition, which is dramatically faster in practice than the standard composition algorithm when combining more than two transducers. The worst-case complexity of our algorithm for composing three transducers $T_1$, $T_2$, and $T_3$ resulting in $T$, \ignore{depending on the strategy used, is $O(|T|_Q d(T_1) d(T_3) + |T|_E)$ or $(|T|_Q d(T_2) + |T|_E)$,} is $O(|T|_Q \min(d(T_1) d(T_3), d(T_2)) + |T|_E)$, where $|\cdot|_Q$ denotes the number of states, $|\cdot|_E$ the number of transitions, and $d(\cdot)$ the maximum out-degree. As in regular composition, the use of perfect hashing requires a pre-processing step with linear-time expected complexity in the size of the input transducers. In many cases, this approach significantly improves on the complexity of standard composition. Our algorithm also leads to a dramatically faster composition in practice. Furthermore, standard composition can be obtained as a special case of our algorithm. We report the results of several experiments demonstrating this improvement. These theoretical and empirical improvements significantly enhance performance in the applications already mentioned. 
007       rubr.cs.CC
007       rubr.cs
007       name.Cyril Allauzen
007       name.Mehryar Mohri
040       2008.02.22 18:02
050       http://arxiv.org/abs/0802.1465
*** 317
010       Parts-of-Speech Tagger Errors Do Not Necessarily Degrade Accuracy in   Extracting Information from Biomedical Text
021       A recent study reported development of Muscorian, a generic text processing tool for extracting protein-protein interactions from text that achieved comparable performance to biomedical-specific text processing tools. 
020       This result was unexpected since potential errors from a series of text analysis processes is likely to adversely affect the outcome of the entire process. Most biomedical entity relationship extraction tools have used biomedical-specific parts-of-speech (POS) tagger as errors in POS tagging and are likely to affect subsequent semantic analysis of the text, such as shallow parsing. This study aims to evaluate the parts-of-speech (POS) tagging accuracy and attempts to explore whether a comparable performance is obtained when a generic POS tagger, MontyTagger, was used in place of MedPost, a tagger trained in biomedical text. Our results demonstrated that MontyTagger, Muscorian's POS tagger, has a POS tagging accuracy of 83.1% when tested on biomedical text. Replacing MontyTagger with MedPost did not result in a significant improvement in entity relationship extraction from text; precision of 55.6% from MontyTagger versus 56.8% from MedPost on directional relationships and 86.1% from MontyTagger compared to 81.8% from MedPost on nondirectional relationships. This is unexpected as the potential for poor POS tagging by MontyTagger is likely to affect the outcome of the information extraction. An analysis of POS tagging errors demonstrated that 78.5% of tagging errors are being compensated by shallow parsing. Thus, despite 83.1% tagging accuracy, MontyTagger has a functional tagging accuracy of 94.6%. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       rubr.cs.IR
007       name.Maurice HT Ling
007       name.Christophe Lefevre
007       name.Kevin R. Nicholas
040       2008.04.02 09:34
050       http://arxiv.org/abs/0804.0317
*** 93
010       Prescription for Cosmic Information Extraction from Multiple Sky Maps
021       This paper presents a prescription for distilling the information contained in the cosmic microwave background radiation from multiple sky maps that also contain both instrument noise and foreground contaminants. 
020       The prescription is well-suited for cosmological parameter estimation and accounts for uncertainties in the cosmic microwave background extraction scheme. The technique is computationally viable at low resolution and may be considered a natural and significant generalization of the "Internal Linear Combination" approach to foreground removal. An important potential application is the analysis of the multi-frequency temperature and polarization data from the forthcoming Planck satellite. 
007       rubr.astro-ph
007       name.Steven Gratton
040       2008.05.01 16:36
050       http://arxiv.org/abs/0805.0093
*** 2983
010       On Probability Distributions for Trees: Representations, Inference and   Learning
021       We study probability distributions over free algebras of trees. 
020       Probability distributions can be seen as particular (formal power) tree series [Berstel et al 82, Esik et al 03], i.e. mappings from trees to a semiring K . A widely studied class of tree series is the class of rational (or recognizable) tree series which can be defined either in an algebraic way or by means of multiplicity tree automata. We argue that the algebraic representation is very convenient to model probability distributions over a free algebra of trees. First, as in the string case, the algebraic representation allows to design learning algorithms for the whole class of probability distributions defined by rational tree series. Note that learning algorithms for rational tree series correspond to learning algorithms for weighted tree automata where both the structure and the weights are learned. Second, the algebraic representation can be easily extended to deal with unranked trees (like XML trees where a symbol may have an unbounded number of children). Both properties are particularly relevant for applications: nondeterministic automata are required for the inference problem to be relevant (recall that Hidden Markov Models are equivalent to nondeterministic string automata); nowadays applications for Web Information Extraction, Web Services and document processing consider unranked trees. 
007       rubr.cs.LG
007       rubr.cs
007       name.Fran&#xe7;ois Denis
007       name.Amaury Habrard
007       name.R&#xe9;mi Gilleron
007       name.Marc Tommasi
007       name.&#xc9;douard Gilbert
040       2008.07.18 14:41
050       http://arxiv.org/abs/0807.2983
*** 944
010       Improving Quantum State Estimation with Mutually Unbiased Bases
021       When used in quantum state estimation, projections onto mutually unbiased bases have the ability to maximize information extraction per measurement and to minimize redundancy. 
020       We present the first experimental demonstration of quantum state tomography of two-qubit polarization states to take advantage of mutually unbiased bases. We demonstrate improved state estimation as compared to standard measurement strategies and discuss how this can be understood from the structure of the measurements we use. We experimentally compared our method to the standard state estimation method for three different states and observe that the infidelity was up to $1.84\pm0.06$ times lower using our technique than it was using standard state estimation methods. 
007       rubr.quant-ph
007       name.Robert B. A. Adamson
007       name.Aephraim M. Steinberg
040       2008.12.02 23:48
050       http://arxiv.org/abs/0808.0944
*** 4530
010       Mining Meaning from Wikipedia
021       Wikipedia is a goldmine of information; not just for its many readers, but also for the growing community of researchers who recognize it as a resource of exceptional scale and utility. 
020       It represents a vast investment of manual effort and judgment: a huge, constantly evolving tapestry of concepts and relations that is being applied to a host of tasks. <br />This article provides a comprehensive description of this work. It focuses on research that extracts and makes use of the concepts, relations, facts and descriptions found in Wikipedia, and organizes the work into four broad categories: applying Wikipedia to natural language processing; using it to facilitate information retrieval and information extraction; and as a resource for ontology building. The article addresses how Wikipedia is being used as is, how it is being improved and adapted, and how it is being combined with other structures to create entirely new resources. We identify the research groups and individuals involved, and how their work has developed in the last few years. We provide a comprehensive list of the open-source software they have produced. 
007       rubr.cs.AI
007       rubr.cs
007       rubr.cs.CL
007       rubr.cs.CL
007       rubr.cs.IR
007       name.Olena Medelyan
007       name.David Milne
007       name.Catherine Legg
007       name.Ian H. Witten
040       2009.05.10 01:51
050       http://arxiv.org/abs/0809.4530
*** 1732
010       Introduction to Searching with Regular Expressions
021       The explosive rate of information growth and availability often makes it increasingly difficult to locate information pertinent to your needs. 
020       These problems are often compounded when keyword based search methodologies are not adequate for describing the information you seek. In many instances, information such as Web site URLs, phone numbers, etc. can often be better identified through the use of a textual pattern than by keyword. For example, many more phone numbers could be picked up by a search for the pattern (XXX) XXX-XXXX, where X could be any digit, than would be by a search for any specific phone number (i.e. the keyword approach). Programming languages typically allow for the matching of textual patterns via the usage of regular expressions. This tutorial will provide an introduction to the basics of programming regular expressions as well as provide an introduction to how regular expressions can be applied to data processing tasks such as information extraction and search refinement. 
007       rubr.cs.IR
007       rubr.cs
007       name.Christopher M. Frenz
040       2008.10.09 19:57
050       http://arxiv.org/abs/0810.1732
*** 2141
010       Analysis of Discrete Signals with Stochastic Components using Flicker   Noise Spectroscopy
021       The problem of information extraction from discrete stochastic time series, produced with some finite sampling frequency, using flicker-noise spectroscopy, a general framework for information extraction based on the analysis of the correlation links between signal irregularities and formulated for continuous signals, is discussed. 
020       It is shown that the mathematical notions of Dirac and Heaviside functions used in the analysis of continuous signals may be interpreted as high-frequency and low-frequency stochastic components, respectively, in the case of discrete series. The analysis of electroencephalogram measurements for a teenager with schizophrenic symptoms at two different sampling frequencies demonstrates that the "power spectrum" and difference moment contain different information in the case of discrete signals, which was formally proven for continuous signals. The sampling interval itself is suggested as an additional parameter that should be included in general parameterization procedures for real signals. 
007       rubr.physics.data-an
007       rubr.physics
007       rubr.physics.med-ph
007       name.Serge F. Timashev
007       name.Yuriy S. Polyakov
040       2008.12.11 13:22
050       http://arxiv.org/abs/0812.2141
*** 213
010       Filtering Microarray Correlations by Statistical Literature Analysis   Yields Potential Hypotheses for Lactation Research
021       Our results demonstrated that a previously reported protein name co-occurrence method (5-mention PubGene) which was not based on a hypothesis testing framework, it is generally statistically more significant than the 99th percentile of Poisson distribution-based method of calculating co-occurrence. 
020       It agrees with previous methods using natural language processing to extract protein-protein interaction from text as more than 96% of the interactions found by natural language processing methods to overlap with the results from 5-mention PubGene method. However, less than 2% of the gene co-expressions analyzed by microarray were found from direct co-occurrence or interaction information extraction from the literature. At the same time, combining microarray and literature analyses, we derive a novel set of 7 potential functional protein-protein interactions that had not been previously described in the literature. 
007       rubr.cs.DL
007       rubr.cs
007       rubr.cs.DB
007       name.Maurice HT Ling
007       name.Christophe Lefevre
007       name.Kevin R. Nicholas
040       2009.01.02 03:47
050       http://arxiv.org/abs/0901.0213
*** 2266
010       PCA Tomography: how to extract information from datacubes
021       Astronomy has evolved almost exclusively by the use of spectroscopic and imaging techniques, operated separately. 
020       With the development of modern technologies it is possible to obtain datacubes in which one combines both techniques simultaneously, producing images with spectral resolution. To extract information from them can be quite complex, and hence the development of new methods of data analysis is desirable. We present a method of analysis of datacube (data from single field observations, containing two spatial and one spectral dimension) that uses PCA (Principal Component Analysis) to express the data in the form of reduced dimensionality, facilitating efficient information extraction from very large data sets. PCA transforms the system of correlated coordinates into a system of uncorrelated coordinates ordered by principal components of decreasing variance. The new coordinates are referred to as eigenvectors, and the projections of the data onto these coordinates produce images we will call tomograms. The association of the tomograms (images) to eigenvectors (spectra) is important for the interpretation of both. The eigenvectors are mutually orthogonal and this information is fundamental for their handling and interpretation. When the datacube shows objects that present uncorrelated physical phenomena, the eigenvector's orthogonality may be instrumental in separating and identifying them. By handling eigenvectors and tomograms one can enhance features, extract noise, compress data, extract spectra, etc. We applied the method, for illustration purpose only, to the central region of the LINER galaxy NGC 4736, and demonstrate that it has a type 1 active nucleus, not known before. Furthermore we show that it is displaced from the centre of its stellar bulge. 
007       rubr.astro-ph.IM
007       rubr.astro-ph
007       name.J. E. Steiner
007       name.R. B. Menezes
007       name.T.V. Ricci
007       name.A. S. Oliveira
040       2009.01.15 13:54
050       http://arxiv.org/abs/0901.2266
*** 589
010       Generalized Collective Inference with Symmetric Clique Potentials
021       Collective graphical models exploit inter-instance associative dependence to output more accurate labelings. 
020       However existing models support very limited kind of associativity which restricts accuracy gains. This paper makes two major contributions. First, we propose a general collective inference framework that biases data instances to agree on a set of {\em properties} of their labelings. Agreement is encouraged through symmetric clique potentials. We show that rich properties leads to bigger gains, and present a systematic inference procedure for a large class of such properties. The procedure performs message passing on the cluster graph, where property-aware messages are computed with cluster specific algorithms. This provides an inference-only solution for domain adaptation. Our experiments on bibliographic information extraction illustrate significant test error reduction over unseen domains. Our second major contribution consists of algorithms for computing outgoing messages from clique clusters with symmetric clique potentials. Our algorithms are exact for arbitrary symmetric potentials on binary labels and for max-like and majority-like potentials on multiple labels. For majority potentials, we also provide an efficient Lagrangian Relaxation based algorithm that compares favorably with the exact algorithm. We present a 13/15-approximation algorithm for the NP-hard Potts potential, with runtime sub-quadratic in the clique size. In contrast, the best known previous guarantee for graphs with Potts potentials is only 1/2. We empirically show that our method for Potts potentials is an order of magnitude faster than the best alternatives, and our Lagrangian Relaxation based algorithm for majority potentials beats the best applicable heuristic -- ICM. 
007       rubr.cs.AI
007       rubr.cs
007       name.Rahul Gupta
007       name.Sunita Sarawagi
007       name.Ajit A. Diwan
040       2009.07.07 13:31
050       http://arxiv.org/abs/0907.0589
*** 1632
010       Incorporating Integrity Constraints in Uncertain Databases
021       We develop an approach to incorporate additional knowledge, in the form of general purpose integrity constraints (ICs), to reduce uncertainty in probabilistic databases. 
020       While incorporating ICs improves data quality (and hence quality of answers to a query), it significantly complicates query processing. To overcome the additional complexity, we develop an approach to map an uncertain relation U with ICs to another uncertain relation U', that approximates the set of consistent worlds represented by U. Queries over U can instead be evaluated over U' achieving higher quality (due to reduced uncertainty in U') without additional complexity in query processing due to ICs. We demonstrate the effectiveness and scalability of our approach to large data-sets with complex constraints. We also present experimental results demonstrating the utility of incorporating integrity constraints in uncertain relations, in the context of an information extraction application. 
007       rubr.cs.DB
007       rubr.cs
007       rubr.cs.IR
007       rubr.cs.IR
007       name.Naveen Ashish
007       name.Sharad Mehrotra
007       name.Pouria Pirzadeh
040       2009.07.09 18:45
050       http://arxiv.org/abs/0907.1632
*** 1786
010       DBMSs Should Talk Back Too
021       Natural language user interfaces to database systems have been studied for several decades now. 
020       They have mainly focused on parsing and interpreting natural language queries to generate them in a formal database language. We envision the reverse functionality, where the system would be able to take the internal result of that translation, say in SQL form, translate it back into natural language, and show it to the initiator of the query for verification. Likewise, information extraction has received considerable attention in the past ten years or so, identifying structured information in free text so that it may then be stored appropriately and queried. Validation of the records stored with a backward translation into text would again be very powerful. Verification and validation of query and data input of a database system correspond to just one example of the many important applications that would benefit greatly from having mature techniques for translating such database constructs into free-flowing text. The problem appears to be deceivingly simple, as there are no ambiguities or other complications in interpreting internal database elements, so initially a straightforward translation appears adequate. Reality teaches us quite the opposite, however, as the resulting text should be expressive, i.e., accurate in capturing the underlying queries or data, and effective, i.e., allowing fast and unique interpretation of them. Achieving both of these qualities is very difficult and raises several technical challenges that need to be addressed. In this paper, we first expose the reader to several situations and applications that need translation into natural language, thereby, motivating the problem. We then outline, by example, the research problems that need to be solved, separately for data translations and query translations. 
007       rubr.cs.DB
007       rubr.cs
007       rubr.cs.HC
007       rubr.cs.HC
007       name.Alkis Simitsis
007       name.Yannis Ioannidis
040       2009.09.09 18:10
050       http://arxiv.org/abs/0909.1786
*** 1386
010       Machine Learning: When and Where the Horses Went Astray?
021       Machine Learning is usually defined as a subfield of AI, which is busy with information extraction from raw data sets. 
020       Despite of its common acceptance and widespread recognition, this definition is wrong and groundless. Meaningful information does not belong to the data that bear it. It belongs to the observers of the data and it is a shared agreement and a convention among them. Therefore, this private information cannot be extracted from the data by any means. Therefore, all further attempts of Machine Learning apologists to justify their funny business are inappropriate. 
007       rubr.cs.AI
007       rubr.cs
007       rubr.cs.LG
007       rubr.cs.LG
007       name.Emanuel Diamant
040       2009.11.07 02:52
050       http://arxiv.org/abs/0911.1386
*** 2311
010       VirusPKT: A Search Tool For Assimilating Assorted Acquaintance For   Viruses
021       Viruses utilize various means to circumvent the immune detection in the biological systems. 
020       Several mathematical models have been investigated for the description of viral dynamics in the biological system of human and various other species. One common strategy for evasion and recognition of viruses is, through acquaintance in the systems by means of search engines. In this perspective a search tool have been developed to provide a wider comprehension about the structure and other details on viruses which have been narrated in this paper. This provides an adequate knowledge in evolution and building of viruses, its functions through information extraction from various websites. Apart from this, tool aim to automate the activities associated with it in a self-maintainable, self-sustainable, proactive one which has been evaluated through analysis made and have been discussed in this paper. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.q-bio
007       rubr.q-bio.OT
007       rubr.q-bio.OT
007       name.Jayanthi Manicassamy
007       name.P. Dhavachelvan
040       2009.12.11 18:45
050       http://arxiv.org/abs/0912.2311
*** 4892
010       Janus: Automatic Ontology Builder from XSD Files
021       The construction of a reference ontology for a large domain still remains an hard human task. 
020       The process is sometimes assisted by software tools that facilitate the information extraction from a textual corpus. Despite of the great use of XML Schema files on the internet and especially in the B2B domain, tools that offer a complete semantic analysis of XML schemas are really rare. In this paper we introduce Janus, a tool for automatically building a reference knowledge base starting from XML Schema files. Janus also provides different useful views to simplify B2B application integration. 
007       rubr.cs.DB
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.AI
007       name.Ivan Bedini
007       name.Benjamin Nguyen
007       name.Georges Gardarin
040       2010.01.27 10:38
050       http://arxiv.org/abs/1001.4892
*** 5274
010       PCA Tomography and its application to nearby galactic nuclei
021       With the development of modern technologies such as IFUs, it is possible to obtain data cubes in which one produces images with spectral resolution. 
020       To extract information from them can be quite complex, and hence the development of new methods of data analysis is desirable. We briefly describe a method of analysis of data cubes (data from single field observations, containing two spatial and one spectral dimension) that uses Principal Component Analysis (PCA) to express the data in the form of reduced dimensionality, facilitating efficient information extraction from very large data sets. We applied the method, for illustration purpose, to the central region of the low ionization nuclear emission region (LINER) galaxy NGC 4736, and demonstrate that it has a type 1 active nucleus, not known before. Furthermore, we show that it is displaced from the centre of its stellar bulge. 
007       rubr.astro-ph.GA
007       rubr.astro-ph
007       rubr.astro-ph.IM
007       rubr.astro-ph.IM
007       name.J. E. Steiner
007       name.R. B. Menezes
007       name.T. V. Ricci
007       name.A. S. Oliveira
040       2010.01.28 21:30
050       http://arxiv.org/abs/1001.5274
*** 2677
010       Classified Ads Harvesting Agent and Notification System
021       The shift from an information society to a knowledge society require rapid information harvesting, reliable search and instantaneous on demand delivery. 
020       Information extraction agents are used to explore and collect data available from Web, in order to effectively exploit such data for business purposes, such as automatic news filtering, advertisement or product searching and price comparing. In this paper, we develop a real-time automatic harvesting agent for adverts posted on Servihoo web portal and an SMS-based notification system. It uses the URL of the web portal and the object model, i.e., the fields of interests and a set of rules written using the HTML parsing functions to extract latest adverts information. The extraction engine executes the extraction rules and stores the information in a database to be processed for automatic notification. This intelligent system helps to tremendously save time. It also enables users or potential product buyers to react more quickly to changes and newly posted sales adverts, paving the way to real-time best buy deals. 
007       rubr.cs.IR
007       name.Razvi Doomun
007       name.Lollmahamod N.
007       name.Auleear Nadeem
007       name.Mozafar Aukin
040       2010.03.13 05:32
050       http://arxiv.org/abs/1003.2677
*** 48
010       Anonimos: An LP based Approach for Anonymizing Weighted Social Network   Graphs
021       The increasing popularity of social networks has initiated a fertile research area in information extraction and data mining. 
020       Anonymization of these social graphs is important to facilitate publishing these data sets for analysis by external entities. Prior work has concentrated mostly on node identity anonymization and structural anonymization. But with the growing interest in analyzing social networks as a weighted network, edge weight anonymization is also gaining importance. We present An\'onimos, a Linear Programming based technique for anonymization of edge weights that preserves linear properties of graphs. Such properties form the foundation of many important graph-theoretic algorithms such as shortest paths problem, k-nearest neighbors, minimum cost spanning tree, and maximizing information spread. As a proof of concept, we apply An\'onimos to the shortest paths problem and its extensions, prove the correctness, analyze complexity, and experimentally evaluate it using real social network data sets. Our experiments demonstrate that An\'onimos anonymizes the weights, improves k-anonymity of the weights, and also scrambles the relative ordering of the edges sorted by weights, thereby providing robust and effective anonymization of the sensitive edge-weights. Additionally, we demonstrate the composability of different models generated using An\'onimos, a property that allows a single anonymized graph to preserve multiple linear properties. 
007       rubr.cs.DB
007       name.Sudipto Das
007       name.Omer Egecioglu
007       name.Amr El Abbadi
040       2010.04.01 03:39
050       http://arxiv.org/abs/1004.0048
*** 1614
010       PROBER: Ad-Hoc Debugging of Extraction and Integration Pipelines
021       Complex information extraction (IE) pipelines assembled by plumbing together off-the-shelf operators, specially customized operators, and operators re-used from other text processing pipelines are becoming an integral component of most text processing frameworks. 
020       A critical task faced by the IE pipeline user is to run a post-mortem analysis on the output. Due to the diverse nature of extraction operators (often implemented by independent groups), it is time consuming and error-prone to describe operator semantics formally or operationally to a provenance system. We introduce the first system that helps IE users analyze pipeline semantics and infer provenance interactively while debugging. This allows the effort to be proportional to the need, and to focus on the portions of the pipeline under the greatest suspicion. We present a generic debugger for running post-execution analysis of any IE pipeline consisting of arbitrary types of operators. We propose an effective provenance model for IE pipelines which captures a variety of operator types, ranging from those for which full or no specifications are available. We present a suite of algorithms to effectively build provenance and facilitate debugging. Finally, we present an extensive experimental study on large-scale real-world extractions from an index of ~500 million Web documents. 
007       rubr.cs.DB
007       name.Anish Das Sarma
007       name.Alpa Jain
007       name.Philip Bohannon
040       2010.04.09 17:33
050       http://arxiv.org/abs/1004.1614
*** 4462
010       BiLingual Information Retrieval System for English and Tamil
021       This paper addresses the design and implementation of BiLingual Information Retrieval system on the domain, Festivals. 
020       A generic platform is built for BiLingual Information retrieval which can be extended to any foreign or Indian language working with the same efficiency. Search for the solution of the query is not done in a specific predefined set of standard languages but is chosen dynamically on processing the user's query. This paper deals with Indian language Tamil apart from English. The task is to retrieve the solution for the user given query in the same language as that of the query. In this process, a Ontological tree is built for the domain in such a way that there are entries in the above listed two languages in every node of the tree. A Part-Of-Speech (POS) Tagger is used to determine the keywords from the given query. Based on the context, the keywords are translated to appropriate languages using the Ontological tree. A search is performed and documents are retrieved based on the keywords. With the use of the Ontological tree, Information Extraction is done. Finally, the solution for the query is translated back to the query language (if necessary) and produced to the user. 
007       rubr.cs.IR
007       name.S.Saraswathi
007       name.Asma Siddhiqaa.M
007       name.Kalaimagal.K
007       name.Kalaiyarasi.M
040       2010.04.26 10:08
050       http://arxiv.org/abs/1004.4462
*** 4464
010       Audio enabled information extraction system for cricket and hockey   domains
021       The proposed system aims at the retrieval of the summarized information from the documents collected from web based search engine as per the user query related to cricket and hockey domain. 
020       The system is designed in a manner that it takes the voice commands as keywords for search. The parts of speech in the query are extracted using the natural language extractor for English. Based on the keywords the search is categorized into 2 types: - 1.Concept wise - information retrieved to the query is retrieved based on the keywords and the concept words related to it. The retrieved information is summarized using the probabilistic approach and weighted means algorithm.2.Keyword search - extracts the result relevant to the query from the highly ranked document retrieved from the search by the search engine. The relevant search results are retrieved and then keywords are used for summarizing part. During summarization it follows the weighted and probabilistic approaches in order to identify the data comparable to the keywords extracted. The extracted information is then refined repeatedly through the aggregation process to reduce redundancy. Finally the resultant data is submitted to the user in the form of audio output. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.MM
007       rubr.cs.SD
007       name.S. Saraswathi
007       name.Narasimha Sravan. V
007       name.Sai Vamsi Krishna. B.V
007       name.Suresh Reddy. S
040       2010.04.26 10:11
050       http://arxiv.org/abs/1004.4464
*** 104
010       Joint Structured Models for Extraction from Overlapping Sources
021       We consider the problem of jointly training structured models for extraction from sources whose instances enjoy partial overlap. 
020       This has important applications like user-driven ad-hoc information extraction on the web. Such applications present new challenges in terms of the number of sources and their arbitrary pattern of overlap not seen by earlier collective training schemes applied on two sources. We present an agreement-based learning framework and alternatives within it to trade-off tractability, robustness to noise, and extent of agreement. We provide a principled scheme to discover low-noise agreement sets in unlabeled data across the sources. Through extensive experiments over 58 real datasets, we establish that our method of additively rewarding agreement over maximal segments of text provides the best trade-offs, and also scores over alternatives such as collective inference, staged training, and multi-view learning. 
007       rubr.cs.AI
007       name.Rahul Gupta
007       name.Sunita Sarawagi
040       2010.05.01 20:55
050       http://arxiv.org/abs/1005.0104
*** 844
010       General Cram\'er-Rao bound for parameter estimation using Gaussian   multimode quantum resources
021       Multimode Gaussian quantum light, including multimode squeezed and/or multipartite quadrature entangled light, is a very general and powerful quantum resource with promising applications to quantum information processing and metrology involving continuous variables. 
020       In this paper, we determine the ultimate sensitivity in the estimation of any parameter when the information about this parameter is encoded in such Gaussian light, irrespective of the exact information extraction protocol used in the estimation. We then show that, for a given set of available quantum resources, the most economical way to maximize the sensitivity is to put the most squeezed state available in a well-defined light mode. This implies that it is not possible to take advantage of the existence of squeezed fluctuations in other modes, nor of quantum correlations and entanglement between different modes. We show that an appropriate homodyne detection scheme allows us to reach this Cramr-Rao bound. We apply finally these considerations to the problem of optimal phase estimation using interferometric techniques. 
007       rubr.quant-ph
007       name.Olivier Pinel
007       name.Julien Fade
007       name.Nicolas Treps
007       name.Claude Fabre
040       2010.08.04 19:06
050       http://arxiv.org/abs/1008.0844
*** 1333
010       An Agent based Approach towards Metadata Extraction, Modelling and   Information Retrieval over the Web
021       Web development is a challenging research area for its creativity and complexity. 
020       The existing raised key challenge in web technology technologic development is the presentation of data in machine read and process able format to take advantage in knowledge based information extraction and maintenance. Currently it is not possible to search and extract optimized results using full text queries because there is no such mechanism exists which can fully extract the semantic from full text queries and then look for particular knowledge based information. 
007       rubr.cs.AI
007       name.Zeeshan Ahmed
007       name.Detlef Gerhard
040       2010.08.07 12:29
050       http://arxiv.org/abs/1008.1333
*** 460
010       Image formation in synthetic aperture radio telescopes
021       Next generation radio telescopes will be much larger, more sensitive, have much larger observation bandwidth and will be capable of pointing multiple beams simultaneously. 
020       Obtaining the sensitivity, resolution and dynamic range supported by the receivers requires the development of new signal processing techniques for array and atmospheric calibration as well as new imaging techniques that are both more accurate and computationally efficient since data volumes will be much larger. This paper provides a tutorial overview of existing image formation techniques and outlines some of the future directions needed for information extraction from future radio telescopes. We describe the imaging process from measurement equation until deconvolution, both as a Fourier inversion problem and as an array processing estimation problem. The latter formulation enables the development of more advanced techniques based on state of the art array processing. We demonstrate the techniques on simulated and measured radio telescope data. 
007       rubr.astro-ph.IM
007       rubr.astro-ph
007       name.Ronny Levanda
007       name.Amir Leshem
040       2010.09.02 16:10
050       http://arxiv.org/abs/1009.0460
*** 4623
010       Opinion Polarity Identification through Adjectives
021       "What other people think" has always been an important piece of information during various decision-making processes. 
020       Today people frequently make their opinions available via the Internet, and as a result, the Web has become an excellent source for gathering consumer opinions. There are now numerous Web resources containing such opinions, e.g., product reviews forums, discussion groups, and Blogs. But, due to the large amount of information and the wide range of sources, it is essentially impossible for a customer to read all of the reviews and make an informed decision on whether to purchase the product. It is also difficult for the manufacturer or seller of a product to accurately monitor customer opinions. For this reason, mining customer reviews, or opinion mining, has become an important issue for research in Web information extraction. One of the important topics in this research area is the identification of opinion polarity. The opinion polarity of a review is usually expressed with values 'positive', 'negative' or 'neutral'. We propose a technique for identifying polarity of reviews by identifying the polarity of the adjectives that appear in them. Our evaluation shows the technique can provide accuracy in the area of 73%, which is well above the 58%-64% provided by naive Bayesian classifiers. 
007       rubr.cs.CL
007       name.Samaneh Moghaddam
007       name.Fred Popowich
040       2010.11.21 00:15
050       http://arxiv.org/abs/1011.4623
*** 2406
010       Automatic Wrappers for Large Scale Web Extraction
021       We present a generic framework to make wrapper induction algorithms tolerant to noise in the training data. 
020       This enables us to learn wrappers in a completely unsupervised manner from automatically and cheaply obtained noisy training data, e.g., using dictionaries and regular expressions. By removing the site-level supervision that wrapper-based techniques require, we are able to perform information extraction at web-scale, with accuracy unattained with existing unsupervised extraction techniques. Our system is used in production at Yahoo! and powers live applications. 
007       rubr.cs.DB
007       name.Nilesh Dalvi
007       name.Ravi Kumar
007       name.Mohamed Soliman
040       2011.03.12 01:05
050       http://arxiv.org/abs/1103.2406
*** 3216
010       Tuffy: Scaling up Statistical Inference in Markov Logic Networks using   an RDBMS
021       Markov Logic Networks (MLNs) have emerged as a powerful framework that combines statistical and logical reasoning; they have been applied to many data intensive problems including information extraction, entity resolution, and text mining. 
020       Current implementations of MLNs do not scale to large real-world data sets, which is preventing their wide-spread adoption. We present Tuffy that achieves scalability via three novel contributions: (1) a bottom-up approach to grounding that allows us to leverage the full power of the relational optimizer, (2) a novel hybrid architecture that allows us to perform AI-style local search efficiently using an RDBMS, and (3) a theoretical insight that shows when one can (exponentially) improve the efficiency of stochastic local search. We leverage (3) to build novel partitioning, loading, and parallel algorithms. We show that our approach outperforms state-of-the-art implementations in both quality and speed on several publicly available datasets. 
007       rubr.cs.DB
007       name.Feng Niu
007       name.Christopher R&#xe9;
007       name.AnHai Doan
007       name.Jude Shavlik
040       2011.04.16 08:52
050       http://arxiv.org/abs/1104.3216
*** 2640
010       Ultimate sensitivity of precision measurements with Gaussian quantum   light : a multi-modal approach
021       Multimode Gaussian quantum light, which includes multimode squeezed and multipartite quadrature entangled light, is a very general and powerful quantum resource with promising applications in quantum information processing and metrology. 
020       In this paper, we determine the ultimate sensitivity in the estimation of any parameter when the information about this parameter is encoded in such light, irrespective of the information extraction protocol used in the estimation and of the measured observable. In addition we show that an appropriate homodyne detection scheme allows us to reach this ultimate sensitivity. We show that, for a given set of available quantum resources, the most economical way to maximize the sensitivity is to put the most squeezed state available in a well-de ned light mode. This implies that it is not possible to take advantage of the existence of squeezed fluctuations in other modes, nor of quantum correlations and entanglement between diff erent modes. 
007       rubr.quant-ph
007       name.Olivier Pinel
007       name.Julien Fade
007       name.Daniel Braun
007       name.Pu Jian
007       name.Nicolas Treps
007       name.Claude Fabre
040       2011.10.07 09:46
050       http://arxiv.org/abs/1105.2644
*** 5739
010       BioSimplify: an open source sentence simplification engine to improve   recall in automatic biomedical information extraction
021       BioSimplify is an open source tool written in Java that introduces and facilitates the use of a novel model for sentence simplification tuned for automatic discourse analysis and information extraction (as opposed to sentence simplification for improving human readability). 
020       The model is based on a "shot-gun" approach that produces many different (simpler) versions of the original sentence by combining variants of its constituent elements. This tool is optimized for processing biomedical scientific literature such as the abstracts indexed in PubMed. We tested our tool on its impact to the task of PPI extraction and it improved the f-score of the PPI tool by around 7%, with an improvement in recall of around 20%. The BioSimplify tool and test corpus can be downloaded from <a href="https://biosimplify.sourceforge.net.">this https URL</a> 
007       rubr.cs.CL
007       name.Siddhartha Jonnalagadda
007       name.Graciela Gonzalez
040       2011.07.28 15:40
050       http://arxiv.org/abs/1107.5744
*** 5747
010       An Effective Approach to Biomedical Information Extraction with Limited   Training Data
021       Overall, the two main contributions of this work include the application of sentence simplification to association extraction as described above, and the use of distributional semantics for concept extraction. 
020       The proposed work on concept extraction amalgamates for the first time two diverse research areas -distributional semantics and information extraction. This approach renders all the advantages offered in other semi-supervised machine learning systems, and, unlike other proposed semi-supervised approaches, it can be used on top of different basic frameworks and algorithms. <a href="http://gradworks.umi.com/34/49/3449837.html">this http URL</a> 
007       rubr.cs.CL
007       name.Siddhartha Jonnalagadda
040       2011.09.12 19:29
050       http://arxiv.org/abs/1107.5752
*** 5456
010       Personalized Web Services for Web Information Extraction
021       The field of information extraction from the Web emerged with the growth of the Web and the multiplication of online data sources. 
020       This paper is an analysis of information extraction methods. It presents a service oriented approach for web information extraction considering both web data management and extraction services. Then we propose an SOA based architecture to enhance flexibility and on-the-fly modification of web extraction services. An implementation of the proposed architecture is proposed on the middleware level of Java Enterprise Edition (JEE) servers. 
007       rubr.cs.IR
007       name.Zahi Jarir
007       name.Mohamed Quafafou
007       name.Mahammed Erradi
040       2011.08.27 15:49
050       http://arxiv.org/abs/1108.5460
*** 475
010       On the preservation of unitarity during black hole evolution and   information extraction from its interior
021       For more than 30 years the discovery that black holes radiate like black bodies of specific temperature has triggered a multitude of puzzling questions concerning their nature and the fate of information that goes down the black hole during its lifetime. 
020       The most tricky issue in what is known as information loss paradox is the apparent violation of unitarity during the formation/evaporation process of black holes. A new idea is proposed based on the combination of our knowledge on Hawking radiation as well as the Einstein-Podolsky-Rosen phenomenon, that could resolve the paradox and spare physicists from the unpalatable idea that unitarity can ultimately be violated even under special conditions. 
007       rubr.gr-qc
007       rubr.hep-th
007       rubr.quant-ph
007       name.Nikolaos D. Pappas
040       2012.05.22 14:41
050       http://arxiv.org/abs/1201.0475
*** 5073
010       Massively Increasing TIMEX3 Resources: A Transduction Approach
021       Automatic annotation of temporal expressions is a research challenge of great interest in the field of information extraction. 
020       Gold standard temporally-annotated resources are limited in size, which makes research using them difficult. Standards have also evolved over the past decade, so not all temporally annotated data is in the same format. We vastly increase available human-annotated temporal expression resources by converting older format resources to TimeML/TIMEX3. This task is difficult due to differing annotation methods. We present a robust conversion tool and a new, large temporal expression resource. Using this, we evaluate our conversion process by using it as training data for an existing TimeML annotation tool, achieving a 0.87 F1 measure -- better than any system in the TempEval-2 timex recognition exercise. 
007       rubr.cs.CL
007       name.Leon Derczynski
007       name.H&#xe9;ctor Llorens
007       name.Estela Saquete
040       2012.03.22 18:45
050       http://arxiv.org/abs/1203.5076
*** 6048
010       A new approach to the optimization of the extraction of astrometric and   photometric information from multi-wavelength images in cosmological fields
021       This paper describes a new approach to the optimization of information extraction in multi-wavelength image cubes of cosmological fields. 
020       The objective is to create a framework for the automatic identification and tagging of sources according to various criteria (isolated source, partially overlapped, fully overlapped, cross-matched, etc) and to set the basis for the automatic production of the SEDs (spectral energy distributions) for all objects detected in the many multi-wavelength images in cosmological fields.In order to do so, a processing pipeline is designed that combines Voronoi tessellation, Bayesian cross-matching, and active contours to create a graph-based representation of the cross-match probabilities. This pipeline produces a set of SEDs with quality tags suitable for the application of already-proven data mining methods. The pipeline briefly described here is also applicable to other astrophysical scenarios such as star forming regions. 
007       rubr.astro-ph.IM
007       rubr.astro-ph
007       rubr.astro-ph.GA
007       rubr.astro-ph.GA
007       name.Maria Jose Marquez
040       2012.08.09 21:53
050       http://arxiv.org/abs/1203.6052
*** 6402
010       An Analysis of Structured Data on the Web
021       In this paper, we analyze the nature and distribution of structured data on the Web. 
020       Web-scale information extraction, or the problem of creating structured tables using extraction from the entire web, is gathering lots of research interest. We perform a study to understand and quantify the value of Web-scale extraction, and how structured information is distributed amongst top aggregator websites and tail sites for various interesting domains. We believe this is the first study of its kind, and gives us new insights for information extraction over the Web. 
007       rubr.cs.DB
007       name.Nilesh Dalvi
007       name.Ashwin Machanavajjhala
007       name.Bo Pang
040       2012.03.29 00:07
050       http://arxiv.org/abs/1203.6406
*** 905
010       A Fuzzy Approach for Pertinent Information Extraction from Web Resources
021       Recent work in machine learning for information extraction has focused on two distinct sub-problems: the conventional problem of filling template slots from natural language text, and the problem of wrapper induction, learning simple extraction procedures ("wrappers") for highly structured text such as Web pages. 
020       For suitable regular domains, existing wrapper induction algorithms can efficiently learn wrappers that are simple and highly accurate, but the regularity bias of these algorithms makes them unsuitable for most conventional information extraction tasks. This paper describes a new approach for wrapping semistructured Web pages. The wrapper is capable of learning how to extract relevant information from Web resources on the basis of user supplied examples. It is based on inductive learning techniques as well as fuzzy logic rules. Experimental results show that our approach achieves noticeably better precision and recall coefficient performance measures than SoftMealy, which is one of the most recently reported wrappers capable of wrapping semi-structured Web pages with missing attributes, multiple attributes, variant attribute permutations, exceptions, and typos. 
007       rubr.cs.IR
007       name.Radhouane Boughamoura
007       name.Mohamed Nazih Omri
007       name.Habib Youssef
040       2012.06.05 12:36
050       http://arxiv.org/abs/1206.0905
*** 2010
010       Temporal expression normalisation in natural language texts
021       Automatic annotation of temporal expressions is a research challenge of great interest in the field of information extraction. 
020       In this report, I describe a novel rule-based architecture, built on top of a pre-existing system, which is able to normalise temporal expressions detected in English texts. Gold standard temporally-annotated resources are limited in size and this makes research difficult. The proposed system outperforms the state-of-the-art systems with respect to TempEval-2 Shared Task (value attribute) and achieves substantially better results with respect to the pre-existing system on top of which it has been developed. I will also introduce a new free corpus consisting of 2822 unique annotated temporal expressions. Both the corpus and the system are freely available on-line. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       name.Michele Filannino
040       2012.06.10 09:32
050       http://arxiv.org/abs/1206.2010
*** 246
010       Web Data Extraction, Applications and Techniques: A Survey
021       Web Data Extraction is an important problem that has been studied by means of different scientific tools and in a broad range of applications. 
020       Many approaches to extracting data from the Web have been designed to solve specific problems and operate in ad-hoc domains. Other approaches, instead, heavily reuse techniques and algorithms developed in the field of Information Extraction. <br />This survey aims at providing a structured and comprehensive overview of the literature in the field of Web Data Extraction. We provided a simple classification framework in which existing Web Data Extraction applications are grouped into two main classes, namely applications at the Enterprise level and at the Social Web level. At the Enterprise level, Web Data Extraction techniques emerge as a key tool to perform data analysis in Business and Competitive Intelligence systems as well as for business process re-engineering. At the Social Web level, Web Data Extraction techniques allow to gather a large amount of structured data continuously generated and disseminated by Web 2.0, Social Media and Online Social Network users and this offers unprecedented opportunities to analyze human behavior at a very large scale. We discuss also the potential of cross-fertilization, i.e., on the possibility of re-using Web Data Extraction techniques originally designed to work in a given domain, in other domains. 
007       rubr.cs.IR
007       name.Emilio Ferrara
007       name.Pasquale De Meo
007       name.Giacomo Fiumara
007       name.Robert Baumgartner
040       2014.06.10 03:58
050       http://arxiv.org/abs/1207.0246
*** 1406
010       A Conditional Random Field for Discriminatively-trained Finite-state   String Edit Distance
021       The need to measure sequence similarity arises in information extraction, object identity, data mining, biological sequence analysis, and other domains. 
020       This paper presents discriminative string-edit CRFs, a finitestate conditional random field model for edit sequences between strings. Conditional random fields have advantages over generative approaches to this problem, such as pair HMMs or the work of Ristad and Yianilos, because as conditionally-trained methods, they enable the use of complex, arbitrary actions and features of the input strings. As in generative models, the training data does not have to specify the edit sequences between the given string pairs. Unlike generative models, however, our model is trained on both positive and negative instances of string pairs. We present positive experimental results on several data sets. 
007       rubr.cs.LG
007       rubr.cs
007       rubr.cs.AI
007       name.Andrew McCallum
007       name.Kedar Bellare
007       name.Fernando Pereira
040       2012.07.04 16:20
050       http://arxiv.org/abs/1207.1406
*** 4157
010       An Integrated, Conditional Model of Information Extraction and   Coreference with Applications to Citation Matching
021       Although information extraction and coreference resolution appear together in many applications, most current systems perform them as ndependent steps. 
020       This paper describes an approach to integrated inference for extraction and coreference based on conditionally-trained undirected graphical models. We discuss the advantages of conditional probability training, and of a coreference model structure based on graph partitioning. On a data set of research paper citations, we show significant reduction in error by using extraction uncertainty to improve coreference citation matching accuracy, and using coreference to improve the accuracy of the extracted fields. 
007       rubr.cs.LG
007       rubr.cs
007       rubr.cs.DL
007       rubr.cs.DL
007       rubr.cs.IR
007       rubr.stat.ML
007       name.Ben Wellner
007       name.Andrew McCallum
007       name.Fuchun Peng
007       name.Michael Hay
040       2012.07.11 15:00
050       http://arxiv.org/abs/1207.4157
*** 5745
010       Semantic Information Retrieval Using Ontology In University Domain
021       Today's conventional search engines hardly do provide the essential content relevant to the user's search query. 
020       This is because the context and semantics of the request made by the user is not analyzed to the full extent. So here the need for a semantic web search arises. SWS is upcoming in the area of web search which combines Natural Language Processing and Artificial Intelligence. The objective of the work done here is to design, develop and implement a semantic search engine- SIEU(Semantic Information Extraction in University Domain) confined to the university domain. SIEU uses ontology as a knowledge base for the information retrieval process. It is not just a mere keyword search. It is one layer above what Google or any other search engines retrieve by analyzing just the keywords. Here the query is analyzed both syntactically and semantically. The developed system retrieves the web results more relevant to the user query through keyword expansion. The results obtained here will be accurate enough to satisfy the request made by the user. The level of accuracy will be enhanced since the query is analyzed semantically. The system will be of great use to the developers and researchers who work on web. The Google results are re-ranked and optimized for providing the relevant links. For ranking an algorithm has been applied which fetches more apt results for the user query. 
007       rubr.cs.IR
007       name.Swathi Rajasurya
007       name.Tamizhamudhu Muralidharan
007       name.Sandhiya Devi
007       name.S. Swamynathan
040       2012.07.24 16:51
050       http://arxiv.org/abs/1207.5745
*** 1826
010       A spatio-spectral hybridization for edge preservation and noisy image   restoration via local parametric mixtures and Lagrangian relaxation
021       This paper investigates a fully unsupervised statistical method for edge preserving image restoration and compression using a spatial decomposition scheme. 
020       Smoothed maximum likelihood is used for local estimation of edge pixels from mixture parametric models of local templates. For the complementary smooth part the traditional L2-variational problem is solved in the Fourier domain with Thin Plate Spline (TPS) regularization. It is well known that naive Fourier compression of the whole image fails to restore a piece-wise smooth noisy image satisfactorily due to Gibbs phenomenon. Images are interpreted as relative frequency histograms of samples from bi-variate densities where the sample sizes might be unknown. The set of discontinuities is assumed to be completely unsupervised Lebesgue-null, compact subset of the plane in the continuous formulation of the problem. Proposed spatial decomposition uses a widely used topological concept, partition of unity. The decision on edge pixel neighborhoods are made based on the multiple testing procedure of Holms. Statistical summary of the ?final output is decomposed into two layers of information extraction, one for the subset of edge pixels and the other for the smooth region. Robustness is also demonstrated by applying the technique on noisy degradation of clean images. 
007       rubr.stat.ME
007       rubr.cs
007       rubr.cs.CV
007       rubr.cs.CV
007       rubr.stat.AP
007       name.Kinjal Basu
007       name.Debapriya Sengupta
040       2012.09.09 18:23
050       http://arxiv.org/abs/1209.1826
*** 3497
010       Ontology Based Information Extraction for Disease Intelligence
021       Disease Intelligence (DI) is based on the acquisition and aggregation of fragmented knowledge of diseases at multiple sources all over the world to provide valuable information to doctors, researchers and information seeking community. 
020       Some diseases have their own characteristics changed rapidly at different places of the world and are reported on documents as unrelated and heterogeneous information which may be going unnoticed and may not be quickly available. This research presents an Ontology based theoretical framework in the context of medical intelligence and country/region. Ontology is designed for storing information about rapidly spreading and changing diseases with incorporating existing disease taxonomies to genetic information of both humans and infectious organisms. It further maps disease symptoms to diseases and drug effects to disease symptoms. The machine understandable disease ontology represented as a website thus allows the drug effects to be evaluated on disease symptoms and exposes genetic involvements in the human diseases. Infectious agents which have no known place in an existing classification but have data on genetics would still be identified as organisms through the intelligence of this system. It will further facilitate researchers on the subject to try out different solutions for curing diseases. 
007       rubr.cs.AI
007       rubr.cs
007       rubr.cs.DL
007       rubr.cs.IR
007       name.Prabath Chaminda Abeysiriwardana
007       name.Saluka R Kodituwakku
040       2012.11.15 05:33
050       http://arxiv.org/abs/1211.3497
*** 6470
010       A new class of SETI beacons that contain information (22-aug-2010)
021       In the cm-wavelength range, an extraterrestrial electromagnetic narrow band (sine wave) beacon is an excellent choice to get alien attention across interstellar distances because 1) it is not strongly affected by interstellar / interplanetary dispersion or scattering, and 2) searching for narrowband signals is computationally efficient (scales as Ns log(Ns) where Ns = number of voltage samples). 
020       Here we consider a special case wideband signal where two or more delayed copies of the same signal are transmitted over the same frequency and bandwidth, with the result that ISM dispersion and scattering cancel out during the detection stage. Such a signal is both a good beacon (easy to find) and carries arbitrarily large information rate (limited only by the atmospheric transparency to about 10 GHz). The discovery process uses an autocorrelation algorithm, and we outline a compute scheme where the beacon discovery search can be accomplished with only 2x the processing of a conventional sine wave search, and discuss signal to background response for sighting the beacon. Once the beacon is discovered, the focus turns to information extraction. Information extraction requires similar processing as for generic wideband signal searches, but since we have already identified the beacon, the efficiency of information extraction is negligible. 
007       rubr.astro-ph.IM
007       rubr.astro-ph
007       rubr.cs
007       rubr.cs.OH
007       rubr.cs.OH
007       name.G. R. Harp
007       name.R. F. Ackermann
007       name.Samantha K. Blair
007       name.J. Arbunich
007       name.P. R. Backus
007       name.J. C. Tarter
007       name.ATA Team
040       2014.03.17 00:42
050       http://arxiv.org/abs/1211.6470
*** 6193
010       Learning Joint Query Interpretation and Response Ranking
021       Thanks to information extraction and semantic Web efforts, search on unstructured text is increasingly refined using semantic annotations and structured knowledge bases. 
020       However, most users cannot become familiar with the schema of knowledge bases and ask structured queries. Interpreting free-format queries into a more structured representation is of much current interest. The dominant paradigm is to segment or partition query tokens by purpose (references to types, entities, attribute names, attribute values, relations) and then launch the interpreted query on structured knowledge bases. Given that structured knowledge extraction is never complete, here we use a data representation that retains the unstructured text corpus, along with structured annotations (mentions of entities and relationships) on it. We propose two new, natural formulations for joint query interpretation and response ranking that exploit bidirectional flow of information between the knowledge base and the corpus.One, inspired by probabilistic language models, computes expected response scores over the uncertainties of query interpretation. The other is based on max-margin discriminative learning, with latent variables representing those uncertainties. In the context of typed entity search, both formulations bridge a considerable part of the accuracy gap between a generic query that does not constrain the type at all, and the upper bound where the "perfect" target entity type of each query is provided by humans. Our formulations are also superior to a two-stage approach of first choosing a target type using recent query type prediction techniques, and then launching a type-restricted entity search query. 
007       rubr.cs.IR
007       name.Uma Sawant
007       name.Soumen Chakrabarti
040       2012.12.26 15:28
050       http://arxiv.org/abs/1212.6193
*** 556
010       Learning with Scope, with Application to Information Extraction and   Classification
021       In probabilistic approaches to classification and information extraction, one typically builds a statistical model of words under the assumption that future data will exhibit the same regularities as the training data. 
020       In many data sets, however, there are scope-limited features whose predictive power is only applicable to a certain subset of the data. For example, in information extraction from web pages, word formatting may be indicative of extraction category in different ways on different web pages. The difficulty with using such features is capturing and exploiting the new regularities encountered in previously unseen data. In this paper, we propose a hierarchical probabilistic model that uses both local/scope-limited features, such as word formatting, and global features, such as word content. The local regularities are modeled as an unobserved random parameter which is drawn once for each local data set. This random parameter is estimated during the inference process and then used to perform classification with both the local and global features--- a procedure which is akin to automatically retuning the classifier to the local regularities on each newly encountered web page. Exact inference is intractable and we present approximations via point estimates and variational methods. Empirical results on large collections of web data demonstrate that this method significantly improves performance from traditional models of global features alone. 
007       rubr.cs.LG
007       rubr.cs
007       rubr.cs.IR
007       rubr.cs.IR
007       rubr.stat.ML
007       name.David Blei
007       name.J Andrew Bagnell
007       name.Andrew McCallum
040       2012.12.12 15:55
050       http://arxiv.org/abs/1301.0556
*** 1335
010       Ontology Guided Information Extraction from Unstructured Text
021       In this paper, we describe an approach to populate an existing ontology with instance information present in the natural language text provided as input. 
020       An ontology is defined as an explicit conceptualization of a shared domain. This approach starts with a list of relevant domain ontologies created by human experts, and techniques for identifying the most appropriate ontology to be extended with information from a given text. Then we demonstrate heuristics to extract information from the unstructured text and for adding it as structured information to the selected ontology. This identification of the relevant ontology is critical, as it is used in identifying relevant information in the text. We extract information in the form of semantic triples from the text, guided by the concepts in the ontology. We then convert the extracted information about the semantic class instances into Resource Description Framework (RDF3) and append it to the existing domain ontology. This enables us to perform more precise semantic queries over the semantic triple store thus created. We have achieved 95% accuracy of information extraction in our implementation. 
007       rubr.cs.IR
007       name.Raghu Anantharangachar
007       name.Srinivasan Ramani
007       name.S Rajagopalan
040       2013.02.06 12:19
050       http://arxiv.org/abs/1302.1335
*** 4492
010       Bilingual Terminology Extraction Using Multi-level Termhood
021       Purpose: Terminology is the set of technical words or expressions used in specific contexts, which denotes the core concept in a formal discipline and is usually applied in the fields of machine translation, information retrieval, information extraction and text categorization, etc. 
020       Bilingual terminology extraction plays an important role in the application of bilingual dictionary compilation, bilingual Ontology construction, machine translation and cross-language information retrieval etc. This paper addresses the issues of monolingual terminology extraction and bilingual term alignment based on multi-level termhood. <br />Design/methodology/approach: A method based on multi-level termhood is proposed. The new method computes the termhood of the terminology candidate as well as the sentence that includes the terminology by the comparison of the corpus. Since terminologies and general words usually have differently distribution in the corpus, termhood can also be used to constrain and enhance the performance of term alignment when aligning bilingual terms on the parallel corpus. In this paper, bilingual term alignment based on termhood constraints is presented. <br />Findings: Experiment results show multi-level termhood can get better performance than existing method for terminology extraction. If termhood is used as constrain factor, the performance of bilingual term alignment can be improved. 
007       rubr.cs.CL
007       name.Chengzhi Zhang
007       name.Dan Wu
040       2013.02.19 00:37
050       http://arxiv.org/abs/1302.4492
*** 4813
010       Probabilistic Frame Induction
021       In natural-language discourse, related events tend to appear near each other to describe a larger scenario. 
020       Such structures can be formalized by the notion of a frame (a.k.a. template), which comprises a set of related events and prototypical participants and event transitions. Identifying frames is a prerequisite for information extraction and natural language generation, and is usually done manually. Methods for inducing frames have been proposed recently, but they typically use ad hoc procedures and are difficult to diagnose or extend. In this paper, we propose the first probabilistic approach to frame induction, which incorporates frames, events, participants as latent topics and learns those frame and event transitions that best explain the text. The number of frames is inferred by a novel application of a split-merge method from syntactic parsing. In end-to-end evaluations from text to induced frames and extracted facts, our method produced state-of-the-art results while substantially reducing engineering effort. 
007       rubr.cs.CL
007       name.Jackie Chi Kit Cheung
007       name.Hoifung Poon
007       name.Lucy Vanderwende
040       2013.02.20 05:47
050       http://arxiv.org/abs/1302.4813
*** 5675
010       Development of Yes/No Arabic Question Answering System
021       Developing Question Answering systems has been one of the important research issues because it requires insights from a variety of disciplines,including,Artificial Intelligence,Information Retrieval, Information Extraction,Natural Language Processing, and Psychology.In this paper we realize a formal model for a lightweight semantic based open domain yes/no Arabic question answering system based on paragraph retrieval with variable length. 
020       We propose a constrained semantic representation. Using an explicit unification framework based on semantic similarities and query expansion synonyms and antonyms.This frequently improves the precision of the system. Employing the passage retrieval system achieves a better precision by retrieving more paragraphs that contain relevant answers to the question; It significantly reduces the amount of text to be processed by the system. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       name.Wafa N. Bdour
007       name.Natheer K. Gharaibeh
040       2013.02.22 18:59
050       http://arxiv.org/abs/1302.5675
*** 445
010       Detecting and resolving spatial ambiguity in text using named entity   extraction and self learning fuzzy logic techniques
021       Information extraction identifies useful and relevant text in a document and converts unstructured text into a form that can be loaded into a database table. 
020       Named entity extraction is a main task in the process of information extraction and is a classification problem in which words are assigned to one or more semantic classes or to a default non-entity class. A word which can belong to one or more classes and which has a level of uncertainty in it can be best handled by a self learning Fuzzy Logic Technique. This paper proposes a method for detecting the presence of spatial uncertainty in the text and dealing with spatial ambiguity using named entity extraction techniques coupled with self learning fuzzy logic techniques 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       name.Kanagavalli V R
007       name.Raja. K
040       2013.03.03 01:21
050       http://arxiv.org/abs/1303.0445
*** 5260
010       M-ATTEMPT: A New Energy-Efficient Routing Protocol for Wireless Body   Area Sensor Networks
021       In this paper, we propose a new routing protocol for heterogeneous Wireless Body Area Sensor Networks (WBASNs); Mobility-supporting Adaptive Threshold-based Thermal-aware Energy-efficientMulti-hop ProTocol (M-ATTEMPT). 
020       A prototype is defined for employing heterogeneous sensors on human body. Direct communication is used for real-time traffic (critical data) or on-demand data while Multi-hop communication is used for normal data delivery. One of the prime challenges in WBASNs is sensing of the heat generated by the implanted sensor nodes. The proposed routing algorithm is thermal-aware which senses the link Hot-spot and routes the data away from these links. Continuous mobility of human body causes disconnection between previous established links. So, mobility support and energy-management is introduced to overcome the problem. Linear Programming (LP) model for maximum information extraction and minimum energy consumption is presented in this study. MATLAB simulations of proposed routing algorithm are performed for lifetime and successful packet delivery in comparison with Multi-hop communication. The results show that the proposed routing algorithm has less energy consumption and more reliable as compared to Multi-hop communication. 
007       rubr.cs.NI
007       name.N. Javaid
007       name.Z. Abbas
007       name.M. S. Fareed
007       name.Z. A. Khan
007       name.N. Alrajeh
040       2013.03.21 14:01
050       http://arxiv.org/abs/1303.5260
*** 6428
010       Bicompletions of distance matrices
021       In the practice of information extraction, the input data are usually arranged into pattern matrices, and analyzed by the methods of linear algebra and statistics, such as principal component analysis. 
020       In some applications, the tacit assumptions of these methods lead to wrong results. The usual reason is that the matrix composition of linear algebra presents information as flowing in waves, whereas it sometimes flows in particles, which seek the shortest paths. This wave-particle duality in computation and information processing has been originally observed by Abramsky. In this paper we pursue a particle view of information, formalized in *distance spaces*, which generalize metric spaces, but are slightly less general than Lawvere's *generalized metric spaces*. In this framework, the task of extracting the 'principal components' from a given matrix of data boils down to a bicompletio}, in the sense of enriched category theory. We describe the bicompletion construction for distance matrices. The practical goal that motivates this research is to develop a method to estimate the hardness of attack constructions in security. 
007       rubr.cs.LO
007       rubr.cs
007       rubr.math
007       rubr.math.CT
007       rubr.math.MG
007       name.Dusko Pavlovic
040       2013.03.26 10:32
050       http://arxiv.org/abs/1303.6428
*** 3268
010       Web Services Discovery and Recommendation Based on Information   Extraction and Symbolic Reputation
021       This paper shows that the problem of web services representation is crucial and analyzes the various factors that influence on it. 
020       It presents the traditional representation of web services considering traditional textual descriptions based on the information contained in WSDL files. Unfortunately, textual web services descriptions are dirty and need significant cleaning to keep only useful information. To deal with this problem, we introduce rules based text tagging method, which allows filtering web service description to keep only significant information. A new representation based on such filtered data is then introduced. Many web services have empty descriptions. Also, we consider web services representations based on the WSDL file structure (types, attributes, etc.). Alternatively, we introduce a new representation called symbolic reputation, which is computed from relationships between web services. The impact of the use of these representations on web service discovery and recommendation is studied and discussed in the experimentation using real world web services. 
007       rubr.cs.IR
007       name.Mustapha Aznag
007       name.Mohamed Quafafou
007       name.Nicolas Durand
007       name.Zahi Jarir
040       2013.04.11 12:21
050       http://arxiv.org/abs/1304.3268
*** 5170
010       Clinical Relationships Extraction Techniques from Patient Narratives
021       The Clinical E-Science Framework (CLEF) project was used to extract important information from medical texts by building a system for the purpose of clinical research, evidence-based healthcare and genotype-meets-phenotype informatics. 
020       The system is divided into two parts, one part concerns with the identification of relationships between clinically important entities in the text. The full parses and domain-specific grammars had been used to apply many approaches to extract the relationship. In the second part of the system, statistical machine learning (ML) approaches are applied to extract relationship. A corpus of oncology narratives that hand annotated with clinical relationships can be used to train and test a system that has been designed and implemented by supervised machine learning (ML) approaches. Many features can be extracted from these texts that are used to build a model by the classifier. Multiple supervised machine learning algorithms can be applied for relationship extraction. Effects of adding the features, changing the size of the corpus, and changing the type of the algorithm on relationship extraction are examined. Keywords: Text mining; information extraction; NLP; entities; and relations. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       name.Wafaa Tawfik Abdel-moneim
007       name.Mohamed Hashem Abdel-Aziz
007       name.Mohamed Monier Hassan
040       2013.06.21 15:30
050       http://arxiv.org/abs/1306.5170
*** 261
010       WebSets: Extracting Sets of Entities from the Web Using Unsupervised   Information Extraction
021       We describe a open-domain information extraction method for extracting concept-instance pairs from an HTML corpus. 
020       Most earlier approaches to this problem rely on combining clusters of distributionally similar terms and concept-instance pairs obtained with Hearst patterns. In contrast, our method relies on a novel approach for clustering terms found in HTML tables, and then assigning concept names to these clusters using Hearst patterns. The method can be efficiently applied to a large corpus, and experimental results on several datasets show that our method can accurately extract large numbers of concept-instance pairs. 
007       rubr.cs.LG
007       rubr.cs
007       rubr.cs.CL
007       rubr.cs.IR
007       name.Bhavana Dalvi
007       name.William W. Cohen
007       name.Jamie Callan
040       2013.07.01 02:49
050       http://arxiv.org/abs/1307.0261
*** 2434
010       Major Limitations of Satellite images
021       Remote sensing has proven to be a powerful tool for the monitoring of the Earth surface to improve our perception of our surroundings has led to unprecedented developments in sensor and information technologies. 
020       However, technologies for effective use of the data and for extracting useful information from the data of Remote sensing are still very limited since no single sensor combines the optimal spectral, spatial and temporal resolution. This paper briefly reviews the limitations of satellite remote sensing. Also, reviews on the problems of image fusion techniques. The conclusion of this, According to literature, the remote sensing is still the lack of software tools for effective information extraction from remote sensing data. The trade-off in spectral and spatial resolution will remain and new advanced data fusion approaches are needed to make optimal use of remote sensors for extract the most useful information. 
007       rubr.cs.CV
007       name.Firouz A. Al-Wassai
007       name.N.V. Kalyankar
040       2013.07.09 13:01
050       http://arxiv.org/abs/1307.2434
*** 2440
010       Image Fusion Technologies In Commercial Remote Sensing Packages
021       Several remote sensing software packages are used to the explicit purpose of analyzing and visualizing remotely sensed data, with the developing of remote sensing sensor technologies from last ten years. 
020       Accord-ing to literature, the remote sensing is still the lack of software tools for effective information extraction from remote sensing data. So, this paper provides a state-of-art of multi-sensor image fusion technologies as well as review on the quality evaluation of the single image or fused images in the commercial remote sensing pack-ages. It also introduces program (ALwassaiProcess) developed for image fusion and classification. 
007       rubr.cs.CV
007       name.Firouz Abdullah Al-Wassai
007       name.N.V. Kalyankar
040       2013.07.09 13:14
050       http://arxiv.org/abs/1307.2440
*** 4941
010       Automatic Labeling for Entity Extraction in Cyber Security
021       Timely analysis of cyber-security information necessitates automated information extraction from unstructured text. 
020       While state-of-the-art extraction methods produce extremely accurate results, they require ample training data, which is generally unavailable for specialized applications, such as detecting security related entities; moreover, manual annotation of corpora is very costly and often not a viable solution. In response, we develop a very precise method to automatically label text from several data sources by leveraging related, domain-specific, structured data and provide public access to a corpus annotated with cyber-security entities. Next, we implement a Maximum Entropy Model trained with the average perceptron on a portion of our corpus ($\sim$750,000 words) and achieve near perfect precision, recall, and accuracy, with training times under 17 seconds. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       name.Robert A. Bridges
007       name.Corinne L. Jones
007       name.Michael D. Iannacone
007       name.Kelly M. Testa
007       name.John R. Goodall
040       2014.06.09 23:51
050       http://arxiv.org/abs/1308.4941
*** 575
010       Development of Marathi Part of Speech Tagger Using Statistical Approach
021       Part-of-speech (POS) tagging is a process of assigning the words in a text corresponding to a particular part of speech. 
020       A fundamental version of POS tagging is the identification of words as nouns, verbs, adjectives etc. For processing natural languages, Part of Speech tagging is a prominent tool. It is one of the simplest as well as most constant and statistical model for many NLP applications. POS Tagging is an initial stage of linguistics, text analysis like information retrieval, machine translator, text to speech synthesis, information extraction etc. In POS Tagging we assign a Part of Speech tag to each word in a sentence and literature. Various approaches have been proposed to implement POS taggers. In this paper we present a Marathi part of speech tagger. It is morphologically rich language. Marathi is spoken by the native people of Maharashtra. The general approach used for development of tagger is statistical using Unigram, Bigram, Trigram and HMM Methods. It presents a clear idea about all the algorithms with suitable examples. It also introduces a tag set for Marathi which can be used for tagging Marathi text. In this paper we have shown the development of the tagger as well as compared to check the accuracy of taggers output. The three Marathi POS taggers viz. Unigram, Bigram, Trigram and HMM gives the accuracy of 77.38%, 90.30%, 91.46% and 93.82% respectively. 
007       rubr.cs.CL
007       name.Jyoti Singh
007       name.Nisheeth Joshi
007       name.Iti Mathur
040       2013.10.09 10:57
050       http://arxiv.org/abs/1310.0575
*** 1964
010       Named entity recognition using conditional random fields with non-local   relational constraints
021       We begin by introducing the Computer Science branch of Natural Language Processing, then narrowing the attention on its subbranch of Information Extraction and particularly on Named Entity Recognition, discussing briefly its main methodological approaches. 
020       It follows an introduction to state-of-the-art Conditional Random Fields under the form of linear chains. Subsequently, the idea of constrained inference as a way to model long-distance relationships in a text is presented, based on an Integer Linear Programming representation of the problem. Adding such relationships to the problem as automatically inferred logical formulas, translatable into linear conditions, we propose to solve the resulting more complex problem with the aid of Lagrangian relaxation, of which some technical details are explained. Lastly, we give some experimental results. 
007       rubr.cs.CL
007       name.Flavio Massimiliano Cecchini
007       name.Elisabetta Fersini
040       2013.10.07 22:08
050       http://arxiv.org/abs/1310.1964
*** 2453
010       High Capacity Imaging and Object Identification with Correlated Orbital   Angular Momentum Spectroscopy
021       The method of correlated orbital angular momentum (OAM) spectroscopy is shown to be capable of image reconstruction for complex, off-axis objects, with information extraction rates exceeding one bit per photon. 
020       Computer simulations of OAM-correlated beams are combined with digitized representations of opaque objects in order to study various effects of the object on the joint OAM coincidence spectrum. It is shown through simulations of OAM-correlated beams that complex image identification and even reconstruction is possible without any measurement in position space. In addition to demonstrating the novel image reconstruction capabilities of this correlation method, the unique off-diagonal spectral signatures, as well as the mutual information rates associated to each object studied, are presented. In particular, changes in these properties due to off-axis translation in the beam field are considered; it is shown that spectral signatures and information rates are independent of environmental factors sufficiently far from the beam center. The results suggest further application in small-scale biological contexts where symmetry and small numbers of noninvasive measurements are important. 
007       rubr.quant-ph
007       name.Casey A. Fitzpatrick
007       name.David S. Simon
007       name.Alexander V. Sergienko
040       2013.10.07 20:42
050       http://arxiv.org/abs/1310.2453
*** 3987
010       Big Data and Cross-Document Coreference Resolution: Current State and   Future Opportunities
021       Information Extraction (IE) is the task of automatically extracting structured information from unstructured/semi-structured machine-readable documents. 
020       Among various IE tasks, extracting actionable intelligence from ever-increasing amount of data depends critically upon Cross-Document Coreference Resolution (CDCR) - the task of identifying entity mentions across multiple documents that refer to the same underlying entity. Recently, document datasets of the order of peta-/tera-bytes has raised many challenges for performing effective CDCR such as scaling to large numbers of mentions and limited representational power. The problem of analysing such datasets is called "big data". The aim of this paper is to provide readers with an understanding of the central concepts, subtasks, and the current state-of-the-art in CDCR process. We provide assessment of existing tools/techniques for CDCR subtasks and highlight big data challenges in each of them to help readers identify important and outstanding issues for further investigation. Finally, we provide concluding remarks and discuss possible directions for future work. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.DC
007       rubr.cs.IR
007       name.Seyed-Mehdi-Reza Beheshti
007       name.Srikumar Venugopal
007       name.Seung Hwan Ryu
007       name.Boualem Benatallah
007       name.Wei Wang
040       2013.11.14 06:10
050       http://arxiv.org/abs/1311.3987
*** 4180
010       Towards a New Science of a Clinical Data Intelligence
021       In this paper we define Clinical Data Intelligence as the analysis of data generated in the clinical routine with the goal of improving patient care. 
020       We define a science of a Clinical Data Intelligence as a data analysis that permits the derivation of scientific, i.e., generalizable and reliable results. We argue that a science of a Clinical Data Intelligence is sensible in the context of a Big Data analysis, i.e., with data from many patients and with complete patient information. We discuss that Clinical Data Intelligence requires the joint efforts of knowledge engineering, information extraction (from textual and other unstructured data), and statistics and statistical machine learning. We describe some of our main results as conjectures and relate them to a recently funded research project involving two major German university hospitals. 
007       rubr.cs.CY
007       rubr.cs
007       rubr.cs.AI
007       name.Volker Tresp
007       name.Sonja Zillner
007       name.Maria J. Costa
007       name.Yi Huang
007       name.Alexander Cavallaro
007       name.Peter A. Fasching
007       name.Andre Reis
007       name.Martin Sedlmayr
007       name.Thomas Ganslandt
007       name.Klemens Budde
007       name.Carl Hinrichs
007       name.Danilo Schmidt
007       name.Philipp Daumke
007       name.Daniel Sonntag
007       name.Thomas Wittenberg
007       name.Patricia G. Oppelt
007       name.Denis Krompass
040       2013.12.30 09:58
050       http://arxiv.org/abs/1311.4180
*** 5204
010       On Quantifying Qualitative Geospatial Data: A Probabilistic Approach
021       Living in the era of data deluge, we have witnessed a web content explosion, largely due to the massive availability of User-Generated Content (UGC). 
020       In this work, we specifically consider the problem of geospatial information extraction and representation, where one can exploit diverse sources of information (such as image and audio data, text data, etc), going beyond traditional volunteered geographic information. Our ambition is to include available narrative information in an effort to better explain geospatial relationships: with spatial reasoning being a basic form of human cognition, narratives expressing such experiences typically contain qualitative spatial data, i.e., spatial objects and spatial relationships. <br />To this end, we formulate a quantitative approach for the representation of qualitative spatial relations extracted from UGC in the form of texts. The proposed method quantifies such relations based on multiple text observations. Such observations provide distance and orientation features which are utilized by a greedy Expectation Maximization-based (EM) algorithm to infer a probability distribution over predefined spatial relationships; the latter represent the quantified relationships under user-defined probabilistic assumptions. We evaluate the applicability and quality of the proposed approach using real UGC data originating from an actual travel blog text corpus. To verify the quality of the result, we generate grid-based maps visualizing the spatial extent of the various relations. 
007       rubr.cs.DB
007       name.Georgios Skoumas
007       name.Dieter Pfoser
007       name.Anastasios Kyrillidis
040       2013.11.18 09:06
050       http://arxiv.org/abs/1311.5204
*** 6945
010       Quantum Ensemble Classification: A Sampling-based Learning Control   Approach
021       Quantum ensemble classification has significant applications in discrimination of atoms (or molecules), separation of isotopic molecules and quantum information extraction. 
020       However, quantum mechanics forbids deterministic discrimination among nonorthogonal states. The classification of inhomogeneous quantum ensembles is very challenging since there exist variations in the parameters characterizing the members within different classes. In this paper, we recast quantum ensemble classification as a supervised quantum learning problem. A systematic classification methodology is presented by using a sampling-based learning control (SLC) approach for quantum discrimination. The classification task is accomplished via simultaneously steering members belonging to different classes to their corresponding target states (e.g., mutually orthogonal states). Firstly a new discrimination method is proposed for two similar quantum systems. Then an SLC method is presented for quantum ensemble classification. Numerical results demonstrate the effectiveness of the proposed approach for the binary classification of two-level quantum ensembles and the multiclass classification of multilevel quantum ensembles. 
007       rubr.quant-ph
007       rubr.cs
007       rubr.cs.SY
007       rubr.cs.SY
007       name.Chunlin Chen
007       name.Daoyi Dong
007       name.Bo Qi
007       name.Ian R. Petersen
007       name.Herschel Rabitz
040       2013.12.25 08:49
050       http://arxiv.org/abs/1312.6945
*** 694
010       Uncertainty-based information extraction in wireless sensor networks for   control applications
021       Design of control applications over wireless sensor networks (WSNs) is a challenging issue due to the bandwidth-limited communication medium, energy constraints and real-time data delivery requirements. 
020       This paper introduces a new information extraction method for WSN-based control applications, which reduces the number of required data transmissions to save energy and avoid data congestion. According to the proposed approach, control applications recognize when new data readings have to be collected and determine sensor nodes that have to be activated on the basis of uncertainty analysis. Processing of the selectively collected input data is based on definition of information granules that describe state of the controlled system as well as performance of particular control decisions. This method was implemented for object tracking in WSNs. The task is to control movement of a mobile sink, which has to reach a target in the shortest possible time. Extensive simulation experiments were performed to compare performance of the proposed approach against state-of-the-art methods. Results of the experiments show that the presented information extraction method allows for substantial reduction in the amount of transmitted data with no significant negative effect on tracking performance. 
007       rubr.cs.NI
007       name.Bartlomiej Placzek
007       name.Marcin Bernas
040       2014.01.03 18:10
050       http://arxiv.org/abs/1401.0694
*** 2871
010       Tensor Representation and Manifold Learning Methods for Remote Sensing   Images
021       One of the main purposes of earth observation is to extract interested information and knowledge from remote sensing (RS) images with high efficiency and accuracy. 
020       However, with the development of RS technologies, RS system provide images with higher spatial and temporal resolution and more spectral channels than before, and it is inefficient and almost impossible to manually interpret these images. Thus, it is of great interests to explore automatic and intelligent algorithms to quickly process such massive RS data with high accuracy. This thesis targets to develop some efficient information extraction algorithms for RS images, by relying on the advanced technologies in machine learning. More precisely, we adopt the manifold learning algorithms as the mainline and unify the regularization theory, tensor-based method, sparse learning and transfer learning into the same framework. The main contributions of this thesis are as follows. 
007       rubr.cs.CV
007       name.Lefei Zhang
040       2014.01.13 15:33
050       http://arxiv.org/abs/1401.2871
*** 2937
010       A survey of methods to ease the development of highly multilingual text   mining applications
021       Multilingual text processing is useful because the information content found in different languages is complementary, both regarding facts and opinions. 
020       While Information Extraction and other text mining software can, in principle, be developed for many languages, most text analysis tools have only been applied to small sets of languages because the development effort per language is large. Self-training tools obviously alleviate the problem, but even the effort of providing training data and of manually tuning the results is usually considerable. In this paper, we gather insights by various multilingual system developers on how to minimise the effort of developing natural language processing applications for many languages. We also explain the main guidelines underlying our own effort to develop complex text mining software for tens of languages. While these guidelines - most of all: extreme simplicity - can be very restrictive and limiting, we believe to have shown the feasibility of the approach through the development of the Europe Media Monitor (EMM) family of applications (<a href="http://emm.newsbrief.eu/overview.html">this http URL</a>). EMM is a set of complex media monitoring tools that process and analyse up to 100,000 online news articles per day in between twenty and fifty languages. We will also touch upon the kind of language resources that would make it easier for all to develop highly multilingual text mining applications. We will argue that - to achieve this - the most needed resources would be freely available, simple, parallel and uniform multilingual dictionaries, corpora and software tools. 
007       rubr.cs.CL
007       name.Ralf Steinberger
040       2014.01.13 18:05
050       http://arxiv.org/abs/1401.2937
*** 3832
010       Constructing Reference Sets from Unstructured, Ungrammatical Text
021       Vast amounts of text on the Web are unstructured and ungrammatical, such as classified ads, auction listings, forum postings, etc. 
020       We call such text "posts." Despite their inconsistent structure and lack of grammar, posts are full of useful information. This paper presents work on semi-automatically building tables of relational information, called "reference sets," by analyzing such posts directly. Reference sets can be applied to a number of tasks such as ontology maintenance and information extraction. Our reference-set construction method starts with just a small amount of background knowledge, and constructs tuples representing the entities in the posts to form a reference set. We also describe an extension to this approach for the special case where even this small amount of background knowledge is impossible to discover and use. To evaluate the utility of the machine-constructed reference sets, we compare them to manually constructed reference sets in the context of reference-set-based information extraction. Our results show the reference sets constructed by our method outperform manually constructed reference sets. We also compare the reference-set-based extraction approach using the machine-constructed reference set to supervised extraction approaches using generic features. These results demonstrate that using machine-constructed reference sets outperforms the supervised methods, even though the supervised methods require training data. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       name.Matthew Michelson
007       name.Craig A. Knoblock
040       2014.01.16 04:49
050       http://arxiv.org/abs/1401.3832
*** 3865
010       Evaluating Temporal Graphs Built from Texts via Transitive Reduction
021       Temporal information has been the focus of recent attention in information extraction, leading to some standardization effort, in particular for the task of relating events in a text. 
020       This task raises the problem of comparing two annotations of a given text, because relations between events in a story are intrinsically interdependent and cannot be evaluated separately. A proper evaluation measure is also crucial in the context of a machine learning approach to the problem. Finding a common comparison referent at the text level is not obvious, and we argue here in favor of a shift from event-based measures to measures on a unique textual object, a minimal underlying temporal graph, or more formally the transitive reduction of the graph of relations between event boundaries. We support it by an investigation of its properties on synthetic data and on a well-know temporal corpus. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       name.Xavier Tannier
007       name.Philippe Muller
040       2014.01.16 05:05
050       http://arxiv.org/abs/1401.3865
*** 5696
010       Unsupervised Methods for Determining Object and Relation Synonyms on the   Web
021       The task of identifying synonymous relations and objects, or synonym resolution, is critical for high-quality information extraction. 
020       This paper investigates synonym resolution in the context of unsupervised information extraction, where neither hand-tagged training examples nor domain knowledge is available. The paper presents a scalable, fully-implemented system that runs in O(KN log N) time in the number of extractions, N, and the maximum number of synonyms per word, K. The system, called Resolver, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. On a set of two million assertions extracted from the Web, Resolver resolves objects with 78% precision and 68% recall, and resolves relations with 90% precision and 35% recall. Several variations of resolvers probabilistic model are explored, and experiments demonstrate that under appropriate conditions these variations can improve F1 by 5%. An extension to the basic Resolver system allows it to handle polysemous names with 97% precision and 95% recall on a data set from the TREC corpus. 
007       rubr.cs.CL
007       name.Alexander Pieter Yates
007       name.Oren Etzioni
040       2014.01.15 05:33
050       http://arxiv.org/abs/1401.5696
*** 6427
010       Towards Unsupervised Learning of Temporal Relations between Events
021       Automatic extraction of temporal relations between event pairs is an important task for several natural language processing applications such as Question Answering, Information Extraction, and Summarization. 
020       Since most existing methods are supervised and require large corpora, which for many languages do not exist, we have concentrated our efforts to reduce the need for annotated data as much as possible. This paper presents two different algorithms towards this goal. The first algorithm is a weakly supervised machine learning approach for classification of temporal relations between events. In the first stage, the algorithm learns a general classifier from an annotated corpus. Then, inspired by the hypothesis of "one type of temporal relation per discourse, it extracts useful information from a cluster of topically related documents. We show that by combining the global information of such a cluster with local decisions of a general classifier, a bootstrapping cross-document classifier can be built to extract temporal relations between events. Our experiments show that without any additional annotated data, the accuracy of the proposed algorithm is higher than that of several previous successful systems. The second proposed method for temporal relation extraction is based on the expectation maximization (EM) algorithm. Within EM, we used different techniques such as a greedy best-first search and integer linear programming for temporal inconsistency removal. We think that the experimental results of our EM based algorithm, as a first step toward a fully unsupervised temporal relation extraction method, is encouraging. 
007       rubr.cs.LG
007       rubr.cs
007       rubr.cs.CL
007       name.Seyed Abolghasem Mirroshandel
007       name.Gholamreza Ghassem-Sani
040       2014.01.23 02:50
050       http://arxiv.org/abs/1401.6427
*** 7005
010       Bayesian Multi-Scale Optimistic Optimization
021       Bayesian optimization is a powerful global optimization technique for expensive black-box functions. 
020       One of its shortcomings is that it requires auxiliary optimization of an acquisition function at each iteration. This auxiliary optimization can be costly and very hard to carry out in practice. Moreover, it creates serious theoretical concerns, as most of the convergence results assume that the exact optimum of the acquisition function can be found. In this paper, we introduce a new technique for efficient global optimization that combines Gaussian process confidence bounds and treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions. The experiments with global optimization benchmarks and a novel application to automatic information extraction demonstrate that the resulting technique is more efficient than the two approaches from which it draws inspiration. Unlike most theoretical analyses of Bayesian optimization with Gaussian processes, our finite-time convergence rate proofs do not require exact optimization of an acquisition function. That is, our approach eliminates the unsatisfactory assumption that a difficult, potentially NP-hard, problem has to be solved in order to obtain vanishing regret rates. 
007       rubr.stat.ML
007       rubr.cs
007       rubr.cs.LG
007       name.Ziyu Wang
007       name.Babak Shakibi
007       name.Lin Jin
007       name.Nando de Freitas
040       2014.02.27 18:38
050       http://arxiv.org/abs/1402.7005
*** 6023
010       Ensemble Detection of Single &amp; Multiple Events at Sentence-Level
021       Event classification at sentence level is an important Information Extraction task with applications in several NLP, IR, and personalization systems. 
020       Multi-label binary relevance (BR) are the state-of-art methods. In this work, we explored new multi-label methods known for capturing relations between event types. These new methods, such as the ensemble Chain of Classifiers, improve the F1 on average across the 6 labels by 2.8% over the Binary Relevance. The low occurrence of multi-label sentences motivated the reduction of the hard imbalanced multi-label classification problem with low number of occurrences of multiple labels per instance to an more tractable imbalanced multiclass problem with better results (+ 4.6%). We report the results of adding new features, such as sentiment strength, rhetorical signals, domain-id (source-id and date), and key-phrases in both single-label and multi-label event classification scenarios. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.CL
007       name.Lu&#xed;s Marujo
007       name.Anatole Gershman
007       name.Jaime Carbonell
007       name.Jo&#xe3;o P. Neto
007       name.David Martins de Matos
040       2014.03.24 16:21
050       http://arxiv.org/abs/1403.6023
*** 3301
010       Efficient Inference and Learning in a Large Knowledge Base: Reasoning   with Extracted Information using a Locally Groundable First-Order   Probabilistic Logic
021       One important challenge for probabilistic logics is reasoning with very large knowledge bases (KBs) of imperfect information, such as those produced by modern web-scale information extraction systems. 
020       One scalability problem shared by many probabilistic logics is that answering queries involves "grounding" the query---i.e., mapping it to a propositional representation---and the size of a "grounding" grows with database size. To address this bottleneck, we present a first-order probabilistic language called ProPPR in which that approximate "local groundings" can be constructed in time independent of database size. Technically, ProPPR is an extension to stochastic logic programs (SLPs) that is biased towards short derivations; it is also closely related to an earlier relational learning algorithm called the path ranking algorithm (PRA). We show that the problem of constructing proofs for this logic is related to computation of personalized PageRank (PPR) on a linearized version of the proof space, and using on this connection, we develop a proveably-correct approximate grounding scheme, based on the PageRank-Nibble algorithm. Building on this, we develop a fast and easily-parallelized weight-learning algorithm for ProPPR. In experiments, we show that learning for ProPPR is orders magnitude faster than learning for Markov logic networks; that allowing mutual recursion (joint learning) in KB inference leads to improvements in performance; and that ProPPR can learn weights for a mutually recursive program with hundreds of clauses, which define scores of interrelated predicates, over a KB containing one million entities. 
007       rubr.cs.AI
007       name.William Yang Wang
007       name.Kathryn Mazaitis
007       name.Ni Lao
007       name.Tom Mitchell
007       name.William W. Cohen
040       2014.04.12 16:59
050       http://arxiv.org/abs/1404.3301
*** 7709
010       Using Local Alignments for Relation Recognition
021       This paper discusses the problem of marrying structural similarity with semantic relatedness for Information Extraction from text. 
020       Aiming at accurate recognition of relations, we introduce local alignment kernels and explore various possibilities of using them for this task. We give a definition of a local alignment (LA) kernel based on the Smith-Waterman score as a sequence similarity measure and proceed with a range of possibilities for computing similarity between elements of sequences. We show how distributional similarity measures obtained from unlabeled data can be incorporated into the learning task as semantic knowledge. Our experiments suggest that the LA kernel yields promising results on various biomedical corpora outperforming two baselines by a large margin. Additional series of experiments have been conducted on the data sets of seven general relation types, where the performance of the LA kernel is comparable to the current state-of-the-art results. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       rubr.cs.LG
007       name.Sophia Katrenko
007       name.Pieter Adriaans
007       name.Maarten van Someren
040       2014.01.16 04:51
050       http://arxiv.org/abs/1405.7713
*** 1274
010       Real-time dynamics acquisition from irregular samples -- with   application to anesthesia evaluation
021       The first objective of this paper is to introduce a unified approach to the D/A conversion, a real-time algorithm referred to as {\it blending operator}, based on spline functions of arbitrarily desired order, to interpolate the irregular data samples, while preserving all polynomials of the same spline order, with assured maximum order of approximation. 
020       This helps remove the two main obstacles for adapting the recently proposed time-frequency analysis technique {\it Synchrosqueezing transform} (SST) to irregular data samples in order to allow online computation. Secondly, for real-time dynamic information extraction from an oscillatory signal via SST, a family of vanishing-moment and minimum-supported spline-wavelets (to be called VM wavelets) are introduced for on-line computation of the CWT and its derivative. The second objective of this paper is to apply the proposed real-time algorithm and VM wavelets to clinical applications, particularly to the study of the "anesthetic depth" of a patient during surgery, with emphasis on analyzing two dynamic quantities: the "instantaneous frequencies" and the "non-rhythmic to rhythmic ratios" of the patient's respiration, based on a one-lead electrocardiogram (ECG) signal.It is envisioned that the proposed algorithm and VM wavelets should enable real-time monitoring of "anesthetic depth", during surgery, from the respiration signal via ECG measurement. 
007       rubr.math.NA
007       name.Charles K. Chui
007       name.Yu-Ting Lin
007       name.Hau-tieng Wu
040       2014.06.05 06:14
050       http://arxiv.org/abs/1406.1276
*** 1967
010       A Semantic Enhanced Model for effective Spatial Information Retrieval
021       A lot of information on the web is geographically referenced. 
020       Discovering and retrieving this geographic information to satisfy various users needs across both open and distributed Spatial Data Infrastructures (SDI) poses eminent research challenges. However, this is mostly caused by semantic heterogeneity in users query and lack of semantic referencing of the Geographic Information (GI) metadata. To addressing these challenges, this paper discusses ontology based semantic enhanced model, which explicitly represents GI metadata, and provides linked RDF instances of each entity. The system focuses on semantic search, ontology, and efficient spatial information retrieval. In particular, an integrated model that uses specific domain information extraction to improve the searching and retrieval of ranked spatial search results. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.IT
007       name.Adeyinka K. Akanbi
007       name.Olusanya Y. Agunbiade
007       name.Sadiq Kuti
007       name.Olumuyiwa J. Dehinbo
040       2014.06.08 10:52
050       http://arxiv.org/abs/1406.1969
*** 2536
010       FrameNet CNL: a Knowledge Representation and Information Extraction   Language
021       The paper presents a FrameNet-based information extraction and knowledge representation framework, called FrameNet-CNL. 
020       The framework is used on natural language documents and represents the extracted knowledge in a tailor-made Frame-ontology from which unambiguous FrameNet-CNL paraphrase text can be generated automatically in multiple languages. This approach brings together the fields of information extraction and CNL, because a source text can be considered belonging to FrameNet-CNL, if information extraction parser produces the correct knowledge representation as a result. We describe a state-of-the-art information extraction parser used by a national news agency and speculate that FrameNet-CNL eventually could shape the natural language subset used for writing the newswire articles. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.IR
007       rubr.cs.LG
007       name.Guntis Barzdins
040       2014.06.10 13:16
050       http://arxiv.org/abs/1406.2538
*** 3039
010       The Number of Information Bits Related to the Minimum Quantum and   Gravitational Masses in a Vacuum Dominated Universe
021       Wesson obtained a limit on quantum and gravitational mass in the universe by combining the cosmological constant Lambda, Planck constant, the speed of light c, and also the gravitational constant G. 
020       The corresponding masses are 2.0x10E-62 kg and 2.3E+54 kg respectively, and in general can be obtained with the help of a generic dimensional analysis, or from an analysis where the cosmological constant appears in a four dimensional space-time and as a result of a higher dimensional reduction. In this paper our goal is to establish a relation for both quantum and gravitational mass as function of the information number bit N. For this reason, we first derive an expression for the cosmological constant as a function of information bit, since both masses depend on it, and then various resulting relations are explored, in relation to information number of bits N. Fractional information bits imply no information extraction is possible. We see, that the order of magnitude of the various parameters as well as their ratios involve the large number 10E+122, that is produced naturally from the fundamental parameters of modern cosmology. Finally, we propose that in a complete quantum gravity theory the idea of information the might have to be included, with the quantum bits of information (q-bits) as one of its fundamental parameters, resulting thus to a more complete understanding of the universe, its laws, and its evolution. 
007       rubr.physics.gen-ph
007       name.Ioannis Haranas
007       name.Ioannis Gkigkitzis
040       2014.06.09 15:24
050       http://arxiv.org/abs/1406.3041
*** 3712
010       Mining of product reviews at aspect level
021       Todays world is a world of Internet, almost all work can be done with the help of it, from simple mobile phone recharge to biggest business deals can be done with the help of this technology. 
020       People spent their most of the times on surfing on the Web it becomes a new source of entertainment, education, communication, shopping etc. Users not only use these websites but also give their feedback and suggestions that will be useful for other users. In this way a large amount of reviews of users are collected on the Web that needs to be explored, analyse and organized for better decision making. Opinion Mining or Sentiment Analysis is a Natural Language Processing and Information Extraction task that identifies the users views or opinions explained in the form of positive, negative or neutral comments and quotes underlying the text. Aspect based opinion mining is one of the level of Opinion mining that determines the aspect of the given reviews and classify the review for each feature. In this paper an aspect based opinion mining system is proposed to classify the reviews as positive, negative and neutral for each feature. Negation is also handled in the proposed system. Experimental results using reviews of products show the effectiveness of the system. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       name.Richa Sharma
007       name.Shweta Nigam
007       name.Rekha Jain
040       2014.06.14 10:39
050       http://arxiv.org/abs/1406.3714
*** 5596
010       A survey on phrase structure learning methods for text classification
021       Text classification is a task of automatic classification of text into one of the predefined categories. 
020       The problem of text classification has been widely studied in different communities like natural language processing, data mining and information retrieval. Text classification is an important constituent in many information management tasks like topic identification, spam filtering, email routing, language identification, genre classification, readability assessment etc. The performance of text classification improves notably when phrase patterns are used. The use of phrase patterns helps in capturing non-local behaviours and thus helps in the improvement of text classification task. Phrase structure extraction is the first step to continue with the phrase pattern identification. In this survey, detailed study of phrase structure learning methods have been carried out. This will enable future work in several NLP tasks, which uses syntactic information from phrase structure like grammar checkers, question answering, information extraction, machine translation, text classification. The paper also provides different levels of classification and detailed comparison of the phrase structure learning methods. 
007       rubr.cs.CL
007       name.Reshma Prasad
007       name.Mary Priya Sebastian
040       2014.06.21 11:30
050       http://arxiv.org/abs/1406.5598
*** 6447
010       The Links Have It: Infobox Generation by Summarization over Linked   Entities
021       Online encyclopedia such as Wikipedia has become one of the best sources of knowledge. 
020       Much effort has been devoted to expanding and enriching the structured data by automatic information extraction from unstructured text in Wikipedia. Although remarkable progresses have been made, their effectiveness and efficiency is still limited as they try to tackle an extremely difficult natural language understanding problems and heavily relies on supervised learning approaches which require large amount effort to label the training data. In this paper, instead of performing information extraction over unstructured natural language text directly, we focus on a rich set of semi-structured data in Wikipedia articles: linked entities. The idea of this paper is the following: If we can summarize the relationship between the entity and its linked entities, we immediately harvest some of the most important information about the entity. To this end, we propose a novel rank aggregation approach to remove noise, an effective clustering and labeling algorithm to extract knowledge. 
007       rubr.cs.IR
007       name.Kezun Zhang
007       name.Yanghua Xiao
007       name.Hanghang Tong
007       name.Haixun Wang
007       name.Wei Wang
040       2014.06.25 03:33
050       http://arxiv.org/abs/1406.6449
*** 2918
010       A Survey of Named Entity Recognition in Assamese and other Indian   Languages
021       Named Entity Recognition is always important when dealing with major Natural Language Processing tasks such as information extraction, question-answering, machine translation, document summarization etc so in this paper we put forward a survey of Named Entities in Indian Languages with particular reference to Assamese. 
020       There are various rule-based and machine learning approaches available for Named Entity Recognition. At the very first of the paper we give an idea of the available approaches for Named Entity Recognition and then we discuss about the related research in this field. Assamese like other Indian languages is agglutinative and suffers from lack of appropriate resources as Named Entity Recognition requires large data sets, gazetteer list, dictionary etc and some useful feature like capitalization as found in English cannot be found in Assamese. Apart from this we also describe some of the issues faced in Assamese while doing Named Entity Recognition. 
007       rubr.cs.CL
007       name.Gitimoni Talukdar
007       name.Pranjal Protim Borah
007       name.Arup Baruah
040       2014.07.09 13:59
050       http://arxiv.org/abs/1407.2918
*** 4833
010       $OntoMath^{PRO}$ Ontology: A Linked Data Hub for Mathematics
021       In this paper, we present an ontology of mathematical knowledge concepts that covers a wide range of the fields of mathematics and introduces a balanced representation between comprehensive and sensible models. 
020       We demonstrate the applications of this representation in information extraction, semantic search, and education. We argue that the ontology can be a core of future integration of math-aware data sets in the Web of Data and, therefore, provide mappings onto relevant datasets, such as DBpedia and ScienceWISE. 
007       rubr.cs.AI
007       rubr.cs
007       rubr.cs.DL
007       rubr.cs.IR
007       name.Olga Nevzorova
007       name.Nikita Zhiltsov
007       name.Alexander Kirillovich
007       name.Evgeny Lipachev
040       2014.08.11 06:54
050       http://arxiv.org/abs/1407.4833
*** 6439
010       Feature Engineering for Knowledge Base Construction
021       Knowledge base construction (KBC) is the process of populating a knowledge base, i.e., a relational database together with inference rules, with information extracted from documents and structured sources. 
020       KBC blurs the distinction between two traditional database problems, information extraction and information integration. For the last several years, our group has been building knowledge bases with scientific collaborators. Using our approach, we have built knowledge bases that have comparable and sometimes better quality than those constructed by human volunteers. In contrast to these knowledge bases, which took experts a decade or more human years to construct, many of our projects are constructed by a single graduate student. <br />Our approach to KBC is based on joint probabilistic inference and learning, but we do not see inference as either a panacea or a magic bullet: inference is a tool that allows us to be systematic in how we construct, debug, and improve the quality of such systems. In addition, inference allows us to construct these systems in a more loosely coupled way than traditional approaches. To support this idea, we have built the DeepDive system, which has the design goal of letting the user "think about features---not algorithms." We think of DeepDive as declarative in that one specifies what they want but not how to get it. We describe our approach with a focus on feature engineering, which we argue is an understudied problem relative to its importance to end-to-end quality. 
007       rubr.cs.DB
007       rubr.cs
007       rubr.cs.CL
007       rubr.cs.LG
007       name.Christopher R&#xe9;
007       name.Amir Abbas Sadeghian
007       name.Zifei Shan
007       name.Jaeho Shin
007       name.Feiran Wang
007       name.Sen Wu
007       name.Ce Zhang
040       2014.09.18 14:38
050       http://arxiv.org/abs/1407.6439
*** 1260
010       Unstable markup: A template-based information extraction from web sites   with unstable markup
021       This paper presents results of a work on crawling CEUR Workshop proceedings web site to a Linked Open Data (LOD) dataset in the framework of ESWC 2014 Semantic Publishing Challenge 2014. 
020       Our approach is based on using an extensible template-dependent crawler and DBpedia for linking extracted entities, such as the names of universities and countries. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.DL
007       name.Maxim Kolchin
007       name.Fedor Kozlov
040       2014.08.06 12:36
050       http://arxiv.org/abs/1408.1260
*** 1928
010       Microtask crowdsourcing for disease mention annotation in PubMed   abstracts
021       Identifying concepts and relationships in biomedical text enables knowledge to be applied in computational analyses. 
020       Many biological natural language process (BioNLP) projects attempt to address this challenge, but the state of the art in BioNLP still leaves much room for improvement. Progress in BioNLP research depends on large, annotated corpora for evaluating information extraction systems and training machine learning models. Traditionally, such corpora are created by small numbers of expert annotators often working over extended periods of time. Recent studies have shown that workers on microtask crowdsourcing platforms such as Amazon's Mechanical Turk (AMT) can, in aggregate, generate high-quality annotations of biomedical text. Here, we investigated the use of the AMT in capturing disease mentions in PubMed abstracts. We used the NCBI Disease corpus as a gold standard for refining and benchmarking our crowdsourcing protocol. After several iterations, we arrived at a protocol that reproduced the annotations of the 593 documents in the training set of this gold standard with an overall F measure of 0.872 (precision 0.862, recall 0.883). The output can also be tuned to optimize for precision (max = 0.984 when recall = 0.269) or recall (max = 0.980 when precision = 0.436). Each document was examined by 15 workers, and their annotations were merged based on a simple voting method. In total 145 workers combined to complete all 593 documents in the span of 1 week at a cost of $.06 per abstract per worker. The quality of the annotations, as judged with the F measure, increases with the number of workers assigned to each task such that the system can be tuned to balance cost against quality. These results demonstrate that microtask crowdsourcing can be a valuable tool for generating well-annotated corpora in BioNLP. 
007       rubr.cs.CL
007       name.Benjamin M Good
007       name.Max Nanis
007       name.Andrew I. Su
040       2014.08.08 17:49
050       http://arxiv.org/abs/1408.1928
*** 5894
010       Location Estimation Using Crowdsourced Geospatial Narratives
021       The "crowd" has become a very important geospatial data provider. 
020       Subsumed under the term Volunteered Geographic Information (VGI), non-expert users have been providing a wealth of quantitative geospatial data online. With spatial reasoning being a basic form of human cognition, narratives expressing geospatial experiences, e.g., travel blogs, would provide an even bigger source of geospatial data. Textual narratives typically contain qualitative data in the form of objects and spatial relationships. The scope of this work is (i) to extract these relationships from user-generated texts, (ii) to quantify them and (iii) to reason about object locations based only on this qualitative data. We use information extraction methods to identify toponyms and spatial relationships and to formulate a quantitative approach based on distance and orientation features to represent the latter. Positional probability distributions for spatial relationships are determined by means of a greedy Expectation Maximization-based (EM) algorithm. These estimates are then used to "triangulate" the positions of unknown object locations. Experiments using a text corpus harvested from travel blog sites establish the considerable location estimation accuracy of the proposed approach. 
007       rubr.cs.DB
007       name.Georgios Skoumas
007       name.Dieter Pfoser
007       name.Anastasios Kyrillidis
040       2014.08.25 15:05
050       http://arxiv.org/abs/1408.5894
*** 3899
010       Methods for Joint Imaging and RNA-seq Data Analysis
021       Emerging integrative analysis of genomic and anatomical imaging data which has not been well developed, provides invaluable information for the holistic discovery of the genomic structure of disease and has the potential to open a new avenue for discovering novel disease susceptibility genes which cannot be identified if they are analyzed separately. 
020       A key issue to the success of imaging and genomic data analysis is how to reduce their dimensions. Most previous methods for imaging information extraction and RNA-seq data reduction do not explore imaging spatial information and often ignore gene expression variation at genomic positional level. To overcome these limitations, we extend functional principle component analysis from one dimension to two dimension (2DFPCA) for representing imaging data and develop a multiple functional linear model (MFLM) in which functional principal scores of images are taken as multiple quantitative traits and RNA-seq profile across a gene is taken as a function predictor for assessing the association of gene expression with images. The developed method has been applied to image and RNA-seq data of ovarian cancer and KIRC studies. We identified 24 and 84 genes whose expressions were associated with imaging variations in ovarian cancer and KIRC studies, respectively. Our results showed that many significantly associated genes with images were not differentially expressed, but revealed their morphological and metabolic functions. The results also demonstrated that the peaks of the estimated regression coefficient function in the MFLM often allowed the discovery of splicing sites and multiple isoform of gene expressions. 
007       rubr.q-bio.GN
007       name.Junhai Jiang
007       name.Nan Lin
007       name.Shicheng Guo
007       name.Jinyun Chen
007       name.Momiao Xiong
040       2014.09.13 02:05
050       http://arxiv.org/abs/1409.3899
*** 7182
010       Analysis of Named Entity Recognition and Linking for Tweets
021       Applying natural language processing for mining and intelligent information access to tweets (a form of microblog) is a challenging, emerging research area. 
020       Unlike carefully authored news text and other longer content, tweets pose a number of new challenges, due to their short, noisy, context-dependent, and dynamic nature. Information extraction from tweets is typically performed in a pipeline, comprising consecutive stages of language identification, tokenisation, part-of-speech tagging, named entity recognition and entity disambiguation (e.g. with respect to DBpedia). In this work, we describe a new Twitter entity disambiguation dataset, and conduct an empirical analysis of named entity recognition and disambiguation, investigating how robust a number of state-of-the-art systems are on such noisy texts, what the main sources of error are, and which problems should be further investigated to improve the state of the art. 
007       rubr.cs.CL
007       name.Leon Derczynski
007       name.Diana Maynard
007       name.Giuseppe Rizzo
007       name.Marieke van Erp
007       name.Genevieve Gorrell
007       name.Rapha&#xeb;l Troncy
007       name.Johann Petrak
007       name.Kalina Bontcheva
040       2014.10.27 11:09
050       http://arxiv.org/abs/1410.7182
*** 7
010       Rapid Adaptation of POS Tagging for Domain Specific Uses
021       Part-of-speech (POS) tagging is a fundamental component for performing natural language tasks such as parsing, information extraction, and question answering. 
020       When POS taggers are trained in one domain and applied in significantly different domains, their performance can degrade dramatically. We present a methodology for rapid adaptation of POS taggers to new domains. Our technique is unsupervised in that a manually annotated corpus for the new domain is not necessary. We use suffix information gathered from large amounts of raw text as well as orthographic information to increase the lexical coverage. We present an experiment in the Biological domain where our POS tagger achieves results comparable to POS taggers specifically trained to this domain. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.LG
007       rubr.cs.LG
007       rubr.stat.ML
007       name.John E. Miller
007       name.Michael Bloodgood
007       name.Manabu Torii
007       name.K. Vijay-Shanker
040       2014.10.31 20:04
050       http://arxiv.org/abs/1411.0007
*** 686
010       Practical variational tomography for critical 1D systems
021       We improve upon a recently introduced efficient quantum state reconstruction procedure targeted to states well-approximated by the multi-scale entanglement renormalization ansatz (MERA), e.g., ground states of critical models. 
020       We show how to numerically select a subset of experimentally accessible measurements which maximizes information extraction about renormalized particles, thus dramatically reducing the required number of physical measurements. We numerically estimate the number of measurements required to characterize the ground state of the critical 1D Ising (resp. XX) model and find that MERA tomography on 16-qubit (resp. 24-qubit) systems requires the same experimental effort than brute-force tomography on 8 qubits. We derive a bound computable from experimental data which certifies the distance between the experimental and reconstructed states. 
007       rubr.quant-ph
007       name.Jong Yeon Lee
007       name.Olivier Landon-Cardinal
040       2014.12.01 21:16
050       http://arxiv.org/abs/1412.0686
*** 3997
010       Holistic random encoding for imaging through multimode fibers
021       The input numerical aperture (NA) of multimode fiber (MMF) can be effectively increased by placing turbid media at the input end of the MMF. 
020       This provides the potential for high-resolution imaging through the MMF. While the input NA is increased, the number of propagation modes in the MMF and hence the output NA remains the same. This makes the image reconstruction process underdetermined and may limit the quality of the image reconstruction. In this paper, we aim to improve the signal to noise ratio (SNR) of the image reconstruction in imaging through MMF. We notice that turbid media placed in the input of the MMF transforms the incoming waves into a better format for information transmission and information extraction. We call this transformation as holistic random (HR) encoding of turbid media. By exploiting the HR encoding, we make a considerable improvement on the SNR of the image reconstruction. For efficient utilization of the HR encoding, we employ sparse representation (SR), a relatively new signal reconstruction framework when it is provided with a HR encoded signal. This study shows for the first time to our knowledge the benefit of utilizing the HR encoding of turbid media for recovery in the optically underdetermined systems where the output NA of it is smaller than the input NA for imaging through MMF. 
007       rubr.physics.optics
007       rubr.cs
007       rubr.cs.CV
007       name.Hwanchol Jang
007       name.Changhyeong Yoon
007       name.Euiheon Chung
007       name.Wonshik Choi
007       name.Heung-No Lee
040       2014.12.30 10:27
050       http://arxiv.org/abs/1501.03997
*** 3519
010       Knowledge-Based Trust: Estimating the Trustworthiness of Web Sources
021       The quality of web sources has been traditionally evaluated using exogenous signals such as the hyperlink structure of the graph. 
020       We propose a new approach that relies on endogenous signals, namely, the correctness of factual information provided by the source. A source that has few false facts is considered to be trustworthy. The facts are automatically extracted from each source by information extraction methods commonly used to construct knowledge bases. We propose a way to distinguish errors made in the extraction process from factual errors in the web source per se, by using joint inference in a novel multi-layer probabilistic model. We call the trustworthiness score we computed Knowledge-Based Trust (KBT). On synthetic data, we show that our method can reliably compute the true trustworthiness levels of the sources. We then apply it to a database of 2.8B facts extracted from the web, and thereby estimate the trustworthiness of 119M webpages. Manual evaluation of a subset of the results confirms the effectiveness of the method. 
007       rubr.cs.DB
007       rubr.cs
007       rubr.cs.IR
007       name.Xin Luna Dong
007       name.Evgeniy Gabrilovich
007       name.Kevin Murphy
007       name.Van Dang
007       name.Wilko Horn
007       name.Camillo Lugaresi
007       name.Shaohua Sun
007       name.Wei Zhang
040       2015.02.12 02:45
050       http://arxiv.org/abs/1502.03519
*** 4049
010       How essential are unstructured clinical narratives and information   fusion to clinical trial recruitment?
021       Electronic health records capture patient information using structured controlled vocabularies and unstructured narrative text. 
020       While structured data typically encodes lab values, encounters and medication lists, unstructured data captures the physician's interpretation of the patient's condition, prognosis, and response to therapeutic intervention. In this paper, we demonstrate that information extraction from unstructured clinical narratives is essential to most clinical applications. We perform an empirical study to validate the argument and show that structured data alone is insufficient in resolving eligibility criteria for recruiting patients onto clinical trials for chronic lymphocytic leukemia (CLL) and prostate cancer. Unstructured data is essential to solving 59% of the CLL trial criteria and 77% of the prostate cancer trial criteria. More specifically, for resolving eligibility criteria with temporal constraints, we show the need for temporal reasoning and information integration with medical events within and across unstructured clinical narratives and structured data. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.CY
007       name.Preethi Raghavan
007       name.James L. Chen
007       name.Eric Fosler-Lussier
007       name.Albert M. Lai
040       2015.02.13 16:28
050       http://arxiv.org/abs/1502.04049
*** 5472
010       On the Effects of Low-Quality Training Data on Information Extraction   from Clinical Reports
021       In the last five years there has been a flurry of work on information extraction from clinical documents, i.e., on algorithms capable of extracting, from the informal and unstructured texts that are generated during everyday clinical practice, mentions of concepts relevant to such practice. 
020       Most of this literature is about methods based on supervised learning, i.e., methods for training an information extraction system from manually annotated examples. While a lot of work has been devoted to devising learning methods that generate more and more accurate information extractors, no work has been devoted to investigating the effect of the quality of training data on the learning process. Low quality in training data often derives from the fact that the person who has annotated the data is different from the one against whose judgment the automatically annotated data must be evaluated. In this paper we test the impact of such data quality issues on the accuracy of information extraction systems as applied to the clinical domain. We do this by comparing the accuracy deriving from training data annotated by the authoritative coder (i.e., the one who has also annotated the test data, and by whose judgment we must abide), with the accuracy deriving from training data annotated by a different coder. The results indicate that, although the disagreement between the two coders (as measured on the training set) is substantial, the difference is (surprisingly enough) not always statistically significant. 
007       rubr.cs.LG
007       rubr.cs
007       rubr.cs.CL
007       rubr.cs.IR
007       name.Diego Marcheggiani
007       name.Fabrizio Sebastiani
040       2015.03.04 08:08
050       http://arxiv.org/abs/1502.05472
*** 759
010       A Review of Relational Machine Learning for Knowledge Graphs: From   Multi-Relational Link Prediction to Automated Knowledge Graph Construction
021       Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. 
020       In this paper, we provide a review of how such statistical models can be "trained" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on tensor factorization methods and related latent variable models. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. In particular, we discuss Google's Knowledge Vault project. 
007       rubr.stat.ML
007       rubr.cs
007       rubr.cs.LG
007       name.Maximilian Nickel
007       name.Kevin Murphy
007       name.Volker Tresp
007       name.Evgeniy Gabrilovich
040       2015.03.02 21:35
050       http://arxiv.org/abs/1503.00759
*** 1549
010       Visualization of Clandestine Labs from Seizure Reports: Thematic Mapping   and Data Mining Research Directions
021       The problem of spatiotemporal event visualization based on reports entails subtasks ranging from named entity recognition to relationship extraction and mapping of events. 
020       We present an approach to event extraction that is driven by data mining and visualization goals, particularly thematic mapping and trend analysis. This paper focuses on bridging the information extraction and visualization tasks and investigates topic modeling approaches. We develop a static, finite topic model and examine the potential benefits and feasibility of extending this to dynamic topic modeling with a large number of topics and continuous time. We describe an experimental test bed for event mapping that uses this end-to-end information retrieval system, and report preliminary results on a geoinformatics problem: tracking of methamphetamine lab seizure events across time and space. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       name.William Hsu
007       name.Mohammed Abduljabbar
007       name.Ryuichi Osuga
007       name.Max Lu
007       name.Wesam Elshamy
040       2015.03.05 06:22
050       http://arxiv.org/abs/1503.01549
*** 2235
010       Extraction of Protein Sequence Motif Information using PSO K-Means
021       The main objective of the paper is to find the motif information.The functionalities of the proteins are ideally found from their motif information which is extracted using various techniques like clustering with k-means, hybrid k-means, self-organising maps, etc., in the literature. 
020       In this work protein sequence information is extracted using optimised k-means algorithm. The particle swarm optimisation technique is one of the frequently used optimisation method. In the current work the PSO k-means is used for motif information extraction. This paper also deals with the comparison between the motif information obtained from clusters and biclustersusing PSO k-means algorithm. The motif information acquired is based on the structure homogeneity of the protein sequence. 
007       rubr.cs.CV
007       name.R. Gowri
007       name.R. Rathipriya
040       2015.04.09 09:53
050       http://arxiv.org/abs/1504.02235
*** 2523
010       Clustering RDF Databases Using Tunable-LSH
021       The Resource Description Framework (RDF) is a W3C standard for representing graph-structured data, and SPARQL is the standard query language for RDF. 
020       Recent advances in Information Extraction, Linked Data Management and the Semantic Web have led to a rapid increase in both the volume and the variety of RDF data that are publicly available. As businesses start to capitalize on RDF data, RDF data management systems are being exposed to workloads that are far more diverse and dynamic than what they were designed to handle. Consequently, there is a growing need for developing workload-adaptive and self-tuning RDF data management systems. To realize this vision, we introduce a fast and efficient method for dynamically clustering records in an RDF data management system. Specifically, we assume nothing about the workload upfront, but as SPARQL queries are executed, we keep track of records that are co-accessed by the queries in the workload and physically cluster them. To decide dynamically (hence, in constant-time) where a record needs to be placed in the storage system, we develop a new locality-sensitive hashing (LSH) scheme, Tunable-LSH. Using Tunable-LSH, records that are co-accessed across similar sets of queries can be hashed to the same or nearby physical pages in the storage system. What sets Tunable-LSH apart from existing LSH schemes is that it can auto-tune to achieve the aforementioned clustering objective with high accuracy even when the workloads change. Experimental evaluation of Tunable-LSH in our prototype RDF data management system, chameleon-db, as well as in a standalone hashtable shows significant end-to-end improvements over existing solutions. 
007       rubr.cs.DB
007       name.G&#xfc;ne&#x15f; Alu&#xe7;
007       name.M. Tamer &#xd6;zsu
007       name.Khuzaima Daudjee
040       2015.04.19 01:04
050       http://arxiv.org/abs/1504.02523
*** 6080
010       svcR: An R Package for Support Vector Clustering improved with Geometric   Hashing applied to Lexical Pattern Discovery
021       We present a new R package which takes a numerical matrix format as data input, and computes clusters using a support vector clustering method (SVC). 
020       We have implemented an original 2D-grid labeling approach to speed up cluster extraction. In this sense, SVC can be seen as an efficient cluster extraction if clusters are separable in a 2-D map. Secondly we showed that this SVC approach using a Jaccard-Radial base kernel can help to classify well enough a set of terms into ontological classes and help to define regular expression rules for information extraction in documents; our case study concerns a set of terms and documents about developmental and molecular biology. 
007       rubr.cs.LG
007       rubr.cs
007       rubr.cs.CL
007       name.Nicolas Turenne
040       2015.04.23 08:29
050       http://arxiv.org/abs/1504.06080
*** 1303
010       XTreePath: A generalization of XPath to handle real world structural   variation
021       We discuss a key problem in information extraction which deals with wrapper failures due to changing content templates. 
020       A good proportion of wrapper failures are due to HTML templates changing to cause wrappers to become incompatible after element inclusion or removal in a DOM (Tree representation of HTML). We perform a large-scale empirical analyses of the causes of shift and mathematically quantify the levels of domain difficulty based on entropy. We propose the XTreePath annotation method to captures contextual node information from the training DOM. We then utilize this annotation in a supervised manner at test time with our proposed Recursive Tree Matching method which locates nodes most similar in context recursively using the tree edit distance. The search is based on a heuristic function that takes into account the similarity of a tree compared to the structure that was present in the training data. We evaluate XTreePath using 117,422 pages from 75 diverse websites in 8 vertical markets. Our XTreePath method consistently outperforms XPath and a current commercial system in terms of successful extractions in a blackbox test. We make our code and datasets publicly available online. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.DB
007       name.Joseph Paul Cohen
007       name.Wei Ding
007       name.Abraham Bagherjeiran
040       2017.12.27 01:31
050       http://arxiv.org/abs/1505.01303
*** 3823
010       Distant Supervision for Entity Linking
021       Entity linking is an indispensable operation of populating knowledge repositories for information extraction. 
020       It studies on aligning a textual entity mention to its corresponding disambiguated entry in a knowledge repository. In this paper, we propose a new paradigm named distantly supervised entity linking (DSEL), in the sense that the disambiguated entities that belong to a huge knowledge repository (Freebase) are automatically aligned to the corresponding descriptive webpages (Wiki pages). In this way, a large scale of weakly labeled data can be generated without manual annotation and fed to a classifier for linking more newly discovered entities. Compared with traditional paradigms based on solo knowledge base, DSEL benefits more via jointly leveraging the respective advantages of Freebase and Wikipedia. Specifically, the proposed paradigm facilitates bridging the disambiguated labels (Freebase) of entities and their textual descriptions (Wikipedia) for Web-scale entities. Experiments conducted on a dataset of 140,000 items and 60,000 features achieve a baseline F1-measure of 0.517. Furthermore, we analyze the feature performance and improve the F1-measure to 0.545. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       name.Miao Fan
007       name.Qiang Zhou
007       name.Thomas Fang Zheng
040       2015.08.05 01:25
050       http://arxiv.org/abs/1505.03823
*** 6537
010       A survey of SMS based Information Systems
021       Short Message Service (SMS) based Information Systems (SMSbIS) provide an excellent alternative to a traditional approach of obtaining specific information by direct (through phone) or indirect (IVRS, Web, Email) probing. 
020       Information and communication technology and far reaching mobile penetration has opened this new research trend Number of key players in Search industry including Microsoft and Google are attracted by the expected increase in volume of use of such applications. The wide range of applications and their public acceptance has motivated researchers to work in this research domain. Several applications such as SMS based information access using database management services, SMS based information retrieval through internet (search engine), SMS based information extraction, question answering, image retrieval etc. have been emerged. With the aim to understand the functionality involved in these systems, an extensive review of a few of these SMSbISs has been planned and executed by us. These systems are classified into four categories based on the objectives and domains of the applications. As a result of this study a well structured functional model is presented here. The model is evaluated in different dimensions, which is presented in this paper. In addition to this a chronological progress with respect to research and development in this upcoming field is compiled in this paper. Such an extensive review presented in this paper would definitely help the researchers and developers to understand the technical aspects of this field. The functional framework presented here would be useful to the system designers to design and develop an SMS based Information System of any specific domain. 
007       rubr.cs.AI
007       rubr.cs
007       rubr.cs.IR
007       name.Manish R. Joshi
007       name.Varsha M. Pathak
040       2015.05.22 10:08
050       http://arxiv.org/abs/1505.06537
*** 6418
010       Extreme Extraction: Only One Hour per Relation
021       Information Extraction (IE) aims to automatically generate a large knowledge base from natural language text, but progress remains slow. 
020       Supervised learning requires copious human annotation, while unsupervised and weakly supervised approaches do not deliver competitive accuracy. As a result, most fielded applications of IE, as well as the leading TAC-KBP systems, rely on significant amounts of manual engineering. Even "Extreme" methods, such as those reported in Freedman et al. 2011, require about 10 hours of expert labor per relation. <br />This paper shows how to reduce that effort by an order of magnitude. We present a novel system, InstaRead, that streamlines authoring with an ensemble of methods: 1) encoding extraction rules in an expressive and compositional representation, 2) guiding the user to promising rules based on corpus statistics and mined resources, and 3) introducing a new interactive development cycle that provides immediate feedback --- even on large datasets. Experiments show that experts can create quality extractors in under an hour and even NLP novices can author good extractors. These extractors equal or outperform ones obtained by comparably supervised and state-of-the-art distantly supervised approaches. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.IR
007       name.Raphael Hoffmann
007       name.Luke Zettlemoyer
007       name.Daniel S. Weld
040       2015.06.21 22:04
050       http://arxiv.org/abs/1506.06418
*** 7116
010       Scientific Discovery by Machine Intelligence: A New Avenue for Drug   Research
021       The majority of big data is unstructured and of this majority the largest chunk is text. 
020       While data mining techniques are well developed and standardized for structured, numerical data, the realm of unstructured data is still largely unexplored. The general focus lies on information extraction, which attempts to retrieve known information from text. The Holy Grail, however is knowledge discovery, where machines are expected to unearth entirely new facts and relations that were not previously known by any human expert. Indeed, understanding the meaning of text is often considered as one of the main characteristics of human intelligence. The ultimate goal of semantic artificial intelligence is to devise software that can understand the meaning of free text, at least in the practical sense of providing new, actionable information condensed out of a body of documents. As a stepping stone on the road to this vision I will introduce a totally new approach to drug research, namely that of identifying relevant information by employing a self-organizing semantic engine to text mine large repositories of biomedical research papers, a technique pioneered by Merck with the InfoCodex software. I will describe the methodology and a first successful experiment for the discovery of new biomarkers and phenotypes for diabetes and obesity on the basis of PubMed abstracts, public clinical trials and Merck internal documents. The reported approach shows much promise and has potential to impact fundamentally pharmaceutical research as a way to shorten time-to-market of novel drugs, and for early recognition of dead ends. 
007       rubr.cs.AI
007       rubr.cs
007       rubr.cs.IR
007       name.Carlo A. Trugenberger
040       2015.06.23 18:04
050       http://arxiv.org/abs/1506.07116
*** 8454
010       WYSIWYE: An Algebra for Expressing Spatial and Textual Rules for Visual   Information Extraction
021       The visual layout of a webpage can provide valuable clues for certain types of Information Extraction (IE) tasks. 
020       In traditional rule based IE frameworks, these layout cues are mapped to rules that operate on the HTML source of the webpages. In contrast, we have developed a framework in which the rules can be specified directly at the layout level. This has many advantages, since the higher level of abstraction leads to simpler extraction rules that are largely independent of the source code of the page, and, therefore, more robust. It can also enable specification of new types of rules that are not otherwise possible. To the best of our knowledge, there is no general framework that allows declarative specification of information extraction rules based on spatial layout. Our framework is complementary to traditional text based rules framework and allows a seamless combination of spatial layout based rules with traditional text based rules. We describe the algebra that enables such a system and its efficient implementation using standard relational and text indexing features of a relational database. We demonstrate the simplicity and efficiency of this system for a task involving the extraction of software system requirements from software product pages. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.DB
007       rubr.cs.IR
007       name.Vijil Chenthamarakshan
007       name.Prasad M Desphande
007       name.Raghu Krishnapuram
007       name.Ramakrishna Varadarajan
007       name.Knut Stolze
040       2016.09.27 19:49
050       http://arxiv.org/abs/1506.08454
*** 87
010       Information Extraction from Larger Multi-layer Social Networks
021       Social networks often encode community structure using multiple distinct types of links between nodes. 
020       In this paper we introduce a novel method to extract information from such multi-layer networks, where each type of link forms its own layer. Using the concept of Pareto optimality, community detection in this multi-layer setting is formulated as a multiple criterion optimization problem. We propose an algorithm for finding an approximate Pareto frontier containing a family of solutions. The power of this approach is demonstrated on a Twitter dataset, where the nodes are hashtags and the layers correspond to (1) behavioral edges connecting pairs of hashtags whose temporal profiles are similar and (2) relational edges connecting pairs of hashtags that appear in the same tweets. 
007       rubr.cs.SI
007       rubr.cs
007       rubr.physics
007       rubr.physics.soc-ph
007       name.Brandon Oselio
007       name.Alex Kulesza
007       name.Alfred Hero
040       2015.07.01 01:50
050       http://arxiv.org/abs/1507.00087
*** 2447
010       Data Mining of Causal Relations from Text: Analysing Maritime Accident   Investigation Reports
021       Text mining is a process of extracting information of interest from text. 
020       Such a method includes techniques from various areas such as Information Retrieval (IR), Natural Language Processing (NLP), and Information Extraction (IE). In this study, text mining methods are applied to extract causal relations from maritime accident investigation reports collected from the Marine Accident Investigation Branch (MAIB). These causal relations provide information on various mechanisms behind accidents, including human and organizational factors relating to the accident. The objective of this study is to facilitate the analysis of the maritime accident investigation reports, by means of extracting contributory causes with more feasibility. A careful investigation of contributory causes from the reports provide opportunity to improve safety in future. <br />Two methods have been employed in this study to extract the causal relations. They are 1) Pattern classification method and 2) Connectives method. The earlier one uses naive Bayes and Support Vector Machines (SVM) as classifiers. The latter simply searches for the words connecting cause and effect in sentences. <br />The causal patterns extracted using these two methods are compared to the manual (human expert) extraction. The pattern classification method showed a fair and sensible performance with F-measure(average) = 65% when compared to connectives method with F-measure(average) = 58%. This study is an evidence, that text mining methods could be employed in extracting causal relations from marine accident investigation reports. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       name.Santosh Tirunagari
040       2015.07.09 10:20
050       http://arxiv.org/abs/1507.02447
*** 6529
010       Growing homophilic networks are natural navigable small worlds
021       Navigability, an ability to find a logarithmically short path between elements using only local information, is one of the most fascinating properties of real-life networks. 
020       However, the exact mechanism responsible for the formation of navigation properties remained unknown. We show that navigability can be achieved by using only two ingredients present in the majority of networks: network growth and local homophily, giving a persuasive answer how the navigation appears in real-life networks. A very simple algorithm produces hierarchical self-similar optimally wired navigable small world networks with exponential degree distribution by using only local information. Adding preferential attachment produces a scale-free network which has shorter greedy paths, but worse (power law) scaling of the information extraction locality (algorithmic complexity of a search). Introducing saturation of the preferential attachment leads to truncated scale-free degree distribution that offers a good tradeoff between these parameters and can be useful for practical applications. Several features of the model are observed in real-life networks, in particular in the brain neural networks, supporting the earlier suggestions that they are navigable. 
007       rubr.physics.soc-ph
007       rubr.cs
007       rubr.cs.SI
007       name.Yury A. Malkov
007       name.Alexander Ponomarenko
040       2016.03.02 18:31
050       http://arxiv.org/abs/1507.06529
*** 1349
010       Automatic classification of bengali sentences based on sense definitions   present in bengali wordnet
021       Based on the sense definition of words available in the Bengali WordNet, an attempt is made to classify the Bengali sentences automatically into different groups in accordance with their underlying senses. 
020       The input sentences are collected from 50 different categories of the Bengali text corpus developed in the TDIL project of the Govt. of India, while information about the different senses of particular ambiguous lexical item is collected from Bengali WordNet. In an experimental basis we have used Naive Bayes probabilistic model as a useful classifier of sentences. We have applied the algorithm over 1747 sentences that contain a particular Bengali lexical item which, because of its ambiguous nature, is able to trigger different senses that render sentences in different meanings. In our experiment we have achieved around 84% accurate result on the sense classification over the total input sentences. We have analyzed those residual sentences that did not comply with our experiment and did affect the results to note that in many cases, wrong syntactic structures and less semantic information are the main hurdles in semantic classification of sentences. The applicational relevance of this study is attested in automatic text classification, machine learning, information extraction, and word sense disambiguation. 
007       rubr.cs.CL
007       name.Alok Ranjan Pal
007       name.Diganta Saha
007       name.Niladri Sekhar Dash
040       2015.08.06 10:26
050       http://arxiv.org/abs/1508.01349
*** 3116
010       Query-Driven Sampling for Collective Entity Resolution
021       Probabilistic databases play a preeminent role in the processing and management of uncertain data. 
020       Recently, many database research efforts have integrated probabilistic models into databases to support tasks such as information extraction and labeling. Many of these efforts are based on batch oriented inference which inhibits a realtime workflow. One important task is entity resolution (ER). ER is the process of determining records (mentions) in a database that correspond to the same real-world entity. Traditional pairwise ER methods can lead to inconsistencies and low accuracy due to localized decisions. Leading ER systems solve this problem by collectively resolving all records using a probabilistic graphical model and Markov chain Monte Carlo (MCMC) inference. However, for large datasets this is an extremely expensive process. One key observation is that, such exhaustive ER process incurs a huge up-front cost, which is wasteful in practice because most users are interested in only a small subset of entities. In this paper, we advocate pay-as-you-go entity resolution by developing a number of query-driven collective ER techniques. We introduce two classes of SQL queries that involve ER operators --- selection-driven ER and join-driven ER. We implement novel variations of the MCMC Metropolis Hastings algorithm to generate biased samples and selectivity-based scheduling algorithms to support the two classes of ER queries. Finally, we show that query-driven ER algorithms can converge and return results within minutes over a database populated with the extraction from a newswire dataset containing 71 million mentions. 
007       rubr.cs.DB
007       name.Christan Grant
007       name.Daisy Zhe Wang
007       name.Michael L. Wick
040       2015.08.13 04:23
050       http://arxiv.org/abs/1508.03116
*** 6206
010       Semantic Publishing Challenge - Assessing the Quality of Scientific   Output by Information Extraction and Interlinking
021       The Semantic Publishing Challenge series aims at investigating novel approaches for improving scholarly publishing using Linked Data technology. 
020       In 2014 we had bootstrapped this effort with a focus on extracting information from non-semantic publications - computer science workshop proceedings volumes and their papers - to assess their quality. The objective of this second edition was to improve information extraction but also to interlink the 2014 dataset with related ones in the LOD Cloud, thus paving the way for sophisticated end-user services. 
007       rubr.cs.DL
007       name.Angelo Di Iorio
007       name.Christoph Lange
007       name.Anastasia Dimou
007       name.Sahar Vahdati
040       2015.08.25 16:17
050       http://arxiv.org/abs/1508.06206
*** 1653
010       Millimeter Wave Energy Harvesting
021       The millimeter wave (mmWave) band, which is a prime candidate for 5G cellular networks, seems attractive for wireless energy harvesting. 
020       This is because it will feature large antenna arrays as well as extremely dense base station (BS) deployments. The viability of mmWave for energy harvesting though is unclear, due to the differences in propagation characteristics such as extreme sensitivity to building blockages. This paper considers a scenario where low-power devices extract energy and/or information from the mmWave signals. Using stochastic geometry, analytical expressions are derived for the energy coverage probability, the average harvested power, and the overall (energy-and-information) coverage probability at a typical wireless-powered device in terms of the BS density, the antenna geometry parameters, and the channel parameters. Numerical results reveal several network and device level design insights. At the BSs, optimizing the antenna geometry parameters such as beamwidth can maximize the network-wide energy coverage for a given user population. At the device level, the performance can be substantially improved by optimally splitting the received signal for energy and information extraction, and by deploying multi-antenna arrays. For the latter, an efficient low-power multi-antenna mmWave receiver architecture is proposed for simultaneous energy and information transfer. Overall, simulation results suggest that mmWave energy harvesting generally outperforms lower frequency solutions. 
007       rubr.cs.IT
007       rubr.cs
007       name.Talha Ahmed Khan
007       name.Ahmed Alkhateeb
007       name.Robert W. Heath Jr
040       2017.03.11 00:28
050       http://arxiv.org/abs/1509.01653
*** 1964
010       Network comparison using directed graphlets
021       With recent advances in high-throughput cell biology the amount of cellular biological data has grown drastically. 
020       Such data is often modeled as graphs (also called networks) and studying them can lead to new insights into molecule-level organization. A possible way to understand their structure is by analysing the smaller components that constitute them, namely network motifs and graphlets. Graphlets are particularly well suited to compare networks and to assess their level of similarity but are almost always used as small undirected graphs of up to five nodes, thus limiting their applicability in directed networks. However, a large set of interesting biological networks such as metabolic, cell signaling or transcriptional regulatory networks are intrinsically directional, and using metrics that ignore edge direction may gravely hinder information extraction. The applicability of graphlets is extended to directed networks by considering the edge direction of the graphlets. We tested our approach on a set of directed biological networks and verified that they were correctly grouped by type using directed graphlets. However, enumerating all graphlets in a large network is a computationally demanding task. Our implementation addresses this concern by using a state-of-the-art data structure, the g-trie, which is able to greatly reduce the necessary computation. We compared our tool, gtrieScanner, to other state-of-the art methods and verified that it is the fastest general tool for graphlet counting. 
007       rubr.cs.SI
007       rubr.cs
007       rubr.physics
007       rubr.physics.soc-ph
007       rubr.physics.soc-ph
007       rubr.q-bio.MN
007       name.David Apar&#xed;cio
007       name.Pedro Ribeiro
007       name.Fernando Silva
040       2015.11.06 01:05
050       http://arxiv.org/abs/1511.01964
*** 2381
010       Information Extraction Under Privacy Constraints
021       A privacy-constrained information extraction problem is considered where for a pair of correlated discrete random variables $(X,Y)$ governed by a given joint distribution, an agent observes $Y$ and wants to convey to a potentially public user as much information about $Y$ as possible without compromising the amount of information revealed about $X$. 
020       To this end, the so-called {\em rate-privacy function} is introduced to quantify the maximal amount of information (measured in terms of mutual information) that can be extracted from $Y$ under a privacy constraint between $X$ and the extracted information, where privacy is measured using either mutual information or maximal correlation. Properties of the rate-privacy function are analyzed and information-theoretic and estimation-theoretic interpretations of it are presented for both the mutual information and maximal correlation privacy measures. It is also shown that the rate-privacy function admits a closed-form expression for a large family of joint distributions of $(X,Y)$. Finally, the rate-privacy function under the mutual information privacy measure is considered for the case where $(X,Y)$ has a joint probability density function by studying the problem where the extracted information is a uniform quantization of $Y$ corrupted by additive Gaussian noise. The asymptotic behavior of the rate-privacy function is studied as the quantization resolution grows without bound and it is observed that not all of the properties of the rate-privacy function carry over from the discrete to the continuous case. 
007       rubr.cs.IT
007       rubr.cs
007       rubr.math
007       rubr.math.ST
007       rubr.math.ST
007       rubr.stat.ML
007       name.Shahab Asoodeh
007       name.Mario Diaz
007       name.Fady Alajaji
007       name.Tam&#xe1;s Linder
040       2016.01.17 20:39
050       http://arxiv.org/abs/1511.02381
*** 4661
010       A System for Extracting Sentiment from Large-Scale Arabic Social Data
021       Social media data in Arabic language is becoming more and more abundant. 
020       It is a consensus that valuable information lies in social media data. Mining this data and making the process easier are gaining momentum in the industries. This paper describes an enterprise system we developed for extracting sentiment from large volumes of social data in Arabic dialects. First, we give an overview of the Big Data system for information extraction from multilingual social data from a variety of sources. Then, we focus on the Arabic sentiment analysis capability that was built on top of the system including normalizing written Arabic dialects, building sentiment lexicons, sentiment classification, and performance evaluation. Lastly, we demonstrate the value of enriching sentiment results with user profiles in understanding sentiments of a specific user group. 
007       rubr.cs.CL
007       name.Hao Wang
007       name.Vijay R. Bommireddipalli
007       name.Ayman Hanafy
007       name.Mohamed Bahgat
007       name.Sara Noeman
007       name.Ossama S. Emam
040       2015.11.15 05:53
050       http://arxiv.org/abs/1511.04661
*** 5643
010       A New Smooth Approximation to the Zero One Loss with a Probabilistic   Interpretation
021       We examine a new form of smooth approximation to the zero one loss in which learning is performed using a reformulation of the widely used logistic function. 
020       Our approach is based on using the posterior mean of a novel generalized Beta-Bernoulli formulation. This leads to a generalized logistic function that approximates the zero one loss, but retains a probabilistic formulation conferring a number of useful properties. The approach is easily generalized to kernel logistic regression and easily integrated into methods for structured prediction. We present experiments in which we learn such models using an optimization method consisting of a combination of gradient descent and coordinate descent using localized grid search so as to escape from local minima. Our experiments indicate that optimization quality is improved when learning meta-parameters are themselves optimized using a validation set. Our experiments show improved performance relative to widely used logistic and hinge loss methods on a wide variety of problems ranging from standard UC Irvine and libSVM evaluation datasets to product review predictions and a visual information extraction task. We observe that the approach: 1) is more robust to outliers compared to the logistic and hinge losses; 2) outperforms comparable logistic and max margin models on larger scale benchmark problems; 3) when combined with Gaussian- Laplacian mixture prior on parameters the kernelized version of our formulation yields sparser solutions than Support Vector Machine classifiers; and 4) when integrated into a probabilistic structured prediction technique our approach provides more accurate probabilities yielding improved inference and increasing information extraction performance. 
007       rubr.cs.CV
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.IR
007       rubr.cs.LG
007       name.Md Kamrul Hasan
007       name.Christopher J. Pal
040       2015.11.18 02:31
050       http://arxiv.org/abs/1511.05643
*** 2861
010       Zooming in on Quantum Trajectories
021       We propose to use the effect of measurements instead of their number to study the time evolution of quantum systems under monitoring. 
020       This time redefinition acts like a microscope which blows up the inner details of seemingly instantaneous transitions like quantum jumps. In the simple example of a continuously monitored qubit coupled to a heat bath, we show that this procedure provides well defined and simple evolution equations in an otherwise singular strong monitoring limit. We show that there exists anomalous observable localised on sharp transitions which can only be resolved with our new effective time. We apply our simplified description to study the competition between information extraction and dissipation in the evolution of the linear entropy. Finally, we show that the evolution of the new time as a function of the real time is closely related to a stable Levy process of index 1/2. 
007       rubr.quant-ph
007       rubr.math
007       rubr.math-ph
007       name.Michel Bauer
007       name.Denis Bernard
007       name.Antoine Tilloy
040       2015.12.09 14:12
050       http://arxiv.org/abs/1512.02861
*** 6034
010       Ontology-driven Information Extraction
021       Homogeneous unstructured data (HUD) are collections of unstructured documents that share common properties, such as similar layout, common file format, or common domain of values. 
020       Building on such properties, it would be desirable to automatically process HUD to access the main information through a semantic layer -- typically an ontology -- called semantic view. Hence, we propose an ontology-based approach for extracting semantically rich information from HUD, by integrating and extending recent technologies and results from the fields of classical information extraction, table recognition, ontologies, text annotation, and logic programming. Moreover, we design and implement a system, named KnowRex, that has been successfully applied to curriculum vitae in the Europass style to offer a semantic view of them, and be able, for example, to select those which exhibit required skills. 
007       rubr.cs.AI
007       rubr.cs
007       rubr.cs.IR
007       name.Weronika T. Adrian
007       name.Nicola Leone
007       name.Marco Manna
040       2015.12.18 16:56
050       http://arxiv.org/abs/1512.06034
*** 620
010       Distant IE by Bootstrapping Using Lists and Document Structure
021       Distant labeling for information extraction (IE) suffers from noisy training data. 
020       We describe a way of reducing the noise associated with distant IE by identifying coupling constraints between potential instance labels. As one example of coupling, items in a list are likely to have the same label. A second example of coupling comes from analysis of document structure: in some corpora, sections can be identified such that items in the same section are likely to have the same label. Such sections do not exist in all corpora, but we show that augmenting a large corpus with coupling constraints from even a small, well-structured corpus can improve performance substantially, doubling F1 on one task. 
007       rubr.cs.CL
007       name.Lidong Bing
007       name.Mingyang Ling
007       name.Richard C. Wang
007       name.William W. Cohen
040       2016.01.04 19:46
050       http://arxiv.org/abs/1601.00620
*** 269
010       Numerical Atrribute Extraction from Clinical Texts
021       This paper describes about information extraction system, which is an extension of the system developed by team Hitachi for "Disease/Disorder Template filling" task organized by ShARe/CLEF eHealth Evolution Lab 2014. 
020       In this extension module we focus on extraction of numerical attributes and values from discharge summary records and associating correct relation between attributes and values. We solve the problem in two steps. First step is extraction of numerical attributes and values, which is developed as a Named Entity Recognition (NER) model using Stanford NLP libraries. Second step is correctly associating the attributes to values, which is developed as a relation extraction module in Apache cTAKES framework. We integrated Stanford NER model as cTAKES pipeline component and used in relation extraction module. Conditional Random Field (CRF) algorithm is used for NER and Support Vector Machines (SVM) for relation extraction. For attribute value relation extraction, we observe 95% accuracy using NER alone and combined accuracy of 87% with NER and SVM. 
007       rubr.cs.AI
007       name.Sarath P R
007       name.Sunil Mandhan
007       name.Yoshiki Niwa
040       2016.01.31 15:58
050       http://arxiv.org/abs/1602.00269
*** 467
010       Differential network analysis and graph classification: a glocal   approach
021       Based on the glocal HIM metric and its induced graph kernel, we propose a novel solution in differential network analysis that integrates network comparison and classification tasks. 
020       The HIM distance is defined as the one-parameter family of product metrics linearly combining the normalised Hamming distance H and the normalised Ipsen-Mikhailov spectral distance IM. The combination of the two components within a single metric allows overcoming their drawbacks and obtaining a measure that is simultaneously global and local. Furthermore, plugging the HIM kernel into a Support Vector Machine gives us a classification algorithm based on the HIM distance. First, we outline the theory underlying the metric construction. We introduce two diverse applications of the HIM distance and the HIM kernel to biological datasets. This versatility supports the adoption of the HIM family as a general tool for information extraction, quantifying difference among diverse in- stances of a complex system. An Open Source implementation of the HIM metrics is provided by the R package nettols and in its web interface ReNette. 
007       rubr.q-bio.MN
007       rubr.q-bio
007       rubr.q-bio.QM
007       name.Giuseppe Jurman
007       name.Michele Filosi
007       name.Samantha Riccadonna
007       name.Roberto Visintainer
007       name.Cesare Furlanello
040       2016.02.01 10:45
050       http://arxiv.org/abs/1602.00467
*** 515
010       Marvin: Semantic annotation using multiple knowledge sources
021       People are producing more written material then anytime in the history. 
020       The increase is so high that professionals from the various fields are no more able to cope with this amount of publications. Text mining tools can offer tools to help them and one of the tools that can aid information retrieval and information extraction is semantic text annotation. In this report we present Marvin, a text annotator written in Java, which can be used as a command line tool and as a Java library. Marvin is able to annotate text using multiple sources, including WordNet, MetaMap, DBPedia and thesauri represented as SKOS. 
007       rubr.cs.AI
007       rubr.cs
007       rubr.cs.CL
007       name.Nikola Milosevic
040       2016.02.02 11:31
050       http://arxiv.org/abs/1602.00515
*** 3960
010       TabMCQ: A Dataset of General Knowledge Tables and Multiple-choice   Questions
021       We describe two new related resources that facilitate modelling of general knowledge reasoning in 4th grade science exams. 
020       The first is a collection of curated facts in the form of tables, and the second is a large set of crowd-sourced multiple-choice questions covering the facts in the tables. Through the setup of the crowd-sourced annotation task we obtain implicit alignment information between questions and tables. We envisage that the resources will be useful not only to researchers working on question answering, but also to people investigating a diverse range of other applications such as information extraction, question parsing, answer type identification, and lexical semantic modelling. 
007       rubr.cs.CL
007       name.Sujay Kumar Jauhar
007       name.Peter Turney
007       name.Eduard Hovy
040       2016.02.12 03:54
050       http://arxiv.org/abs/1602.03960
*** 7749
010       Toward Mention Detection Robustness with Recurrent Neural Networks
021       One of the key challenges in natural language processing (NLP) is to yield good performance across application domains and languages. 
020       In this work, we investigate the robustness of the mention detection systems, one of the fundamental tasks in information extraction, via recurrent neural networks (RNNs). The advantage of RNNs over the traditional approaches is their capacity to capture long ranges of context and implicitly adapt the word embeddings, trained on a large corpus, into a task-specific word representation, but still preserve the original semantic generalization to be helpful across domains. Our systematic evaluation for RNN architectures demonstrates that RNNs not only outperform the best reported systems (up to 9\% relative error reduction) in the general setting but also achieve the state-of-the-art performance in the cross-domain setting for English. Regarding other languages, RNNs are significantly better than the traditional methods on the similar task of named entity recognition for Dutch (up to 22\% relative error reduction). 
007       rubr.cs.CL
007       name.Thien Huu Nguyen
007       name.Avirup Sil
007       name.Georgiana Dinu
007       name.Radu Florian
040       2016.02.24 23:14
050       http://arxiv.org/abs/1602.07749
*** 786
010       Improving Named Entity Recognition for Chinese Social Media with Word   Segmentation Representation Learning
021       Named entity recognition, and other information extraction tasks, frequently use linguistic features such as part of speech tags or chunkings. 
020       For languages where word boundaries are not readily identified in text, word segmentation is a key first step to generating features for an NER system. While using word boundary tags as features are helpful, the signals that aid in identifying these boundaries may provide richer information for an NER system. New state-of-the-art word segmentation systems use neural models to learn representations for predicting word boundaries. We show that these same representations, jointly trained with an NER system, yield significant improvements in NER for Chinese social media. In our experiments, jointly training NER and word segmentation with an LSTM-CRF model yields nearly 5% absolute improvement over previously published results. 
007       rubr.cs.CL
007       name.Nanyun Peng
007       name.Mark Dredze
040       2017.03.28 19:14
050       http://arxiv.org/abs/1603.00786
*** 4467
010       TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed   Systems
021       TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. 
020       A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org. 
007       rubr.cs.DC
007       rubr.cs
007       rubr.cs.LG
007       name.Mart&#xed;n Abadi
007       name.Ashish Agarwal
007       name.Paul Barham
007       name.Eugene Brevdo
007       name.Zhifeng Chen
007       name.Craig Citro
007       name.Greg S. Corrado
007       name.Andy Davis
007       name.Jeffrey Dean
007       name.Matthieu Devin
007       name.Sanjay Ghemawat
007       name.Ian Goodfellow
007       name.Andrew Harp
007       name.Geoffrey Irving
007       name.Michael Isard
007       name.Yangqing Jia
007       name.Rafal Jozefowicz
007       name.Lukasz Kaiser
007       name.Manjunath Kudlur
007       name.Josh Levenberg
007       name.Dan Mane
007       name.Rajat Monga
007       name.Sherry Moore
007       name.Derek Murray
007       name.Chris Olah
007       name.Mike Schuster
007       name.Jonathon Shlens
007       name.Benoit Steiner
007       name.Ilya Sutskever
007       name.Kunal Talwar
007       name.Paul Tucker
007       name.Vincent Vanhoucke
007       name.Vijay Vasudevan
007       name.Fernanda Viegas
007       name.Oriol Vinyals
007       name.Pete Warden
007       name.Martin Wattenberg
007       name.Martin Wicke
007       name.Yuan Yu
007       name.Xiaoqiang Zheng
040       2016.03.16 16:57
050       http://arxiv.org/abs/1603.04467
*** 6679
010       Recursive Neural Conditional Random Fields for Aspect-based Sentiment   Analysis
021       In aspect-based sentiment analysis, extracting aspect terms along with the opinions being expressed from user-generated content is one of the most important subtasks. 
020       Previous studies have shown that exploiting connections between aspect and opinion terms is promising for this task. In this paper, we propose a novel joint model that integrates recursive neural networks and conditional random fields into a unified framework for explicit aspect and opinion terms co-extraction. The proposed model learns high-level discriminative features and double propagate information between aspect and opinion terms, simultaneously. Moreover, it is flexible to incorporate hand-crafted features into the proposed model to further boost its information extraction performance. Experimental results on the SemEval Challenge 2014 dataset show the superiority of our proposed model over several baseline methods as well as the winning systems of the challenge. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       rubr.cs.LG
007       name.Wenya Wang
007       name.Sinno Jialin Pan
007       name.Daniel Dahlmeier
007       name.Xiaokui Xiao
040       2016.09.19 14:00
050       http://arxiv.org/abs/1603.06679
*** 7954
010       Improving Information Extraction by Acquiring External Evidence with   Reinforcement Learning
021       Most successful information extraction systems operate with access to a large collection of documents. 
020       In this work, we explore the task of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce. This process entails issuing search queries, extraction from new sources and reconciliation of extracted values, which are repeated until sufficient evidence is collected. We approach the problem using a reinforcement learning framework where our model learns to select optimal actions based on contextual information. We employ a deep Q-network, trained to optimize a reward function that reflects extraction accuracy while penalizing extra effort. Our experiments on two databases -- of shooting incidents, and food adulteration cases -- demonstrate that our system significantly outperforms traditional extractors and a competitive meta-classifier baseline. 
007       rubr.cs.CL
007       name.Karthik Narasimhan
007       name.Adam Yala
007       name.Regina Barzilay
040       2016.09.27 23:33
050       http://arxiv.org/abs/1603.07954
*** 9381
010       Clinical Information Extraction via Convolutional Neural Network
021       We report an implementation of a clinical information extraction tool that leverages deep neural network to annotate event spans and their attributes from raw clinical notes and pathology reports. 
020       Our approach uses context words and their part-of-speech tags and shape information as features. Then we hire temporal (1D) convolutional neural network to learn hidden feature representations. Finally, we use Multilayer Perceptron (MLP) to predict event spans. The empirical evaluation demonstrates that our approach significantly outperforms baselines. 
007       rubr.cs.LG
007       rubr.cs
007       rubr.cs.CL
007       rubr.cs.NE
007       name.Peng Li
007       name.Heng Huang
040       2016.03.30 20:57
050       http://arxiv.org/abs/1603.09381
*** 2843
010       Method of Tibetan Person Knowledge Extraction
021       Person knowledge extraction is the foundation of the Tibetan knowledge graph construction, which provides support for Tibetan question answering system, information retrieval, information extraction and other researches, and promotes national unity and social stability. 
020       This paper proposes a SVM and template based approach to Tibetan person knowledge extraction. Through constructing the training corpus, we build the templates based the shallow parsing analysis of Tibetan syntactic, semantic features and verbs. Using the training corpus, we design a hierarchical SVM classifier to realize the entity knowledge extraction. Finally, experimental results prove the method has greater improvement in Tibetan person knowledge extraction. 
007       rubr.cs.CL
007       name.Yuan Sun
007       name.Zhen Zhu
040       2016.04.11 08:56
050       http://arxiv.org/abs/1604.02843
*** 4987
010       Information retrieval from black holes
021       It is generally believed that, when matter collapses to form a black hole, the complete information about the initial state of the matter cannot be retrieved by future asymptotic observers, through local measurements. 
020       This is contrary to the expectation from a unitary evolution in quantum theory and leads to (a version of) the black hole information paradox. Classically, nothing else, apart from mass, charge and angular momentum is expected to be revealed to such asymptotic observers after the formation of a black hole. Semi-classically, black holes evaporate after their formation through the Hawking radiation. The dominant part of the radiation is expected to be thermal and hence one cannot know anything about the initial data from the resultant radiation. However, there can be sources of distortions which make the radiation non-thermal. Although the distortions are not strong enough to make the evolution unitary, these distortions carry some part of information regarding the in-state. In this work, we show how one can decipher the information about the in-state of the field from these distortions. We show that the distortions of a particular kind --- which we call {\it non-vacuum distortions} --- can be used to \emph{fully} reconstruct the initial data. The asymptotic observer can do this operationally by measuring certain well-defined observables of the quantum field at late times. We demonstrate that a general class of in-states encode all their information content in the correlation of late time out-going modes. Further, using a $1+1$ dimensional CGHS model to accommodate back-reaction self-consistently, we show that observers can also infer and track the information content about the initial data, during the course of evaporation, unambiguously. Implications of such information extraction are discussed. 
007       rubr.gr-qc
007       rubr.hep-th
007       name.Kinjalk Lochan
007       name.Sumanta Chakraborty
007       name.T. Padmanabhan
040       2016.04.18 05:08
050       http://arxiv.org/abs/1604.04987
*** 6361
010       Row-less Universal Schema
021       Universal schema jointly embeds knowledge bases and textual patterns to reason about entities and relations for automatic knowledge base construction and information extraction. 
020       In the past, entity pairs and relations were represented as learned vectors with compatibility determined by a scoring function, limiting generalization to unseen text patterns and entities. Recently, 'column-less' versions of Universal Schema have used compositional pattern encoders to generalize to all text patterns. In this work we take the next step and propose a 'row-less' model of universal schema, removing explicit entity pair representations. Instead of learning vector representations for each entity pair in our training set, we treat an entity pair as a function of its relation types. In experimental results on the FB15k-237 benchmark we demonstrate that we can match the performance of a comparable model with explicit entity pair representations using a model of attention over relation types. We further demonstrate that the model per- forms with nearly the same accuracy on entity pairs never seen during training. 
007       rubr.cs.CL
007       name.Patrick Verga
007       name.Andrew McCallum
040       2016.04.21 15:39
050       http://arxiv.org/abs/1604.06361
*** 4227
010       Relation Schema Induction using Tensor Factorization with Side   Information
021       Given a set of documents from a specific domain (e.g., medical research journals), how do we automatically build a Knowledge Graph (KG) for that domain? Automatic identification of relations and their schemas, i.e., type signature of arguments of relations (e.g., undergo(Patient, Surgery)), is an important first step towards this goal. 
020       We refer to this problem as Relation Schema Induction (RSI). In this paper, we propose Schema Induction using Coupled Tensor Factorization (SICTF), a novel tensor factorization method for relation schema induction. SICTF factorizes Open Information Extraction (OpenIE) triples extracted from a domain corpus along with additional side information in a principled way to induce relation schemas. To the best of our knowledge, this is the first application of tensor factorization for the RSI problem. Through extensive experiments on multiple real-world datasets, we find that SICTF is not only more accurate than state-of-the-art baselines, but also significantly faster (about 14x faster). 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       rubr.cs.DB
007       name.Madhav Nimishakavi
007       name.Uday Singh Saini
007       name.Partha Talukdar
040       2016.11.16 04:53
050       http://arxiv.org/abs/1605.04227
*** 7918
010       Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks   with Feedback Negative Sampling
021       Previous studies in Open Information Extraction (Open IE) are mainly based on extraction patterns. 
020       They manually define patterns or automatically learn them from a large corpus. However, these approaches are limited when grasping the context of a sentence, and they fail to capture implicit relations. In this paper, we address this problem with the following methods. First, we exploit long short-term memory (LSTM) networks to extract higher-level features along the shortest dependency paths, connecting headwords of relations and arguments. The path-level features from LSTM networks provide useful clues regarding contextual information and the validity of arguments. Second, we constructed samples to train LSTM networks without the need for manual labeling. In particular, feedback negative sampling picks highly negative samples among non-positive samples through a model trained with positive samples. The experimental results show that our approach produces more precise and abundant extractions than state-of-the-art open IE systems. To the best of our knowledge, this is the first work to apply deep learning to Open IE. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.NE
007       name.Byungsoo Kim
007       name.Hwanjo Yu
007       name.Gary Geunbae Lee
040       2016.05.25 14:59
050       http://arxiv.org/abs/1605.07918
*** 9441
010       Blind Modulation Classification based on MLP and PNN
021       In this work, a pattern recognition system is investigated for blind automatic classification of digitally modulated communication signals. 
020       The proposed technique is able to discriminate the type of modulation scheme which is eventually used for demodulation followed by information extraction. The proposed system is composed of two subsystems namely feature extraction sub-system (FESS) and classifier sub-system (CSS). The FESS consists of continuous wavelet transform (CWT) for feature generation and principal component analysis (PCA) for selection of the feature subset which is rich in discriminatory information. The CSS uses the selected features to accurately classify the modulation class of the received signal. The proposed technique uses probabilistic neural network (PNN) and multilayer perceptron forward neural network (MLPFN) for comparative study of their recognition ability. PNN have been found to perform better in terms of classification accuracy as well as testing and training time than MLPFN. The proposed approach is robust to presence of phase offset and additive Gaussian noise. 
007       rubr.cs.CV
007       rubr.cs
007       rubr.cs.IT
007       name.Harishchandra Dubey
007       name.Nandita
007       name.Ashutosh Kumar Tiwari
040       2016.05.30 23:00
050       http://arxiv.org/abs/1605.09441
*** 1404
010       Generating Natural Language Inference Chains
021       The ability to reason with natural language is a fundamental prerequisite for many NLP tasks such as information extraction, machine translation and question answering. 
020       To quantify this ability, systems are commonly tested whether they can recognize textual entailment, i.e., whether one sentence can be inferred from another one. However, in most NLP applications only single source sentences instead of sentence pairs are available. Hence, we propose a new task that measures how well a model can generate an entailed sentence from a source sentence. We take entailment-pairs of the Stanford Natural Language Inference corpus and train an LSTM with attention. On a manually annotated test set we found that 82% of generated sentences are correct, an improvement of 10.3% over an LSTM baseline. A qualitative analysis shows that this model is not only capable of shortening input sentences, but also inferring new statements via paraphrasing and phrase entailment. We then apply this model recursively to input-output pairs, thereby generating natural language inference chains that can be used to automatically construct an entailment graph from source sentences. Finally, by swapping source and target sentences we can also train a model that given an input sentence invents additional information to generate a new sentence. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.NE
007       name.Vladyslav Kolesnyk
007       name.Tim Rockt&#xe4;schel
007       name.Sebastian Riedel
040       2016.06.04 18:34
050       http://arxiv.org/abs/1606.01404
*** 1433
010       Brundlefly at SemEval-2016 Task 12: Recurrent Neural Networks vs. Joint   Inference for Clinical Temporal Information Extraction
021       We submitted two systems to the SemEval-2016 Task 12: Clinical TempEval challenge, participating in Phase 1, where we identified text spans of time and event expressions in clinical notes and Phase 2, where we predicted a relation between an event and its parent document creation time. 
020       <br />For temporal entity extraction, we find that a joint inference-based approach using structured prediction outperforms a vanilla recurrent neural network that incorporates word embeddings trained on a variety of large clinical document sets. For document creation time relations, we find that a combination of date canonicalization and distant supervision rules for predicting relations on both events and time expressions improves classification, though gains are limited, likely due to the small scale of training data. 
007       rubr.cs.CL
007       name.Jason Alan Fries
040       2016.06.04 23:22
050       http://arxiv.org/abs/1606.01433
*** 3126
010       Key-Value Memory Networks for Directly Reading Documents
021       Directly reading documents and being able to answer questions from them is an unsolved challenge. 
020       To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, WikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing WikiQA benchmark. 
007       rubr.cs.CL
007       name.Alexander Miller
007       name.Adam Fisch
007       name.Jesse Dodge
007       name.Amir-Hossein Karimi
007       name.Antoine Bordes
007       name.Jason Weston
040       2016.10.10 20:14
050       http://arxiv.org/abs/1606.03126
*** 3502
010       Intelligent audit code generation from free text in the context of   neurosurgery
021       Clinical auditing requires codified data for aggregation and analysis of patterns. 
020       However in the medical domain obtaining structured data can be difficult as the most natural, expressive and comprehensive way to record a clinical encounter is through natural language. The task of creating structured data from naturally expressed information is known as information extraction. Specialised areas of medicine use their own language and data structures; the translation process has unique challenges, and often requires a fresh approach. This research is devoted to creating a novel semi-automated method for generating codified auditing data from clinical notes recorded in a neurosurgical department in an Australian teaching hospital. The method encapsulates specialist knowledge in rules that instantaneously make precise decisions for the majority of the matches, followed up by dictionary-based matching of the remaining text. 
007       rubr.cs.CY
007       name.Sedigheh Khademi
007       name.Christopher Palmer
007       name.Pari Delir Haghighi
007       name.Philip Lewis
007       name.Frada Burstein
040       2016.06.10 23:52
050       http://arxiv.org/abs/1606.03502
*** 5994
010       The Role of CNL and AMR in Scalable Abstractive Summarization for   Multilingual Media Monitoring
021       In the era of Big Data and Deep Learning, there is a common view that machine learning approaches are the only way to cope with the robust and scalable information extraction and summarization. 
020       It has been recently proposed that the CNL approach could be scaled up, building on the concept of embedded CNL and, thus, allowing for CNL-based information extraction from e.g. normative or medical texts that are rather controlled by nature but still infringe the boundaries of CNL. Although it is arguable if CNL can be exploited to approach the robust wide-coverage semantic parsing for use cases like media monitoring, its potential becomes much more obvious in the opposite direction: generation of story highlights from the summarized AMR graphs, which is in the focus of this position paper. 
007       rubr.cs.CL
007       name.Normunds Gruzitis
007       name.Guntis Barzdins
040       2016.06.20 07:15
050       http://arxiv.org/abs/1606.05994
*** 6424
010       A Novel Framework to Expedite Systematic Reviews by Automatically   Building Information Extraction Training Corpora
021       A systematic review identifies and collates various clinical studies and compares data elements and results in order to provide an evidence based answer for a particular clinical question. 
020       The process is manual and involves lot of time. A tool to automate this process is lacking. The aim of this work is to develop a framework using natural language processing and machine learning to build information extraction algorithms to identify data elements in a new primary publication, without having to go through the expensive task of manual annotation to build gold standards for each data element type. The system is developed in two stages. Initially, it uses information contained in existing systematic reviews to identify the sentences from the PDF files of the included references that contain specific data elements of interest using a modified Jaccard similarity measure. These sentences have been treated as labeled data.A Support Vector Machine (SVM) classifier is trained on this labeled data to extract data elements of interests from a new article. We conducted experiments on Cochrane Database systematic reviews related to congestive heart failure using inclusion criteria as an example data element. The empirical results show that the proposed system automatically identifies sentences containing the data element of interest with a high recall (93.75%) and reasonable precision (27.05% - which means the reviewers have to read only 3.7 sentences on average). The empirical results suggest that the tool is retrieving valuable information from the reference articles, even when it is time-consuming to identify them manually. Thus we hope that the tool will be useful for automatic data extraction from biomedical research publications. The future scope of this work is to generalize this information framework for all types of systematic reviews. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       rubr.cs.LG
007       name.Tanmay Basu
007       name.Shraman Kumar
007       name.Abhishek Kalyan
007       name.Priyanka Jayaswal
007       name.Pawan Goyal
007       name.Stephen Pettifer
007       name.Siddhartha R. Jonnalagadda
040       2016.06.21 04:56
050       http://arxiv.org/abs/1606.06424
*** 7784
010       Satellite Images Analysis with Symbolic Time Series: A Case Study of the   Algerian Zone
021       Satellite Image Time Series (SITS) are an important source of information for studying land occupation and its evolution. 
020       Indeed, the very large volumes of digital data stored, usually are not ready to a direct analysis. In order to both reduce the dimensionality and information extraction, time series data mining generally gives rise to change of time series representation. In an objective of information intelligibility extracted from the representation change, we may use symbolic representations of time series. Many high level representations of time series have been proposed for data mining, including Fourier transforms, wavelets, piecewise polynomial models, etc. Many researchers have also considered symbolic representations of time series, noting that such representations would potentiality allow researchers to avail of the wealth of data structures and algorithms from the text processing and bioinformatics communities. We present in this work, one of the main symbolic representation methods "SAX"(Symbolic Aggregate Approximation) and we experience this method to symbolize and reduce the dimensionality of a Satellite Image Times Series acquired over a period of 5 years by characterizing the evolution of a vegetation index (NDVI). 
007       rubr.cs.DB
007       name.Dalila Attaf
007       name.Djamila Hamdadou
007       name.Sidahmed Benabderrahmane
007       name.Aicha Lafrid
040       2016.06.23 18:47
050       http://arxiv.org/abs/1606.07784
*** 7901
010       Corpus-level Fine-grained Entity Typing Using Contextual Information
021       This paper addresses the problem of corpus-level entity typing, i.e., inferring from a large corpus that an entity is a member of a class such as "food" or "artist". 
020       The application of entity typing we are interested in is knowledge base completion, specifically, to learn which classes an entity is a member of. We propose FIGMENT to tackle this problem. FIGMENT is embedding-based and combines (i) a global model that scores based on aggregated contextual information of an entity and (ii) a context model that first scores the individual occurrences of an entity and then aggregates the scores. In our evaluation, FIGMENT strongly outperforms an approach to entity typing that relies on relations obtained by an open information extraction system. 
007       rubr.cs.CL
007       name.Yadollah Yaghoobzadeh
007       name.Hinrich Sch&#xfc;tze
040       2016.06.25 12:22
050       http://arxiv.org/abs/1606.07901
*** 7993
010       Learning for Biomedical Information Extraction: Methodological Review of   Recent Advances
021       Biomedical information extraction (BioIE) is important to many applications, including clinical decision support, integrative biology, and pharmacovigilance, and therefore it has been an active research. 
020       Unlike existing reviews covering a holistic view on BioIE, this review focuses on mainly recent advances in learning based approaches, by systematically summarizing them into different aspects of methodological development. In addition, we dive into open information extraction and deep learning, two emerging and influential techniques and envision next generation of BioIE. 
007       rubr.cs.CL
007       name.Feifan Liu
007       name.Jinying Chen
007       name.Abhyuday Jagannatha
007       name.Hong Yu
040       2016.06.26 04:39
050       http://arxiv.org/abs/1606.07993
*** 9604
010       SnapToGrid: From Statistical to Interpretable Models for Biomedical   Information Extraction
021       We propose an approach for biomedical information extraction that marries the advantages of machine learning models, e.g., learning directly from data, with the benefits of rule-based approaches, e.g., interpretability. 
020       Our approach starts by training a feature-based statistical model, then converts this model to a rule-based variant by converting its features to rules, and "snapping to grid" the feature weights to discrete votes. In doing so, our proposal takes advantage of the large body of work in machine learning, but it produces an interpretable model, which can be directly edited by experts. We evaluate our approach on the BioNLP 2009 event extraction task. Our results show that there is a small performance penalty when converting the statistical model to rules, but the gain in interpretability compensates for that: with minimal effort, human experts improve this model to have similar performance to the statistical model that served as starting point. 
007       rubr.cs.CL
007       name.Marco A. Valenzuela-Escarcega
007       name.Gus Hahn-Powell
007       name.Dane Bell
007       name.Mihai Surdeanu
040       2016.06.30 18:23
050       http://arxiv.org/abs/1606.09604
*** 667
010       Reducing the Energy Cost of Inference via In-sensor Information   Processing
021       There is much interest in incorporating inference capabilities into sensor-rich embedded platforms such as autonomous vehicles, wearables, and others. 
020       A central problem in the design of such systems is the need to extract information locally from sensed data on a severely limited energy budget. This necessitates the design of energy-efficient sensory embedded system. A typical sensory embedded system enforces a physical separation between sensing and computational subsystems - a separation mandated by the differing requirements of the sensing and computational functions. As a consequence, the energy consumption in such systems tends to be dominated by the energy consumed in transferring data over the sensor-processor interface (communication energy) and the energy consumed in processing the data in digital processor (computational energy). In this article, we propose an in-sensor computing architecture which (mostly) eliminates the sensor-processor interface by embedding inference computations in the noisy sensor fabric in analog and retraining the hyperparameters in order to compensate for non-ideal computations. The resulting architecture referred to as the Compute Sensor - a sensor that computes in addition to sensing - represents a radical departure from the conventional. We show that a Compute Sensor for image data can be designed by embedding both feature extraction and classification functions in the analog domain in close proximity to the CMOS active pixel sensor (APS) array. Significant gains in energy-efficiency are demonstrated using behavioral and energy models in a commercial semiconductor process technology. In the process, the Compute Sensor creates a unique opportunity to develop machine learning algorithms for information extraction from data on a noisy underlying computational fabric. 
007       rubr.cs.AR
007       rubr.cs
007       rubr.cs.ET
007       name.Sai Zhang
007       name.Mingu Kang
007       name.Charbel Sakr
007       name.Naresh Shanbhag
040       2016.07.03 18:43
050       http://arxiv.org/abs/1607.00667
*** 2784
010       Open Information Extraction
021       Open Information Extraction (Open IE) systems aim to obtain relation tuples with highly scalable extraction in portable across domain by identifying a variety of relation phrases and their arguments in arbitrary sentences. 
020       The first generation of Open IE learns linear chain models based on unlexicalized features such as Part-of-Speech (POS) or shallow tags to label the intermediate words between pair of potential arguments for identifying extractable relations. Open IE currently is developed in the second generation that is able to extract instances of the most frequently observed relation types such as Verb, Noun and Prep, Verb and Prep, and Infinitive with deep linguistic analysis. They expose simple yet principled ways in which verbs express relationships in linguistics such as verb phrase-based extraction or clause-based extraction. They obtain a significantly higher performance over previous systems in the first generation. In this paper, we describe an overview of two Open IE generations including strengths, weaknesses and application areas. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       name.Duc-Thuan Vo
007       name.Ebrahim Bagheri
040       2016.07.10 20:39
050       http://arxiv.org/abs/1607.02784
*** 2810
010       The Benefits of Word Embeddings Features for Active Learning in Clinical   Information Extraction
021       This study investigates the use of unsupervised word embeddings and sequence features for sample representation in an active learning framework built to extract clinical concepts from clinical free text. 
020       The objective is to further reduce the manual annotation effort while achieving higher effectiveness compared to a set of baseline features. Unsupervised features are derived from skip-gram word embeddings and a sequence representation approach. The comparative performance of unsupervised features and baseline hand-crafted features in an active learning framework are investigated using a wide range of selection criteria including least confidence, information diversity, information density and diversity, and domain knowledge informativeness. Two clinical datasets are used for evaluation: the i2b2/VA 2010 NLP challenge and the ShARe/CLEF 2013 eHealth Evaluation Lab. Our results demonstrate significant improvements in terms of effectiveness as well as annotation effort savings across both datasets. Using unsupervised features along with baseline features for sample representation lead to further savings of up to 9% and 10% of the token and concept annotation rates, respectively. 
007       rubr.cs.CL
007       name.Mahnoosh Kholghi
007       name.Lance De Vine
007       name.Laurianne Sitbon
007       name.Guido Zuccon
007       name.Anthony Nguyen
040       2016.11.15 05:06
050       http://arxiv.org/abs/1607.02810
*** 3071
010       Quantum Information Processing in the Radical-Pair Mechanism: Haberkorn   theory violates the Ozawa entropy bound
021       Radical-ion-pair reactions, central for understanding the avian magnetic compass and spin transport in photosynthetic reaction centers, were recently shown to be a fruitful paradigm of the new synthesis of quantum information science with biological processes. 
020       We show here that the master equation so far constituting the theoretical foundation of spin chemistry violates fundamental bounds for the entropy of quantum systems, in particular the Ozawa bound. In contrast, a recently developed theory based on quantum measurements, quantum coherence measures, and quantum retrodiction, thus exemplifying the paradigm of quantum biology, satisfies the Ozawa bound as well as the Lanford-Robinson bound on information extraction. By considering Groenewold information, the quantum information extracted during the reaction, we reproduce the known and unravel other magnetic-field effects not conveyed by reaction yields. 
007       rubr.quant-ph
007       rubr.physics
007       rubr.physics.bio-ph
007       rubr.physics.bio-ph
007       rubr.physics.chem-ph
007       name.K. Mouloudakis
007       name.I. K. Kominis
040       2017.02.23 21:56
050       http://arxiv.org/abs/1607.03071
*** 612
010       Structured prediction models for RNN based sequence labeling in clinical   text
021       Sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data. 
020       In clinical domain one major application of sequence labeling involves extraction of medical entities such as medication, indication, and side-effects from Electronic Health Record narratives. Sequence labeling in this domain, presents its own set of challenges and objectives. In this work we experimented with various CRF based structured learning models with Recurrent Neural Networks. We extend the previously studied LSTM-CRF models with explicit modeling of pairwise potentials. We also propose an approximate version of skip-chain CRF inference with RNN potentials. We use these methodologies for structured prediction in order to improve the exact phrase detection of various medical entities. 
007       rubr.cs.CL
007       name.Abhyuday Jagannatha
007       name.Hong Yu
040       2016.08.01 20:54
050       http://arxiv.org/abs/1608.00612
*** 3542
010       WikiReading: A Novel Large-scale Language Understanding Task over   Wikipedia
021       We present WikiReading, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. 
020       The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNN-based architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%. 
007       rubr.cs.CL
007       name.Daniel Hewlett
007       name.Alexandre Lacoste
007       name.Llion Jones
007       name.Illia Polosukhin
007       name.Andrew Fandrianto
007       name.Jay Han
007       name.Matthew Kelcey
007       name.David Berthelot
040       2017.03.15 19:58
050       http://arxiv.org/abs/1608.03542
*** 4767
010       Proceedings of the LexSem+Logics Workshop 2016
021       Lexical semantics continues to play an important role in driving research directions in NLP, with the recognition and understanding of context becoming increasingly important in delivering successful outcomes in NLP tasks. 
020       Besides traditional processing areas such as word sense and named entity disambiguation, the creation and maintenance of dictionaries, annotated corpora and resources have become cornerstones of lexical semantics research and produced a wealth of contextual information that NLP processes can exploit. New efforts both to link and construct from scratch such information - as Linked Open Data or by way of formal tools coming from logic, ontologies and automated reasoning - have increased the interoperability and accessibility of resources for lexical and computational semantics, even in those languages for which they have previously been limited. <br />LexSem+Logics 2016 combines the 1st Workshop on Lexical Semantics for Lesser-Resources Languages and the 3rd Workshop on Logics and Ontologies. The accepted papers in our program covered topics across these two areas, including: the encoding of plurals in Wordnets, the creation of a thesaurus from multiple sources based on semantic similarity metrics, and the use of cross-lingual treebanks and annotations for universal part-of-speech tagging. We also welcomed talks from two distinguished speakers: on Portuguese lexical knowledge bases (different approaches, results and their application in NLP tasks) and on new strategies for open information extraction (the capture of verb-based propositions from massive text corpora). 
007       rubr.cs.CL
007       name.Steven Neale
007       name.Valeria de Paiva
007       name.Arantxa Otegi
007       name.Alexandre Rademaker
040       2016.08.14 16:23
050       http://arxiv.org/abs/1608.04767
*** 6386
010       Which techniques does your application use?: An information extraction   framework for scientific articles
021       Every field of research consists of multiple application areas with various techniques routinely used to solve problems in these wide range of application areas. 
020       With the exponential growth in research volumes, it has become difficult to keep track of the ever-growing number of application areas as well as the corresponding problem solving techniques. In this paper, we consider the computational linguistics domain and present a novel information extraction system that automatically constructs a pool of all application areas in this domain and appropriately links them with corresponding problem solving techniques. Further, we categorize individual research articles based on their application area and the techniques proposed/used in the article. k-gram based discounting method along with handwritten rules and bootstrapped pattern learning is employed to extract application areas. Subsequently, a language modeling approach is proposed to characterize each article based on its application area. Similarly, regular expressions and high-scoring noun phrases are used for the extraction of the problem solving techniques. We propose a greedy approach to characterize each article based on the techniques. Towards the end, we present a table representing the most frequent techniques adopted for a particular application area. Finally, we propose three use cases presenting an extensive temporal analysis of the usage of techniques and application areas. 
007       rubr.cs.CL
007       name.Soham Dan
007       name.Sanyam Agarwal
007       name.Mayank Singh
007       name.Pawan Goyal
007       name.Animesh Mukherjee
040       2016.08.23 05:27
050       http://arxiv.org/abs/1608.06386
*** 6718
010       A Large-Scale Multilingual Disambiguation of Glosses
021       Linking concepts and named entities to knowledge bases has become a crucial Natural Language Understanding task. 
020       In this respect, recent works have shown the key advantage of exploiting textual definitions in various Natural Language Processing applications. However, to date there are no reliable large-scale corpora of sense-annotated textual definitions available to the research community. In this paper we present a large-scale high-quality corpus of disambiguated glosses in multiple languages, comprising sense annotations of both concepts and named entities from a unified sense inventory. Our approach for the construction and disambiguation of the corpus builds upon the structure of a large multilingual semantic network and a state-of-the-art disambiguation system; first, we gather complementary information of equivalent definitions across different languages to provide context for disambiguation, and then we combine it with a semantic similarity-based refinement. As a result we obtain a multilingual corpus of textual definitions featuring over 38 million definitions in 263 languages, and we make it freely available at <a href="http://lcl.uniroma1.it/disambiguated-glosses.">this http URL</a> Experiments on Open Information Extraction and Sense Clustering show how two state-of-the-art approaches improve their performance by integrating our disambiguated corpus into their pipeline. 
007       rubr.cs.CL
007       name.Jos&#xe9; Camacho Collados
007       name.Claudio Delli Bovi
007       name.Alessandro Raganato
007       name.Roberto Navigli
040       2016.08.24 05:30
050       http://arxiv.org/abs/1608.06718
*** 8851
010       Efficient Two-Stream Motion and Appearance 3D CNNs for Video   Classification
021       The video and action classification have extremely evolved by deep neural networks specially with two stream CNN using RGB and optical flow as inputs and they present outstanding performance in terms of video analysis. 
020       One of the shortcoming of these methods is handling motion information extraction which is done out side of the CNNs and relatively time consuming also on GPUs. So proposing end-to-end methods which are exploring to learn motion representation, like 3D-CNN can achieve faster and accurate performance. We present some novel deep CNNs using 3D architecture to model actions and motion representation in an efficient way to be accurate and also as fast as real-time. Our new networks learn distinctive models to combine deep motion features into appearance model via learning optical flow features inside the network. 
007       rubr.cs.CV
007       name.Ali Diba
007       name.Ali Mohammad Pazandeh
007       name.Luc Van Gool
040       2016.09.02 10:39
050       http://arxiv.org/abs/1608.08851
*** 799
010       Lexical-Morphological Modeling for Legal Text Analysis
021       In the context of the Competition on Legal Information Extraction/Entailment (COLIEE), we propose a method comprising the necessary steps for finding relevant documents to a legal question and deciding on textual entailment evidence to provide a correct answer. 
020       The proposed method is based on the combination of several lexical and morphological characteristics, to build a language model and a set of features for Machine Learning algorithms. We provide a detailed study on the proposed method performance and failure cases, indicating that it is competitive with state-of-the-art approaches on Legal Information Retrieval and Question Answering, while not needing extensive training data nor depending on expert produced knowledge. The proposed method achieved significant results in the competition, indicating a substantial level of adequacy for the tasks addressed. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       name.Danilo S. Carvalho
007       name.Minh-Tien Nguyen
007       name.Tran Xuan Chien
007       name.Minh Le Nguyen
040       2016.09.03 07:24
050       http://arxiv.org/abs/1609.00799
*** 1017
010       Crowdsourcing Information Extraction for Biomedical Systematic Reviews
021       Information extraction is a critical step in the practice of conducting biomedical systematic literature reviews. 
020       Extracted structured data can be aggregated via methods such as statistical meta-analysis. Typically highly trained domain experts extract data for systematic reviews. The high expense of conducting biomedical systematic reviews has motivated researchers to explore lower cost methods that achieve similar rigor without compromising quality. Crowdsourcing represents one such promising approach. In this work-in-progress study, we designed a crowdsourcing task for biomedical information extraction. We briefly report the iterative design process and the results of two pilot testings. We found that giving more concrete examples in the task instruction can help workers better understand the task, especially for concepts that are abstract and confusing. We found a few workers completed most of the work, and our payment level appeared more attractive to workers from low-income countries. In the future, we will further evaluate our results with reference to gold standard extractions, thus assessing the feasibility of tasking crowd workers with extracting biomedical intervention information for systematic reviews. 
007       rubr.cs.HC
007       name.Yalin Sun
007       name.Pengxiang Cheng
007       name.Shengwei Wang
007       name.Hao Lyu
007       name.Matthew Lease
007       name.Iain Marshall
007       name.Byron C. Wallace
040       2016.09.05 02:01
050       http://arxiv.org/abs/1609.01017
*** 1592
010       CRTS: A type system for representing clinical recommendations
021       Background: Clinical guidelines and recommendations are the driving wheels of the evidence-based medicine (EBM) paradigm, but these are available primarily as unstructured text and are generally highly heterogeneous in nature. 
020       This significantly reduces the dissemination and automatic application of these recommendations at the point of care. A comprehensive structured representation of these recommendations is highly beneficial in this regard. Objective: The objective of this paper to present Clinical Recommendation Type System (CRTS), a common type system that can effectively represent a clinical recommendation in a structured form. Methods: CRTS is built by analyzing 125 recommendations and 195 research articles corresponding to 6 different diseases available from UpToDate, a publicly available clinical knowledge system, and from the National Guideline Clearinghouse, a public resource for evidence-based clinical practice guidelines. Results: We show that CRTS not only covers the recommendations but also is flexible to be extended to represent information from primary literature. We also describe how our developed type system can be applied for clinical decision support, medical knowledge summarization, and citation retrieval. Conclusion: We showed that our proposed type system is precise and comprehensive in representing a large sample of recommendations available for various disorders. CRTS can now be used to build interoperable information extraction systems that automatically extract clinical recommendations and related data elements from clinical evidence resources, guidelines, systematic reviews and primary publications. <br />Keywords: guidelines and recommendations, type system, clinical decision support, evidence-based medicine, information storage and retrieval 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.CY
007       name.Ravi P Garg
007       name.Kalpana Raja
007       name.Siddhartha R Jonnalagadda
040       2016.09.06 15:02
050       http://arxiv.org/abs/1609.01592
*** 1594
010       An Information Extraction Approach to Prescreen Heart Failure Patients   for Clinical Trials
021       To reduce the large amount of time spent screening, identifying, and recruiting patients into clinical trials, we need prescreening systems that are able to automate the data extraction and decision-making tasks that are typically relegated to clinical research study coordinators. 
020       However, a major obstacle is the vast amount of patient data available as unstructured free-form text in electronic health records. Here we propose an information extraction-based approach that first automatically converts unstructured text into a structured form. The structured data are then compared against a list of eligibility criteria using a rule-based system to determine which patients qualify for enrollment in a heart failure clinical trial. We show that we can achieve highly accurate results, with recall and precision values of 0.95 and 0.86, respectively. Our system allowed us to significantly reduce the time needed for prescreening patients from a few weeks to a few minutes. Our open-source information extraction modules are available for researchers and could be tested and validated in other cardiovascular trials. An approach such as the one we demonstrate here may decrease costs and expedite clinical trials, and could enhance the reproducibility of trials across institutions and populations. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.CY
007       name.Abhishek Kalyan Adupa
007       name.Ravi Prakash Garg
007       name.Jessica Corona-Cox
007       name.Sanjiv. J. Shah
007       name.Siddhartha R. Jonnalagadda
040       2016.09.06 15:05
050       http://arxiv.org/abs/1609.01594
*** 1597
010       A Hybrid Citation Retrieval Algorithm for Evidence-based Clinical   Knowledge Summarization: Combining Concept Extraction, Vector Similarity and   Query Expansion for High Precision
021       Novel information retrieval methods to identify citations relevant to a clinical topic can overcome the knowledge gap existing between the primary literature (MEDLINE) and online clinical knowledge resources such as UpToDate. 
020       Searching the MEDLINE database directly or with query expansion methods returns a large number of citations that are not relevant to the query. The current study presents a citation retrieval system that retrieves citations for evidence-based clinical knowledge summarization. This approach combines query expansion, concept-based screening algorithm, and concept-based vector similarity. We also propose an information extraction framework for automated concept (Population, Intervention, Comparison, and Disease) extraction. We evaluated our proposed system on all topics (as queries) available from UpToDate for two diseases, heart failure (HF) and atrial fibrillation (AFib). The system achieved an overall F-score of 41.2% on HF topics and 42.4% on AFib topics on a gold standard of citations available in UpToDate. This is significantly high when compared to a query-expansion based baseline (F-score of 1.3% on HF and 2.2% on AFib) and a system that uses query expansion with disease hyponyms and journal names, concept-based screening, and term-based vector similarity system (F-score of 37.5% on HF and 39.5% on AFib). Evaluating the system with top K relevant citations, where K is the number of citations in the gold standard achieved a much higher overall F-score of 69.9% on HF topics and 75.1% on AFib topics. In addition, the system retrieved up to 18 new relevant citations per topic when tested on ten HF and six AFib clinical topics. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       name.Kalpana Raja
007       name.Andrew J Sauer
007       name.Ravi P Garg
007       name.Melanie R Klerer
007       name.Siddhartha R Jonnalagadda
040       2016.09.06 15:10
050       http://arxiv.org/abs/1609.01597
*** 3632
010       Joint Extraction of Events and Entities within a Document Context
021       Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. 
020       The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate context-aware predictions. We demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       name.Bishan Yang
007       name.Tom Mitchell
040       2016.09.12 23:27
050       http://arxiv.org/abs/1609.03632
*** 5103
010       Learning Tuple Probabilities
021       Learning the parameters of complex probabilistic-relational models from labeled training data is a standard technique in machine learning, which has been intensively studied in the subfield of Statistical Relational Learning (SRL), but---so far---this is still an under-investigated topic in the context of Probabilistic Databases (PDBs). 
020       In this paper, we focus on learning the probability values of base tuples in a PDB from labeled lineage formulas. The resulting learning problem can be viewed as the inverse problem to confidence computations in PDBs: given a set of labeled query answers, learn the probability values of the base tuples, such that the marginal probabilities of the query answers again yield in the assigned probability labels. We analyze the learning problem from a theoretical perspective, cast it into an optimization problem, and provide an algorithm based on stochastic gradient descent. Finally, we conclude by an experimental evaluation on three real-world and one synthetic dataset, thus comparing our approach to various techniques from SRL, reasoning in information extraction, and optimization. 
007       rubr.cs.DB
007       name.Maximilian Dylla
007       name.Martin Theobald
040       2016.09.20 06:36
050       http://arxiv.org/abs/1609.05103
*** 6423
010       OCR++: A Robust Framework For Information Extraction from Scholarly   Articles
021       This paper proposes OCR++, an open-source framework designed for a variety of information extraction tasks from scholarly articles including metadata (title, author names, affiliation and e-mail), structure (section headings and body text, table and figure headings, URLs and footnotes) and bibliography (citation instances and references). 
020       We analyze a diverse set of scientific articles written in English language to understand generic writing patterns and formulate rules to develop this hybrid framework. Extensive evaluations show that the proposed framework outperforms the existing state-of-the-art tools with huge margin in structural information extraction along with improved performance in metadata and bibliography extraction tasks, both in terms of accuracy (around 50% improvement) and processing time (around 52% improvement). A user experience study conducted with the help of 30 researchers reveals that the researchers found this system to be very helpful. As an additional objective, we discuss two novel use cases including automatically extracting links to public datasets from the proceedings, which would further accelerate the advancement in digital libraries. The result of the framework can be exported as a whole into structured TEI-encoded documents. Our framework is accessible online at <a href="http://cnergres.iitkgp.ac.in/OCR++/home/.">this http URL</a> 
007       rubr.cs.DL
007       rubr.cs
007       rubr.cs.IR
007       name.Mayank Singh
007       name.Barnopriyo Barua
007       name.Priyank Palod
007       name.Manvi Garg
007       name.Sidhartha Satapathy
007       name.Samuel Bushi
007       name.Kumar Ayush
007       name.Krishna Sai Rohith
007       name.Tulasi Gamidi
007       name.Pawan Goyal
007       name.Animesh Mukherjee
040       2016.09.23 13:05
050       http://arxiv.org/abs/1609.06423
*** 8075
010       S-MART: Novel Tree-based Structured Learning Algorithms Applied to Tweet   Entity Linking
021       Non-linear models recently receive a lot of attention as people are starting to discover the power of statistical and embedding features. 
020       However, tree-based models are seldom studied in the context of structured learning despite their recent success on various classification and ranking tasks. In this paper, we propose S-MART, a tree-based structured learning framework based on multiple additive regression trees. S-MART is especially suitable for handling tasks with dense features, and can be used to learn many different structures under various loss functions. <br />We apply S-MART to the task of tweet entity linking --- a core component of tweet information extraction, which aims to identify and link name mentions to entities in a knowledge base. A novel inference algorithm is proposed to handle the special structure of the task. The experimental results show that S-MART significantly outperforms state-of-the-art tweet entity linking systems. 
007       rubr.cs.CL
007       name.Yi Yang
007       name.Ming-Wei Chang
040       2016.09.26 17:01
050       http://arxiv.org/abs/1609.08075
*** 8084
010       Toward Socially-Infused Information Extraction: Embedding Authors,   Mentions, and Entities
021       Entity linking is the task of identifying mentions of entities in text, and linking them to entries in a knowledge base. 
020       This task is especially difficult in microblogs, as there is little additional text to provide disambiguating context; rather, authors rely on an implicit common ground of shared knowledge with their readers. In this paper, we attempt to capture some of this implicit context by exploiting the social network structure in microblogs. We build on the theory of homophily, which implies that socially linked individuals share interests, and are therefore likely to mention the same sorts of entities. We implement this idea by encoding authors, mentions, and entities in a continuous vector space, which is constructed so that socially-connected authors have similar vector representations. These vectors are incorporated into a neural structured prediction model, which captures structural constraints that are inherent in the entity linking task. Together, these design decisions yield F1 improvements of 1%-5% on benchmark datasets, as compared to the previous state-of-the-art. 
007       rubr.cs.CL
007       name.Yi Yang
007       name.Ming-Wei Chang
007       name.Jacob Eisenstein
040       2016.09.26 17:19
050       http://arxiv.org/abs/1609.08084
*** 8409
010       Modelling Radiological Language with Bidirectional Long Short-Term   Memory Networks
021       Motivated by the need to automate medical information extraction from free-text radiological reports, we present a bi-directional long short-term memory (BiLSTM) neural network architecture for modelling radiological language. 
020       The model has been used to address two NLP tasks: medical named-entity recognition (NER) and negation detection. We investigate whether learning several types of word embeddings improves BiLSTM's performance on those tasks. Using a large dataset of chest x-ray reports, we compare the proposed model to a baseline dictionary-based NER system and a negation detection system that leverages the hand-crafted rules of the NegEx algorithm and the grammatical relations obtained from the Stanford Dependency Parser. Compared to these more traditional rule-based systems, we argue that BiLSTM offers a strong alternative for both our tasks. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.stat
007       rubr.stat.ML
007       name.Savelie Cornegruta
007       name.Robert Bakewell
007       name.Samuel Withey
007       name.Giovanni Montana
040       2016.09.27 13:25
050       http://arxiv.org/abs/1609.08409
*** 8893
010       A generic framework for the development of geospatial processing   pipelines on clusters
021       The amount of remote sensing data available to applications is constantly growing due to the rise of very-high-resolution sensors and short repeat cycle satellites. 
020       Consequently, tackling computational complexity in Earth Observation information extraction is rising as a major challenge. Resorting to High Performance Computing (HPC) is becoming a common practice, since it provides environments and programming facilities able to speed-up processes. In particular, clusters are flexible, cost-effective systems able to perform data-intensive tasks ideally fulfilling any computational requirement. However, their use typically implies a significant coding effort to build proper implementations of specific processing pipelines. This paper presents a generic framework for the development of RS images processing applications targeting cluster computing. It is based on common open sources libraries, and leverages the parallelization of a wide variety of image processing pipelines in a transparent way. Performances on typical RS tasks implemented using the proposed framework demonstrate a great potential for the effective and timely processing of large amount of data. 
007       rubr.cs.DC
007       name.Remi Cresson
040       2016.09.28 12:48
050       http://arxiv.org/abs/1609.08893
*** 479
010       Nonsymbolic Text Representation
021       We introduce the first generic text representation model that is completely nonsymbolic, i.e., it does not require the availability of a segmentation or tokenization method that attempts to identify words or other symbolic units in text. 
020       This applies to training the parameters of the model on a training corpus as well as to applying it when computing the representation of a new text. We show that our model performs better than prior work on an information extraction and a text denoising task. 
007       rubr.cs.CL
007       name.Hinrich Schuetze
007       name.Heike Adel
007       name.Ehsaneddin Asgari
040       2017.05.01 14:30
050       http://arxiv.org/abs/1610.00479
*** 1992
010       A New Data Representation Based on Training Data Characteristics to   Extract Drug Named-Entity in Medical Text
021       One essential task in information extraction from the medical corpus is drug name recognition. 
020       Compared with text sources come from other domains, the medical text is special and has unique characteristics. In addition, the medical text mining poses more challenges, e.g., more unstructured text, the fast growing of new terms addition, a wide range of name variation for the same drug. The mining is even more challenging due to the lack of labeled dataset sources and external knowledge, as well as multiple token representations for a single drug name that is more common in the real application setting. Although many approaches have been proposed to overwhelm the task, some problems remained with poor F-score performance (less than 0.75). This paper presents a new treatment in data representation techniques to overcome some of those challenges. We propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training. The first technique is evaluated with the standard NN model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked Denoising Encoders). The third technique represents the sentence as a sequence that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term Memory). In extracting the drug name entities, the third technique gives the best F-score performance compared to the state of the art, with its average F-score being 0.8645. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.LG
007       rubr.cs.NE
007       name.Sadikin Mujiono
007       name.Mohamad Ivan Fanany
007       name.Chan Basaruddin
040       2016.10.06 14:38
050       http://arxiv.org/abs/1610.01891
*** 9326
010       Text Segmentation using Named Entity Recognition and Co-reference   Resolution in English and Greek Texts
021       In this paper we examine the benefit of performing named entity recognition (NER) and co-reference resolution to an English and a Greek corpus used for text segmentation. 
020       The aim here is to examine whether the combination of text segmentation and information extraction can be beneficial for the identification of the various topics that appear in a document. NER was performed manually in the English corpus and was compared with the output produced by publicly available annotation tools while, an already existing tool was used for the Greek corpus. Produced annotations from both corpora were manually corrected and enriched to cover four types of named entities. Co-reference resolution i.e., substitution of every reference of the same instance with the same named entity identifier was subsequently performed. The evaluation, using five text segmentation algorithms for the English corpus and four for the Greek corpus leads to the conclusion that, the benefit highly depends on the segment's topic, the number of named entity instances appearing in it, as well as the segment's length. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       name.Pavlina Fragkou
040       2016.10.28 14:09
050       http://arxiv.org/abs/1610.09226
*** 9389
010       Generalized Common Informations: Measuring Commonness by the Conditional   Maximal Correlation
021       In literature, different common informations were defined by G\'acs and K\"orner, by Wyner, and by Kumar, Li, and Gamal, respectively. 
020       In this paper, we define two generalized versions of common informations, named approximate and exact information-correlation functions, by exploiting the conditional maximal correlation as a commonness or privacy measure. These two generalized common informations encompass the notions of G\'acs-K\"orner's, Wyner's, and Kumar-Li-Gamal's common informations as special cases. Furthermore, to give operational characterizations of these two generalized common informations, we also study the problems of private sources synthesis and common information extraction, and show that the information-correlation functions are equal to the minimum rates of commonness needed to ensure that some conditional maximal correlation constraints are satisfied for the centralized setting versions of these problems. As a byproduct, the conditional maximal correlation has been studied as well. 
007       rubr.cs.IT
007       rubr.cs
007       rubr.cs.CR
007       rubr.cs.CR
007       rubr.math.PR
007       rubr.math.ST
007       name.Lei Yu
007       name.Houqiang Li
007       name.Chang Wen Chen
040       2017.07.24 14:08
050       http://arxiv.org/abs/1610.09289
*** 9690
010       A Scalable and Robust Framework for Intelligent Real-time Video   Surveillance
021       In this paper, we present an intelligent, reliable and storage-efficient video surveillance system using Apache Storm and OpenCV. 
020       As a Storm topology, we have added multiple information extraction modules that only write important content to the disk. Our topology is extensible, capable of adding novel algorithms as per the use case without affecting the existing ones, since all the processing is independent of each other. This framework is also highly scalable and fault tolerant, which makes it a best option for organisations that need to monitor a large network of surveillance cameras. 
007       rubr.cs.CV
007       rubr.cs
007       rubr.cs.DC
007       name.Shreenath Dutt
007       name.Ankita Kalra
040       2016.10.30 01:22
050       http://arxiv.org/abs/1610.09590
*** 9994
010       Mining Social Media for Open Innovation in Transportation Systems
021       This work proposes a novel framework for the development of new products and services in transportation through an open innovation approach based on automatic content analysis of social media data. 
020       The framework is able to extract users comments from Online Social Networks (OSN), to process and analyze text through information extraction and sentiment analysis techniques to obtain relevant information about product reception on the market. A use case was developed using the mobile application Uber, which is today one of the fastest growing technology companies in the world. We measured how a controversial, highly diffused event influences the volume of tweets about Uber and the perception of its users. While there is no change in the image of Uber, a large increase in the number of tweets mentioning the company is observed, which meant a free and important diffusion of its product. 
007       rubr.cs.CY
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.SI
007       name.Daniela Ulloa
007       name.Pedro Saleiro
007       name.Rosaldo J. F. Rossetti
007       name.Elis Regina Silva
040       2016.10.31 12:25
050       http://arxiv.org/abs/1610.09894
*** 315
010       Rapid Prototyping of a Text Mining Application for Cryptocurrency Market   Intelligence
021       Blockchain represents a technology for establishing a shared, immutable version of the truth between a network of participants that do not trust one another, and therefore has the potential to disrupt any financial or other industries that rely on third-parties to establish trust. 
020       Recent trends in computing including: prevalence of Free and Open Source Software (FOSS); easy access to High Performance Computing (HPC i.e. 'The Cloud'); and increasingly advanced analytics capabilities such as Natural Language Processing (NLP) and Machine Learning (ML) allow for rapidly prototyping applications for analysis of trends in the emergence of Blockchain technology. A scaleable proof-of-concept pipeline that lays the groundwork for analysis of multiple streams of semi-structured data posted on social media is demonstrated. Preliminary analysis and performance metrics are presented and discussed. Future work is described that will scale the system to cloud-based, real-time, analysis of multiple data streams, with Information Extraction (IE) (ex. sentiment analysis) and Machine Learning capability. 
007       rubr.cs.CY
007       rubr.cs
007       rubr.cs.ET
007       name.Marek Laskowski
007       name.Henry M. Kim
040       2016.08.28 20:04
050       http://arxiv.org/abs/1611.00315
*** 2454
010       Building a comprehensive syntactic and semantic corpus of Chinese   clinical texts
021       Objective: To build a comprehensive corpus covering syntactic and semantic annotations of Chinese clinical texts with corresponding annotation guidelines and methods as well as to develop tools trained on the annotated corpus, which supplies baselines for research on Chinese texts in the clinical domain. 
020       <br />Materials and methods: An iterative annotation method was proposed to train annotators and to develop annotation guidelines. Then, by using annotation quality assurance measures, a comprehensive corpus was built, containing annotations of part-of-speech (POS) tags, syntactic tags, entities, assertions, and relations. Inter-annotator agreement (IAA) was calculated to evaluate the annotation quality and a Chinese clinical text processing and information extraction system (CCTPIES) was developed based on our annotated corpus. <br />Results: The syntactic corpus consists of 138 Chinese clinical documents with 47,424 tokens and 2553 full parsing trees, while the semantic corpus includes 992 documents that annotated 39,511 entities with their assertions and 7695 relations. IAA evaluation shows that this comprehensive corpus is of good quality, and the system modules are effective. <br />Discussion: The annotated corpus makes a considerable contribution to natural language processing (NLP) research into Chinese texts in the clinical domain. However, this corpus has a number of limitations. Some additional types of clinical text should be introduced to improve corpus coverage and active learning methods should be utilized to promote annotation efficiency. <br />Conclusions: In this study, several annotation guidelines and an annotation method for Chinese clinical texts were proposed, and a comprehensive corpus with its NLP modules were constructed, providing a foundation for further study of applying NLP techniques to Chinese texts in the clinical domain. 
007       rubr.cs.CL
007       name.Bin He
007       name.Bin Dong
007       name.Yi Guan
007       name.Jinfeng Yang
007       name.Zhipeng Jiang
007       name.Qiubin Yu
007       name.Jianyi Cheng
007       name.Chunyan Qu
040       2016.11.08 08:48
050       http://arxiv.org/abs/1611.02091
*** 3202
010       Old Content and Modern Tools - Searching Named Entities in a Finnish   OCRed Historical Newspaper Collection 1771-1910
021       Named Entity Recognition (NER), search, classification and tagging of names and name like frequent informational elements in texts, has become a standard information extraction procedure for textual data. 
020       NER has been applied to many types of texts and different types of entities: newspapers, fiction, historical records, persons, locations, chemical compounds, protein families, animals etc. In general a NER system's performance is genre and domain dependent and also used entity categories vary (Nadeau and Sekine, 2007). The most general set of named entities is usually some version of three partite categorization of locations, persons and organizations. In this paper we report first large scale trials and evaluation of NER with data out of a digitized Finnish historical newspaper collection Digi. Experiments, results and discussion of this research serve development of the Web collection of historical Finnish newspapers. <br />Digi collection contains 1,960,921 pages of newspaper material from years 1771-1910 both in Finnish and Swedish. We use only material of Finnish documents in our evaluation. The OCRed newspaper collection has lots of OCR errors; its estimated word level correctness is about 70-75 % (Kettunen and P\"a\"akk\"onen, 2016). Our principal NER tagger is a rule-based tagger of Finnish, FiNER, provided by the FIN-CLARIN consortium. We show also results of limited category semantic tagging with tools of the Semantic Computing Research Group (SeCo) of the Aalto University. Three other tools are also evaluated briefly. <br />This research reports first published large scale results of NER in a historical Finnish OCRed newspaper collection. Results of the research supplement NER results of other languages with similar noisy data. 
007       rubr.cs.CL
007       name.Kimmo Kettunen
007       name.Eetu M&#xe4;kel&#xe4;
007       name.Teemu Ruokolainen
007       name.Juha Kuokkala
007       name.Laura L&#xf6;fberg
040       2016.11.09 07:37
050       http://arxiv.org/abs/1611.02839
*** 9383
010       Developing a cardiovascular disease risk factor annotated corpus of   Chinese electronic medical records
021       Cardiovascular disease (CVD) has become the leading cause of death in China, and most of the cases can be prevented by controlling risk factors. 
020       The goal of this study was to build a corpus of CVD risk factor annotations based on Chinese electronic medical records (CEMRs). This corpus is intended to be used to develop a risk factor information extraction system that, in turn, can be applied as a foundation for the further study of the progress of risk factors and CVD. We designed a light annotation task to capture CVD risk factors with indicators, temporal attributes and assertions that were explicitly or implicitly displayed in the records. The task included: 1) preparing data; 2) creating guidelines for capturing annotations (these were created with the help of clinicians); 3) proposing an annotation method including building the guidelines draft, training the annotators and updating the guidelines, and corpus construction. Then, a risk factor annotated corpus based on de-identified discharge summaries and progress notes from 600 patients was developed. Built with the help of clinicians, this corpus has an inter-annotator agreement (IAA) F1-measure of 0.968, indicating a high reliability. To the best of our knowledge, this is the first annotated corpus concerning CVD risk factors in CEMRs and the guidelines for capturing CVD risk factor annotations from CEMRs were proposed. The obtained document-level annotations can be applied in future studies to monitor risk factors and CVD over the long term. 
007       rubr.cs.CL
007       name.Jia Su
007       name.Bin He
007       name.Yi Guan
007       name.Jingchi Jiang
007       name.Jinfeng Yang
040       2017.03.03 08:52
050       http://arxiv.org/abs/1611.09020
*** 4118
010       Information Extraction with Character-level Neural Networks and Free   Noisy Supervision
021       We present an architecture for information extraction from text that augments an existing parser with a character-level neural network. 
020       The network is trained using a measure of consistency of extracted data with existing databases as a form of noisy supervision. Our architecture combines the ability of constraint-based information extraction systems to easily incorporate domain knowledge and constraints with the ability of deep neural networks to leverage large amounts of data to learn complex features. Boosting the existing parser's precision, the system led to large improvements over a mature and highly tuned constraint-based production information extraction system used at Bloomberg for financial language text. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       rubr.cs.LG
007       name.Philipp Meerkamp
007       name.Zhengyi Zhou
040       2017.01.24 01:01
050       http://arxiv.org/abs/1612.04118
*** 6162
010       The iCrawl Wizard -- Supporting Interactive Focused Crawl Specification
021       Collections of Web documents about specific topics are needed for many areas of current research. 
020       Focused crawling enables the creation of such collections on demand. Current focused crawlers require the user to manually specify starting points for the crawl (seed URLs). These are also used to describe the expected topic of the collection. The choice of seed URLs influences the quality of the resulting collection and requires a lot of expertise. In this demonstration we present the iCrawl Wizard, a tool that assists users in defining focused crawls efficiently and semi-automatically. Our tool uses major search engines and Social Media APIs as well as information extraction techniques to find seed URLs and a semantic description of the crawl intent. Using the iCrawl Wizard even non-expert users can create semantic specifications for focused crawlers interactively and efficiently. 
007       rubr.cs.DL
007       rubr.cs
007       rubr.cs.IR
007       name.Gerhard Gossen
007       name.Elena Demidova
007       name.Thomas Risse
040       2016.12.19 13:09
050       http://arxiv.org/abs/1612.06162
*** 7495
010       Noise Mitigation for Neural Entity Typing and Relation Extraction
021       In this paper, we address two different types of noise in information extraction models: noise from distant supervision and noise from pipeline input features. 
020       Our target tasks are entity typing and relation extraction. For the first noise type, we introduce multi-instance multi-label learning algorithms using neural network models, and apply them to fine-grained entity typing for the first time. This gives our models comparable performance with the state-of-the-art supervised approach which uses global embeddings of entities. For the second noise type, we propose ways to improve the integration of noisy entity type predictions into relation extraction. Our experiments show that probabilistic predictions are more robust than discrete predictions and that joint training of the two tasks performs best. 
007       rubr.cs.CL
007       name.Yadollah Yaghoobzadeh
007       name.Heike Adel
007       name.Hinrich Sch&#xfc;tze
040       2017.01.10 15:01
050       http://arxiv.org/abs/1612.07495
*** 7843
010       &quot;What is Relevant in a Text Document?&quot;: An Interpretable Machine   Learning Approach
021       Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. 
020       Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       rubr.cs.IR
007       rubr.cs.LG
007       rubr.stat.ML
007       name.Leila Arras
007       name.Franziska Horn
007       name.Gr&#xe9;goire Montavon
007       name.Klaus-Robert M&#xfc;ller
007       name.Wojciech Samek
040       2016.12.23 00:31
050       http://arxiv.org/abs/1612.07843
*** 9327
010       Intelligent information extraction based on artificial neural network
021       Question Answering System (QAS) is used for information retrieval and natural language processing (NLP) to reduce human effort. 
020       There are numerous QAS based on the user documents present today, but they all are limited to providing objective answers and process simple questions only. Complex questions cannot be answered by the existing QAS, as they require interpretation of the current and old data as well as the question asked by the user. The above limitations can be overcome by using deep cases and neural network. Hence we propose a modified QAS in which we create a deep artificial neural network with associative memory from text documents. The modified QAS processes the contents of the text document provided to it and find the answer to even complex questions in the documents. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       name.Ahlam Ansari
007       name.Moonish Maknojia
007       name.Altamash Shaikh
040       2016.04.11 06:09
050       http://arxiv.org/abs/1612.09327
*** 5625
010       CEVO: Comprehensive EVent Ontology Enhancing Cognitive Annotation
021       While the general analysis of named entities has received substantial research attention, the analysis of relations over named entities has not. 
020       In fact, a review of the literature on unstructured as well as structured data revealed a deficiency in research on the abstract conceptualization required to organize relations. We believe that such an abstract conceptualization can benefit various communities and applications such as natural language processing, information extraction, machine learning and ontology engineering. In this paper, we present CEVO (i.e., a Comprehensive EVent Ontology) built on Levin's conceptual hierarchy of English verbs that categorizes verbs with the shared meaning and syntactic behavior. We present the fundamental concepts and requirements for this ontology. Furthermore, we present three use cases for demonstrating the benefits of this ontology on annotation tasks: 1) annotating relations in plain text, 2) annotating ontological properties and 3) linking textual relations to ontological properties. 
007       rubr.cs.CL
007       name.Saeedeh Shekarpour
007       name.Valerie Shalin
007       name.Krishnaprasad Thirunarayan
007       name.Amit P. Sheth
040       2017.01.19 22:14
050       http://arxiv.org/abs/1701.05625
*** 6481
010       Security Analysis of Cache Replacement Policies
021       Modern computer architectures share physical resources between different programs in order to increase area-, energy-, and cost-efficiency. 
020       Unfortunately, sharing often gives rise to side channels that can be exploited for extracting or transmitting sensitive information. We currently lack techniques for systematic reasoning about this interplay between security and efficiency. In particular, there is no established way for quantifying security properties of shared caches. <br />In this paper, we propose a novel model that enables us to characterize important security properties of caches. Our model encompasses two aspects: (1) The amount of information that can be absorbed by a cache, and (2) the amount of information that can effectively be extracted from the cache by an adversary. We use our model to compute both quantities for common cache replacement policies (FIFO, LRU, and PLRU) and to compare their isolation properties. We further show how our model for information extraction leads to an algorithm that can be used to improve the bounds delivered by the CacheAudit static analyzer. 
007       rubr.cs.CR
007       name.Pablo Ca&#xf1;ones
007       name.Boris K&#xf6;pf
007       name.Jan Reineke
040       2017.01.23 16:27
050       http://arxiv.org/abs/1701.06481
*** 1348
010       Information Estimation with Node Placement Strategy in 3D Wireless   Sensor Networks
021       The cluster formation in Three Dimensional Wireless Sensor Networks (3D-WSN) give rise to overlapping of signals due to spherical sensing range which leads to information redundancy in the network. 
020       To address this problem, we develop a sensing algorithm for 3D-WSN based on dodecahedron topology which we call Three Dimensional Distributed Clustering (3D-DC) algorithm. Using 3D-DC algorithm in 3D-WSN, accurate information extraction appears to be a major challenge due to the environmental noise where a Cluster Head (CH) node gathers and estimates information in each dodecahedron cluster. Hence, to extract precise information in each dodecahedron cluster, we propose Three Dimensional Information Estimation (3D-IE) algorithm. Moreover, Node deployment strategy also plays an important factor to maximize information accuracy in 3D-WSN. In most cases, sensor nodes are deployed deterministically or randomly. But both the deployment scenario are not aware of where to exactly place the sensor nodes to extract more information in terms of accuracy. Therefore, placing nodes in its appropriate positions in 3D-WSN is a challenging task. We propose a Three Dimensional Node Placement (3D-NP) algorithm which can find the possible nodes and their deployment strategy to maximize information accuracy in the network. We perform simulations using MATLAB to validate the 3D-DC, 3D-IE and 3D-NP, algorithms respectively. 
007       rubr.cs.NI
007       name.Jyotirmoy Karjee
007       name.H.S Jamadagni
040       2017.02.04 21:37
050       http://arxiv.org/abs/1702.01348
*** 4457
010       Automated Phrase Mining from Massive Text Corpora
021       As one of the fundamental tasks in text analysis, phrase mining aims at extracting quality phrases from a text corpus. 
020       Phrase mining is important in various tasks such as information extraction/retrieval, taxonomy construction, and topic modeling. Most existing methods rely on complex, trained linguistic analyzers, and thus likely have unsatisfactory performance on text corpora of new domains and genres without extra but expensive adaption. Recently, a few data-driven methods have been developed successfully for extraction of phrases from massive domain-specific text. However, none of the state-of-the-art models is fully automated because they require human experts for designing rules or labeling phrases. <br />Since one can easily obtain many quality phrases from public knowledge bases to a scale that is much larger than that produced by human experts, in this paper, we propose a novel framework for automated phrase mining, AutoPhrase, which leverages this large amount of high-quality phrases in an effective way and achieves better performance compared to limited human labeled phrases. In addition, we develop a POS-guided phrasal segmentation model, which incorporates the shallow syntactic information in part-of-speech (POS) tags to further enhance the performance, when a POS tagger is available. Note that, AutoPhrase can support any language as long as a general knowledge base (e.g., Wikipedia) in that language is available, while benefiting from, but not requiring, a POS tagger. Compared to the state-of-the-art methods, the new method has shown significant improvements in effectiveness on five real-world datasets across different domains and languages. 
007       rubr.cs.CL
007       name.Jingbo Shang
007       name.Jialu Liu
007       name.Meng Jiang
007       name.Xiang Ren
007       name.Clare R Voss
007       name.Jiawei Han
040       2017.03.11 19:33
050       http://arxiv.org/abs/1702.04457
*** 5396
010       Experiment Segmentation in Scientific Discourse as Clause-level   Structured Prediction using Recurrent Neural Networks
021       We propose a deep learning model for identifying structure within experiment narratives in scientific literature. 
020       We take a sequence labeling approach to this problem, and label clauses within experiment narratives to identify the different parts of the experiment. Our dataset consists of paragraphs taken from open access PubMed papers labeled with rhetorical information as a result of our pilot annotation. Our model is a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells that labels clauses. The clause representations are computed by combining word representations using a novel attention mechanism that involves a separate RNN. We compare this model against LSTMs where the input layer has simple or no attention and a feature rich CRF model. Furthermore, we describe how our work could be useful for information extraction from scientific literature. 
007       rubr.cs.CL
007       name.Pradeep Dasigi
007       name.Gully A.P.C. Burns
007       name.Eduard Hovy
007       name.Anita de Waard
040       2017.02.17 15:39
050       http://arxiv.org/abs/1702.05398
*** 6476
010       Syst\`emes du LIA \`a DEFT&#x27;13
021       The 2013 D\'efi de Fouille de Textes (DEFT) campaign is interested in two types of language analysis tasks, the document classification and the information extraction in the specialized domain of cuisine recipes. 
020       We present the systems that the LIA has used in DEFT 2013. Our systems show interesting results, even though the complexity of the proposed tasks. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       name.Xavier Bost
007       name.Ilaria Brunetti
007       name.Luis Adri&#xe1;n Cabrera-Diego
007       name.Jean-Val&#xe8;re Cossu
007       name.Andr&#xe9;a Linhares
007       name.Mohamed Morchid
007       name.Juan-Manuel Torres-Moreno
007       name.Marc El-B&#xe8;ze
007       name.Richard Dufour
040       2017.02.21 17:14
050       http://arxiv.org/abs/1702.06478
*** 7462
010       Deep Models Under the GAN: Information Leakage from Collaborative Deep   Learning
021       Deep Learning has recently become hugely popular in machine learning, providing significant improvements in classification accuracy in the presence of highly-structured and large databases. 
020       <br />Researchers have also considered privacy implications of deep learning. Models are typically trained in a centralized manner with all the data being processed by the same training algorithm. If the data is a collection of users' private data, including habits, personal pictures, geographical positions, interests, and more, the centralized server will have access to sensitive information that could potentially be mishandled. To tackle this problem, collaborative deep learning models have recently been proposed where parties locally train their deep learning structures and only share a subset of the parameters in the attempt to keep their respective training sets private. Parameters can also be obfuscated via differential privacy (DP) to make information extraction even more challenging, as proposed by Shokri and Shmatikov at CCS'15. <br />Unfortunately, we show that any privacy-preserving collaborative deep learning is susceptible to a powerful attack that we devise in this paper. In particular, we show that a distributed, federated, or decentralized deep learning approach is fundamentally broken and does not protect the training sets of honest participants. The attack we developed exploits the real-time nature of the learning process that allows the adversary to train a Generative Adversarial Network (GAN) that generates prototypical samples of the targeted training set that was meant to be private (the samples generated by the GAN are intended to come from the same distribution as the training data). Interestingly, we show that record-level DP applied to the shared parameters of the model, as suggested in previous work, is ineffective (i.e., record-level DP is not designed to address our attack). 
007       rubr.cs.CR
007       rubr.cs
007       rubr.cs.LG
007       rubr.cs.LG
007       rubr.stat.ML
007       name.Briland Hitaj
007       name.Giuseppe Ateniese
007       name.Fernando Perez-Cruz
040       2017.09.14 16:38
050       http://arxiv.org/abs/1702.07464
*** 8448
010       A Knowledge-Based Approach to Word Sense Disambiguation by   distributional selection and semantic features
021       Word sense disambiguation improves many Natural Language Processing (NLP) applications such as Information Retrieval, Information Extraction, Machine Translation, or Lexical Simplification. 
020       Roughly speaking, the aim is to choose for each word in a text its best sense. One of the most popular method estimates local semantic similarity relatedness between two word senses and then extends it to all words from text. The most direct method computes a rough score for every pair of word senses and chooses the lexical chain that has the best score (we can imagine the exponential complexity that returns this comprehensive approach). In this paper, we propose to use a combinatorial optimization metaheuristic for choosing the nearest neighbors obtained by distributional selection around the word to disambiguate. The test and the evaluation of our method concern a corpus written in French by means of the semantic network BabelNet. The obtained accuracy rate is 78 % on all names and verbs chosen for the evaluation. 
007       rubr.cs.CL
007       name.Mokhtar Billami
040       2017.02.27 13:37
050       http://arxiv.org/abs/1702.08450
*** 854
010       Learning the Structure of Generative Models without Labeled Data
021       Curating labeled training data has become the primary bottleneck in machine learning. 
020       Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources. The generative model's dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge. We propose a structure estimation method that maximizes the $\ell_1$-regularized marginal pseudolikelihood of the observed data. Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models. Simulations show that our method is 100$\times$ faster than a maximum likelihood approach and selects $1/4$ as many extraneous dependencies. We also show that our method provides an average of 1.5 F1 points of improvement over existing, user-developed information extraction applications on real-world data such as PubMed journal abstracts. 
007       rubr.cs.LG
007       rubr.cs
007       rubr.stat
007       rubr.stat.ML
007       name.Stephen H. Bach
007       name.Bryan He
007       name.Alexander Ratner
007       name.Christopher R&#xe9;
040       2017.09.09 21:22
050       http://arxiv.org/abs/1703.00854
*** 1386
010       Joining Extractions of Regular Expressions
021       Regular expressions with capture variables, also known as "regex formulas," extract relations of spans (interval positions) from text. 
020       These relations can be further manipulated via Relational Algebra as studied in the context of document spanners, Fagin et al.'s formal framework for information extraction. We investigate the complexity of querying text by Conjunctive Queries (CQs) and Unions of CQs (UCQs) on top of regex formulas. We show that the lower bounds (NP-completeness and W[1]-hardness) from the relational world also hold in our setting; in particular, hardness hits already single-character text! Yet, the upper bounds from the relational world do not carry over. Unlike the relational world, acyclic CQs, and even gamma-acyclic CQs, are hard to compute. The source of hardness is that it may be intractable to instantiate the relation defined by a regex formula, simply because it has an exponential number of tuples. Yet, we are able to establish general upper bounds. In particular, UCQs can be evaluated with polynomial delay, provided that every CQ has a bounded number of atoms (while unions and projection can be arbitrary). Furthermore, UCQ evaluation is solvable with FPT (Fixed-Parameter Tractable) delay when the parameter is the size of the UCQ. 
007       rubr.cs.DB
007       name.Dominik D. Freydenberger
007       name.Benny Kimelfeld
007       name.Liat Peterfreund
040       2017.03.30 08:27
050       http://arxiv.org/abs/1703.10350
*** 4106
010       Information Extraction in Illicit Domains
021       Extracting useful entities and attribute values from illicit domains such as human trafficking is a challenging problem with the potential for widespread social impact. 
020       Such domains employ atypical language models, have `long tails' and suffer from the problem of concept drift. In this paper, we propose a lightweight, feature-agnostic Information Extraction (IE) paradigm specifically designed for such domains. Our approach uses raw, unlabeled text from an initial corpus, and a few (12-120) seed annotations per domain-specific attribute, to learn robust IE models for unobserved pages and websites. Empirically, we demonstrate that our approach can outperform feature-centric Conditional Random Field baselines by over 18\% F-Measure on five annotated sets of real-world human trafficking datasets in both low-supervision and high-supervision settings. We also show that our approach is demonstrably robust to concept drift, and can be efficiently bootstrapped even in a serial computing environment. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       name.Mayank Kejriwal
007       name.Pedro Szekely
040       2017.03.09 01:28
050       http://arxiv.org/abs/1703.03097
*** 5222
010       MetaPAD: Meta Pattern Discovery from Massive Text Corpora
021       Mining textual patterns in news, tweets, papers, and many other kinds of text corpora has been an active theme in text mining and NLP research. 
020       Previous studies adopt a dependency parsing-based pattern discovery approach. However, the parsing results lose rich context around entities in the patterns, and the process is costly for a corpus of large scale. In this study, we propose a novel typed textual pattern structure, called meta pattern, which is extended to a frequent, informative, and precise subsequence pattern in certain context. We propose an efficient framework, called MetaPAD, which discovers meta patterns from massive corpora with three techniques: (1) it develops a context-aware segmentation method to carefully determine the boundaries of patterns with a learnt pattern quality assessment function, which avoids costly dependency parsing and generates high-quality patterns; (2) it identifies and groups synonymous meta patterns from multiple facets---their types, contexts, and extractions; and (3) it examines type distributions of entities in the instances extracted by each group of patterns, and looks for appropriate type levels to make discovered patterns precise. Experiments demonstrate that our proposed framework discovers high-quality typed textual patterns efficiently from different genres of massive corpora and facilitates information extraction. 
007       rubr.cs.CL
007       name.Meng Jiang
007       name.Jingbo Shang
007       name.Taylor Cassidy
007       name.Xiang Ren
007       name.Lance M. Kaplan
007       name.Timothy P. Hanratty
007       name.Jiawei Han
040       2017.03.14 20:26
050       http://arxiv.org/abs/1703.04213
*** 6037
010       Fonduer: Knowledge Base Construction from Richly Formatted Data
021       We introduce Fonduer, a knowledge base construction (KBC) framework for richly formatted information extraction (RFIE), where entity relations and attributes are conveyed via structural, tabular, visual, and textual expressions. 
020       Fonduer introduces a new programming model for KBC built around a unified data representation that accounts for three challenging characteristics of richly formatted data: (1) prevalent document-level relations, (2) multimodality, and (3) data variety. Fonduer is the first KBC system for richly formatted data and uses a human-in-the-loop paradigm for training machine learning systems, referred to as data programming. Data programming softens the burden of traditional supervision by only asking users to provide lightweight functions that programmatically assign (potentially noisy) labels to the input data. Fonduer's unified data model, together with data programming, allows users to use domain expertise as weak signals of supervision that help guide the KBC process over richly formatted data. We evaluate Fonduer on four real-world applications over different domains and achieve an average improvement of 42 F1 points over the upper bound of state-of-the-art approaches. In some domains, our users have produced up to 1.87x the number of correct entires compared to expert-curated public knowledge bases. Fonduer scales gracefully to millions of documents and is used in both academia and industry to create knowledge bases for real-world problems in many domains. 
007       rubr.cs.DB
007       name.Sen Wu
007       name.Luke Hsiao
007       name.Xiao Cheng
007       name.Braden Hancock
007       name.Theodoros Rekatsinas
007       name.Philip Levis
007       name.Christopher R&#xe9;
040       2017.03.15 09:12
050       http://arxiv.org/abs/1703.05028
*** 6329
010       Legal Question Answering using Ranking SVM and Deep Convolutional Neural   Network
021       This paper presents a study of employing Ranking SVM and Convolutional Neural Network for two missions: legal information retrieval and question answering in the Competition on Legal Information Extraction/Entailment. 
020       For the first task, our proposed model used a triple of features (LSI, Manhattan, Jaccard), and is based on paragraph level instead of article level as in previous studies. In fact, each single-paragraph article corresponds to a particular paragraph in a huge multiple-paragraph article. For the legal question answering task, additional statistical features from information retrieval task integrated into Convolutional Neural Network contribute to higher accuracy. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       name.Phong-Khac Do
007       name.Huy-Tien Nguyen
007       name.Chien-Xuan Tran
007       name.Minh-Tien Nguyen
007       name.Minh-Le Nguyen
040       2017.03.16 01:06
050       http://arxiv.org/abs/1703.05320
*** 6715
010       Improving Document Clustering by Eliminating Unnatural Language
021       Technical documents contain a fair amount of unnatural language, such as tables, formulas, pseudo-codes, etc. 
020       Unnatural language can be an important factor of confusing existing NLP tools. This paper presents an effective method of distinguishing unnatural language from natural language, and evaluates the impact of unnatural language detection on NLP tasks such as document clustering. We view this problem as an information extraction task and build a multiclass classification model identifying unnatural language components into four categories. First, we create a new annotated corpus by collecting slides and papers in various formats, PPT, PDF, and HTML, where unnatural language components are annotated into four categories. We then explore features available from plain text to build a statistical model that can handle any format as long as it is converted into plain text. Our experiments show that removing unnatural language components gives an absolute improvement in document clustering up to 15%. Our corpus and tool are publicly available. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       name.Myungha Jang
007       name.Jinho D. Choi
007       name.James Allan
040       2017.03.17 04:03
050       http://arxiv.org/abs/1703.05706
*** 6860
010       Temporal Information Extraction for Question Answering Using Syntactic   Dependencies in an LSTM-based Architecture
021       In this paper, we propose to use a set of simple, uniform in architecture LSTM-based models to recover different kinds of temporal relations from text. 
020       Using the shortest dependency path between entities as input, the same architecture is used to extract intra-sentence, cross-sentence, and document creation time relations. A "double-checking" technique reverses entity pairs in classification, boosting the recall of positive cases and reducing misclassifications between opposite classes. An efficient pruning algorithm resolves conflicts globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our proposed technique outperforms state-of-the-art methods by a large margin. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       name.Yuanliang Meng
007       name.Anna Rumshisky
007       name.Alexey Romanov
040       2017.10.05 21:38
050       http://arxiv.org/abs/1703.05851
*** 10579
010       A Tidy Data Model for Natural Language Processing using cleanNLP
021       The package cleanNLP provides a set of fast tools for converting a textual corpus into a set of normalized tables. 
020       The underlying natural language processing pipeline utilizes Stanford's CoreNLP library, exposing a number of annotation tasks for text written in English, French, German, and Spanish. Annotators include tokenization, part of speech tagging, named entity recognition, entity linking, sentiment analysis, dependency parsing, coreference resolution, and information extraction. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.stat
007       rubr.stat.CO
007       name.Taylor Arnold
040       2017.03.27 02:18
050       http://arxiv.org/abs/1703.09570
*** 440
010       Combining Lexical and Syntactic Features for Detecting Content-dense   Texts in News
021       Content-dense news report important factual information about an event in direct, succinct manner. 
020       Information seeking applications such as information extraction, question answering and summarization normally assume all text they deal with is content-dense. Here we empirically test this assumption on news articles from the business, U.S. international relations, sports and science journalism domains. Our findings clearly indicate that about half of the news texts in our study are in fact not content-dense and motivate the development of a supervised content-density detector. We heuristically label a large training corpus for the task and train a two-layer classifying model based on lexical and unlexicalized syntactic features. On manually annotated data, we compare the performance of domain-specific classifiers, trained on data only from a given news domain and a general classifier in which data from all four domains is pooled together. Our annotation and prediction experiments demonstrate that the concept of content density varies depending on the domain and that naive annotators provide judgement biased toward the stereotypical domain label. Domain-specific classifiers are more accurate for domains in which content-dense texts are typically fewer. Domain independent classifiers reproduce better naive crowdsourced judgements. Classification prediction is high across all conditions, around 80%. 
007       rubr.cs.CL
007       name.Yinfei Yang
007       name.Ani Nenkova
040       2017.04.03 06:22
050       http://arxiv.org/abs/1704.00440
*** 570
010       Spatiotemporal Networks for Video Emotion Recognition
021       Our experiment adapts several popular deep learning methods as well as some traditional methods on the problem of video emotion recognition. 
020       In our experiment, we use the CNN-LSTM architecture for visual information extraction and classification and utilize traditional methods such as for audio feature classification. For multimodal fusion, we use the traditional Support Vector Machine. Our experiment yields a good result on the AFEW 6.0 Dataset. 
007       rubr.cs.CV
007       name.Lijie Fan
007       name.Yunjie Ke
040       2017.04.12 14:39
050       http://arxiv.org/abs/1704.00570
*** 2446
010       Seismic facies recognition based on prestack data using deep   convolutional autoencoder
021       Prestack seismic data carries much useful information that can help us find more complex atypical reservoirs. 
020       Therefore, we are increasingly inclined to use prestack seismic data for seis- mic facies recognition. However, due to the inclusion of ex- cessive redundancy, effective feature extraction from prestack seismic data becomes critical. In this paper, we consider seis- mic facies recognition based on prestack data as an image clus- tering problem in computer vision (CV) by thinking of each prestack seismic gather as a picture. We propose a convo- lutional autoencoder (CAE) network for deep feature learn- ing from prestack seismic data, which is more effective than principal component analysis (PCA) in redundancy removing and valid information extraction. Then, using conventional classification or clustering techniques (e.g. K-means or self- organizing maps) on the extracted features, we can achieve seismic facies recognition. We applied our method to the prestack data from physical model and LZB region. The re- sult shows that our approach is superior to the conventionals. 
007       rubr.cs.CV
007       name.Feng Qian
007       name.Miao Yin
007       name.Ming-Jun Su
007       name.Yaojun Wang
007       name.Guangmin Hu
040       2017.04.08 06:17
050       http://arxiv.org/abs/1704.02446
*** 2853
010       SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations   from Scientific Publications
021       We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. 
020       Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.AI
007       rubr.stat.ML
007       name.Isabelle Augenstein
007       name.Mrinal Das
007       name.Sebastian Riedel
007       name.Lakshmi Vikraman
007       name.Andrew McCallum
040       2017.05.02 15:32
050       http://arxiv.org/abs/1704.02853
*** 4455
010       Cardinal Virtues: Extracting Relation Cardinalities from Text
021       Information extraction (IE) from text has largely focused on relations between individual entities, such as who has won which award. 
020       However, some facts are never fully mentioned, and no IE method has perfect recall. Thus, it is beneficial to also tap contents about the cardinalities of these relations, for example, how many awards someone has won. We introduce this novel problem of extracting cardinalities and discusses the specific challenges that set it apart from standard IE. We present a distant supervision method using conditional random fields. A preliminary evaluation results in precision between 3% and 55%, depending on the difficulty of relations. 
007       rubr.cs.CL
007       name.Paramita Mirza
007       name.Simon Razniewski
007       name.Fariz Darari
007       name.Gerhard Weikum
040       2017.05.27 00:55
050       http://arxiv.org/abs/1704.04455
*** 5572
010       Answering Complex Questions Using Open Information Extraction
021       While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a large body of knowledge and inference techniques. 
020       Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for QA, but to date such knowledge has only been used to answer simple questions with retrieval-based methods. We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for Open IE, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples. Our model significantly outperforms a state-of-the-art structured solver on complex questions of varying difficulty, while also removing the reliance on manually curated knowledge. 
007       rubr.cs.AI
007       rubr.cs
007       rubr.cs.CL
007       name.Tushar Khot
007       name.Ashish Sabharwal
007       name.Peter Clark
040       2017.04.19 01:07
050       http://arxiv.org/abs/1704.05572
*** 2270
010       Models and information-theoretic bounds for nanopore sequencing
021       Nanopore sequencing is an emerging new technology for sequencing DNA, which can read long fragments of DNA (~50,000 bases) in contrast to most current (short-read) sequencing technologies which can only read hundreds of bases. 
020       While nanopore sequencers can acquire long reads, the high error rates (20%-30%) pose a technical challenge. In a nanopore sequencer, a DNA is migrated through a nanopore and current variations are measured. The DNA sequence is inferred from this observed current pattern using an algorithm called a base-caller. In this paper, we propose a mathematical model for the "channel" from the input DNA sequence to the observed current, and calculate bounds on the information extraction capacity of the nanopore sequencer. This model incorporates impairments like (non-linear) inter-symbol interference, deletions, as well as random response. The potential practical application of such information bounds is two-fold: (1) benchmarking present base-calling algorithms against those theoretical bounds, and (2) offering an optimization objective for designing better nanopore sequencers. 
007       rubr.cs.IT
007       rubr.cs
007       name.Wei Mao
007       name.Suhas Diggavi
007       name.Sreeram Kannan
040       2017.05.31 15:47
050       http://arxiv.org/abs/1705.11154
*** 4840
010       A Survey of Deep Learning Methods for Relation Extraction
021       Relation Extraction is an important sub-task of Information Extraction which has the potential of employing deep learning (DL) models with the creation of large datasets using distant supervision. 
020       In this review, we compare the contributions and pitfalls of the various DL models that have been used for the task, to help guide the path ahead. 
007       rubr.cs.CL
007       name.Shantanu Kumar
040       2017.05.10 08:05
050       http://arxiv.org/abs/1705.03645
*** 6632
010       A Biomedical Information Extraction Primer for NLP Researchers
021       Biomedical Information Extraction is an exciting field at the crossroads of Natural Language Processing, Biology and Medicine. 
020       It encompasses a variety of different tasks that require application of state-of-the-art NLP techniques, such as NER and Relation Extraction. This paper provides an overview of the problems in the field and discusses some of the techniques used for solving them. 
007       rubr.cs.CL
007       name.Surag Nair
040       2017.05.10 10:00
050       http://arxiv.org/abs/1705.05437
*** 7658
010       Universal Dependencies Parsing for Colloquial Singaporean English
021       Singlish can be interesting to the ACL community both linguistically as a major creole based on English, and computationally for information extraction and sentiment analysis of regional social media. 
020       We investigate dependency parsing of Singlish by constructing a dependency treebank under the Universal Dependencies scheme, and then training a neural network model by integrating English syntactic knowledge into a state-of-the-art parser trained on the Singlish treebank. Results show that English knowledge can lead to 25% relative error reduction, resulting in a parser of 84.47% accuracies. To the best of our knowledge, we are the first to use neural stacking to improve cross-lingual dependency parsing on low-resource languages. We make both our annotation and parser available for further research. 
007       rubr.cs.CL
007       name.Hongmin Wang
007       name.Yue Zhang
007       name.GuangYong Leonard Chan
007       name.Jie Yang
007       name.Hai Leong Chieu
040       2017.05.18 08:27
050       http://arxiv.org/abs/1705.06463
*** 1336
010       Improving Legal Information Retrieval by Distributional Composition with   Term Order Probabilities
021       Legal professionals worldwide are currently trying to get up-to-pace with the explosive growth in legal document availability through digital means. 
020       This drives a need for high efficiency Legal Information Retrieval (IR) and Question Answering (QA) methods. The IR task in particular has a set of unique challenges that invite the use of semantic motivated NLP techniques. In this work, a two-stage method for Legal Information Retrieval is proposed, combining lexical statistics and distributional sentence representations in the context of Competition on Legal Information Extraction/Entailment (COLIEE). The combination is done with the use of disambiguation rules, applied over the rankings obtained through n-gram statistics. After the ranking is done, its results are evaluated for ambiguity, and disambiguation is done if a result is decided to be unreliable for a given query. Competition and experimental results indicate small gains in overall retrieval performance using the proposed approach. Additionally, an analysis of error and improvement cases is presented for a better understanding of the contributions. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       name.Danilo S. Carvalho
007       name.Duc-Vu Tran
007       name.Van-Khanh Tran
007       name.Le-Nguyen Minh
040       2017.06.10 10:49
050       http://arxiv.org/abs/1706.01038
*** 5373
010       Joint Extraction of Entities and Relations Based on a Novel Tagging   Scheme
021       Joint extraction of entities and relations is an important task in information extraction. 
020       To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What's more, the end-to-end model proposed in this paper, achieves the best results on the public dataset. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.LG
007       name.Suncong Zheng
007       name.Feng Wang
007       name.Hongyun Bao
007       name.Yuexing Hao
007       name.Peng Zhou
007       name.Bo Xu
040       2017.06.07 03:14
050       http://arxiv.org/abs/1706.05075
*** 6476
010       Infinite Mixture Model of Markov Chains
021       We propose a Bayesian nonparametric mixture model for prediction- and information extraction tasks with an efficient inference scheme. 
020       It models categorical-valued time series that exhibit dynamics from multiple underlying patterns (e.g. user behavior traces). We simplify the idea of capturing these patterns by hierarchical hidden Markov models (HHMMs) - and extend the existing approaches by the additional representation of structural information. Our empirical results are based on both synthetic- and real world data. They indicate that the results are easily interpretable, and that the model excels at segmentation and prediction performance: it successfully identifies the generating patterns and can be used for effective prediction of future observations. 
007       rubr.stat.ML
007       name.Jan Reubold
007       name.Thorsten Strufe
007       name.Ulf Brefeld
040       2017.06.19 21:08
050       http://arxiv.org/abs/1706.06178
*** 166
010       Heterogeneous Supervision for Relation Extraction: A Representation   Learning Approach
021       Relation extraction is a fundamental task in information extraction. 
020       Most existing methods have heavy reliance on annotations labeled by human experts, which are costly and time-consuming. To overcome this drawback, we propose a novel framework, REHession, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics. These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task: how to infer the true label from noisy labels for a given instance. Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion. Extensive experimental results demonstrate the superiority of REHession over the state-of-the-art. 
007       rubr.cs.CL
007       name.Liyuan Liu
007       name.Xiang Ren
007       name.Qi Zhu
007       name.Shi Zhi
007       name.Huan Gui
007       name.Heng Ji
007       name.Jiawei Han
040       2017.08.01 18:25
050       http://arxiv.org/abs/1707.00166
*** 827
010       Document Spanners for Extracting Incomplete Information: Expressiveness   and Complexity
021       Rule-based information extraction has lately received a fair amount of attention from the database community, with several languages appearing in the last few years. 
020       Although information extraction systems are intended to deal with semistructured data, all language proposals introduced so far are designed to output relations, thus making them incapable of handling incomplete information. To remedy the situation, we propose to extend information extraction languages with the ability to use mappings, thus allowing us to work with documents which have missing or optional parts. Using this approach, we simplify the semantics of regex formulas and extraction rules, two previously defined methods for extracting information, extend them with the ability to handle incomplete data, and study how they compare in terms of expressive power. We also study computational properties of these languages, focusing on the query enumeration problem, as well as satisfiability and containment. 
007       rubr.cs.DB
007       name.Francisco Maturana
007       name.Cristian Riveros
007       name.Domagoj Vrgo&#x10d;
040       2017.12.29 16:58
050       http://arxiv.org/abs/1707.00827
*** 1916
010       Event Schema Induction using Tensor Factorization with Back-off
021       The goal of Event Schema Induction(ESI) is to identify schemas of events from a corpus of documents. 
020       For example, given documents from the sports domain, we would like to infer that win(WinningPlayer, Trophy, OpponentPlayer, Location) is an important event schema for this domain. Automatic discovery of such event schemas is an important first step towards building domain-specific Knowledge Graphs (KGs). ESI has been the focus of some prior research, with generative models achieving the best performance. In this paper,we propose TFB, a tensor factorization-based method with back-off for ESI. TFB solves a novel objective to factorize Open Information Extraction (OpenIE) tuples for inducing binary schemas. Event schemas are induced out of this set of binary schemas by solving a constrained clique problem. To the best of our knowledge this is the first application of tensor factorization for the ESI problem. TFB outperforms current state-of-the-art by 52 (absolute) points gain in accuracy, while achieving 90x speedup on average. We hope to make all the code and datasets used in the paper publicly available upon publication of the paper. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       name.Madhav Nimishakavi
007       name.Partha Talukdar
040       2017.07.06 18:02
050       http://arxiv.org/abs/1707.01917
*** 3996
010       A Web-Based Tool for Analysing Normative Documents in English
021       Our goal is to use formal methods to analyse normative documents written in English, such as privacy policies and service-level agreements. 
020       This requires the combination of a number of different elements, including information extraction from natural language, formal languages for model representation, and an interface for property specification and verification. We have worked on a collection of components for this task: a natural language extraction tool, a suitable formalism for representing such documents, an interface for building models in this formalism, and methods for answering queries asked of a given model. In this work, each of these concerns is brought together in a web-based tool, providing a single interface for analysing normative texts in English. Through the use of a running example, we describe each component and demonstrate the workflow established by our tool. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.CY
007       name.John J. Camilleri
007       name.Mohammad Reza Haghshenas
007       name.Gerardo Schneider
040       2017.07.13 07:22
050       http://arxiv.org/abs/1707.03997
*** 4243
010       Lithium NLP: A System for Rich Information Extraction from Noisy User   Generated Text on Social Media
021       In this paper, we describe the Lithium Natural Language Processing (NLP) system - a resource-constrained, high- throughput and language-agnostic system for information extraction from noisy user generated text on social media. 
020       Lithium NLP extracts a rich set of information including entities, topics, hashtags and sentiment from text. We discuss several real world applications of the system currently incorporated in Lithium products. We also compare our system with existing commercial and academic NLP systems in terms of performance, information extracted and languages supported. We show that Lithium NLP is at par with and in some cases, outperforms state- of-the-art commercial NLP systems. 
007       rubr.cs.AI
007       rubr.cs
007       rubr.cs.CL
007       rubr.cs.IR
007       name.Preeti Bhargava
007       name.Nemanja Spasojevic
007       name.Guoning Hu
040       2017.07.13 17:52
050       http://arxiv.org/abs/1707.04244
*** 4912
010       End-to-End Information Extraction without Token-Level Supervision
021       Most state-of-the-art information extraction approaches rely on token-level labels to find the areas of interest in text. 
020       Unfortunately, these labels are time-consuming and costly to create, and consequently, not available for many real-life IE tasks. To make matters worse, token-level labels are usually not the desired output, but just an intermediary step. End-to-end (E2E) models, which take raw text as input and produce the desired output directly, need not depend on token-level labels. We propose an E2E model based on pointer networks, which can be trained directly on pairs of raw input and output text. We evaluate our model on the ATIS data set, MIT restaurant corpus and the MIT movie corpus and compare to neural baselines that do use token-level labels. We achieve competitive results, within a few percentage points of the baselines, showing the feasibility of E2E information extraction without the need for token-level labels. This opens up new possibilities, as for many tasks currently addressed by human extractors, raw input and output data are available, but not token-level labels. 
007       rubr.cs.CL
007       name.Rasmus Berg Palm
007       name.Dirk Hovy
007       name.Florian Laws
007       name.Ole Winther
040       2017.07.16 16:57
050       http://arxiv.org/abs/1707.04913
*** 5469
010       DeepProbe: Information Directed Sequence Understanding and Chatbot   Design via Recurrent Neural Networks
021       Information extraction and user intention identification are central topics in modern query understanding and recommendation systems. 
020       In this paper, we propose DeepProbe, a generic information-directed interaction framework which is built around an attention-based sequence to sequence (seq2seq) recurrent neural network. DeepProbe can rephrase, evaluate, and even actively ask questions, leveraging the generative ability and likelihood estimation made possible by seq2seq models. DeepProbe makes decisions based on a derived uncertainty (entropy) measure conditioned on user inputs, possibly with multiple rounds of interactions. Three applications, namely a rewritter, a relevance scorer and a chatbot for ad recommendation, were built around DeepProbe, with the first two serving as precursory building blocks for the third. We first use the seq2seq model in DeepProbe to rewrite a user query into one of standard query form, which is submitted to an ordinary recommendation system. Secondly, we evaluate DeepProbe's seq2seq model-based relevance scoring. Finally, we build a chatbot prototype capable of making active user interactions, which can ask questions that maximize information gain, allowing for a more efficient user intention idenfication process. We evaluate first two applications by 1) comparing with baselines by BLEU and AUC, and 2) human judge evaluation. Both demonstrate significant improvements compared with current state-of-the-art systems, proving their values as useful tools on their own, and at the same time laying a good foundation for the ongoing chatbot application. 
007       rubr.stat.ML
007       rubr.cs
007       rubr.cs.LG
007       name.Zi Yin
007       name.Keng-hao Chang
007       name.Ruofei Zhang
040       2017.07.18 05:12
050       http://arxiv.org/abs/1707.05470
*** 5849
010       A Short Survey of Biomedical Relation Extraction Techniques
021       Biomedical information is growing rapidly in the recent years and retrieving useful data through information extraction system is getting more attention. 
020       In the current research, we focus on different aspects of relation extraction techniques in biomedical domain and briefly describe the state-of-the-art for relation extraction between a variety of biological elements. 
007       rubr.cs.CL
007       name.Elham Shahab
040       2017.07.25 21:00
050       http://arxiv.org/abs/1707.05850
*** 7498
010       Analysing Errors of Open Information Extraction Systems
021       We report results on benchmarking Open Information Extraction (OIE) systems using RelVis, a toolkit for benchmarking Open Information Extraction systems. 
020       Our comprehensive benchmark contains three data sets from the news domain and one data set from Wikipedia with overall 4522 labeled sentences and 11243 binary or n-ary OIE relations. In our analysis on these data sets we compared the performance of four popular OIE systems, ClausIE, OpenIE 4.2, Stanford OpenIE and PredPatt. In addition, we evaluated the impact of five common error classes on a subset of 749 n-ary tuples. From our deep analysis we unreveal important research directions for a next generation of OIE systems. 
007       rubr.cs.CL
007       name.Rudolf Schneider
007       name.Tom Oberhauser
007       name.Tobias Klatt
007       name.Felix A. Gers
007       name.Alexander L&#xf6;ser
040       2017.07.24 11:49
050       http://arxiv.org/abs/1707.07499
*** 7793
010       Relational Learning and Feature Extraction by Querying over   Heterogeneous Information Networks
021       Many real world systems need to operate on heterogeneous information networks that consist of numerous interacting components of different types. 
020       Examples include systems that perform data analysis on biological information networks; social networks; and information extraction systems processing unstructured data to convert raw text to knowledge graphs. Many previous works describe specialized approaches to perform specific types of analysis, mining and learning on such networks. In this work, we propose a unified framework consisting of a data model -a graph with a first order schema along with a declarative language for constructing, querying and manipulating such networks in ways that facilitate relational and structured machine learning. In particular, we provide an initial prototype for a relational and graph traversal query language where queries are directly used as relational features for structured machine learning models. Feature extraction is performed by making declarative graph traversal queries. Learning and inference models can directly operate on this relational representation and augment it with new data and knowledge that, in turn, is integrated seamlessly into the relational structure to support new predictions. We demonstrate this system's capabilities by showcasing tasks in natural language processing and computational biology domains. 
007       rubr.cs.AI
007       rubr.cs
007       rubr.cs.DB
007       name.Parisa Kordjamshidi
007       name.Sameer Singh
007       name.Daniel Khashabi
007       name.Christos Christodoulopoulos
007       name.Mark Summons
007       name.Saurabh Sinha
007       name.Dan Roth
040       2017.07.25 02:32
050       http://arxiv.org/abs/1707.07794
*** 9610
010       Joint Named Entity Recognition and Stance Detection in Tweets
021       Named entity recognition (NER) is a well-established task of information extraction which has been studied for decades. 
020       More recently, studies reporting NER experiments on social media texts have emerged. On the other hand, stance detection is a considerably new research topic usually considered within the scope of sentiment analysis. Stance detection studies are mostly applied to texts of online debates where the stance of the text owner for a particular target, either explicitly or implicitly mentioned in text, is explored. In this study, we investigate the possible contribution of named entities to the stance detection task in tweets. We report the evaluation results of NER experiments as well as that of the subsequent stance detection experiments using named entities, on a publicly-available stance-annotated data set of tweets. Our results indicate that named entities obtained with a high-performance NER system can contribute to stance detection performance on tweets. 
007       rubr.cs.CL
007       name.Dilek K&#xfc;&#xe7;&#xfc;k
040       2017.07.30 11:34
050       http://arxiv.org/abs/1707.09611
*** 247
010       Query Expansion Techniques for Information Retrieval: a Survey
021       With the ever increasing size of web, relevant information extraction on the Internet with a query formed by a few keywords has become a big challenge. 
020       To overcome this, query expansion (QE) plays a crucial role in improving the Internet searches, where the user's initial query is reformulated to a new query by adding new meaningful terms with similar significance. QE -- as part of information retrieval (IR) -- has long attracted researchers' attention. It has also become very influential in the field of personalized social document, Question Answering over Linked Data (QALD), and, Text Retrieval Conference (TREC) and REAL sets. This paper surveys QE techniques in IR from 1960 to 2017 with respect to core techniques, data sources used, weighting and ranking methodologies, user participation and applications (of QE techniques) -- bringing out similarities and differences. 
007       rubr.cs.IR
007       name.Hiteshwar Kumar Azad
007       name.Akshay Deepak
040       2017.08.01 11:04
050       http://arxiv.org/abs/1708.00247
*** 553
010       Low-Rank Hidden State Embeddings for Viterbi Sequence Labeling
021       In textual information extraction and other sequence labeling tasks it is now common to use recurrent neural networks (such as LSTM) to form rich embedded representations of long-term input co-occurrence patterns. 
020       Representation of output co-occurrence patterns is typically limited to a hand-designed graphical model, such as a linear-chain CRF representing short-term Markov dependencies among successive labels. This paper presents a method that learns embedded representations of latent output structure in sequence data. Our model takes the form of a finite-state machine with a large number of latent states per label (a latent variable CRF), where the state-transition matrix is factorized---effectively forming an embedded representation of state-transitions capable of enforcing long-term label dependencies, while supporting exact Viterbi inference over output labels. We demonstrate accuracy improvements and interpretable latent structure in a synthetic but complex task based on CoNLL named entity recognition. 
007       rubr.cs.CL
007       name.Dung Thai
007       name.Shikhar Murty
007       name.Trapit Bansal
007       name.Luke Vilnis
007       name.David Belanger
007       name.Andrew McCallum
040       2017.08.02 00:05
050       http://arxiv.org/abs/1708.00553
*** 2139
010       STARDATA: A StarCraft AI Research Dataset
021       We release a dataset of 65646 StarCraft replays that contains 1535 million frames and 496 million player actions. 
020       We provide full game state data along with the original replays that can be viewed in StarCraft. The game state data was recorded every 3 frames which ensures suitability for a wide variety of machine learning tasks such as strategy classification, inverse reinforcement learning, imitation learning, forward modeling, partial information extraction, and others. We use TorchCraft to extract and store the data, which standardizes the data format for both reading from replays and reading directly from the game. Furthermore, the data can be used on different operating systems and platforms. The dataset contains valid, non-corrupted replays only and its quality and diversity was ensured by a number of heuristics. We illustrate the diversity of the data with various statistics and provide examples of tasks that benefit from the dataset. We make the dataset available at <a href="https://github.com/TorchCraft/StarData">this https URL</a> . En Taro Adun! 
007       rubr.cs.AI
007       name.Zeming Lin
007       name.Jonas Gehring
007       name.Vasil Khalidov
007       name.Gabriel Synnaeve
040       2017.08.07 14:47
050       http://arxiv.org/abs/1708.02139
*** 4120
010       Putting Self-Supervised Token Embedding on the Tables
021       Information distribution by electronic messages is a privileged means of transmission for many businesses and individuals, often under the form of plain-text tables. 
020       As their number grows, it becomes necessary to use an algorithm to extract text and numbers instead of a human. Usual methods are focused on regular expressions or on a strict structure in the data, but are not efficient when we have many variations, fuzzy structure or implicit labels. In this paper we introduce SC2T, a totally self-supervised model for constructing vector representations of tokens in semi-structured messages by using characters and context levels that address these issues. It can then be used for an unsupervised labeling of tokens, or be the basis for a semi-supervised information extraction system. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       name.Marc Szafraniec
007       name.Gautier Marti
007       name.Philippe Donnat
040       2017.10.25 12:53
050       http://arxiv.org/abs/1708.04120
*** 4592
010       Gold Standard Online Debates Summaries and First Experiments Towards   Automatic Summarization of Online Debate Data
021       Usage of online textual media is steadily increasing. 
020       Daily, more and more news stories, blog posts and scientific articles are added to the online volumes. These are all freely accessible and have been employed extensively in multiple research areas, e.g. automatic text summarization, information retrieval, information extraction, etc. Meanwhile, online debate forums have recently become popular, but have remained largely unexplored. For this reason, there are no sufficient resources of annotated debate data available for conducting research in this genre. In this paper, we collected and annotated debate data for an automatic summarization task. Similar to extractive gold standard summary generation our data contains sentences worthy to include into a summary. Five human annotators performed this task. Inter-annotator agreement, based on semantic similarity, is 36% for Cohen's kappa and 48% for Krippendorff's alpha. Moreover, we also implement an extractive summarization system for online debates and discuss prominent features for the task of summarizing online debate data automatically. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.IR
007       name.Nattapong Sanchan
007       name.Ahmet Aker
007       name.Kalina Bontcheva
040       2017.08.15 16:52
050       http://arxiv.org/abs/1708.04592
*** 5148
010       Natural Language Processing: State of The Art, Current Trends and   Challenges
021       Natural language processing (NLP) has recently gained much attention for representing and analysing human language computationally. 
020       It has spread its applications in various fields such as machine translation, email spam detection, information extraction, summarization, medical, and question answering etc. The paper distinguishes four phases by discussing different levels of NLP and components of Natural Language Generation (NLG) followed by presenting the history and evolution of NLP, state of the art presenting the various applications of NLP and current trends and challenges. 
007       rubr.cs.CL
007       name.Diksha Khurana
007       name.Aditya Koli
007       name.Kiran Khatter
007       name.Sukhdev Singh
040       2017.08.17 06:42
050       http://arxiv.org/abs/1708.05148
*** 6075
010       Scientific Information Extraction with Semi-supervised Neural Tagging
021       This paper addresses the problem of extracting keyphrases from scientific articles and categorizing them as corresponding to a task, process, or material. 
020       We cast the problem as sequence tagging and introduce semi-supervised methods to a neural tagging model, which builds on recent advances in named entity recognition. Since annotated training data is scarce in this domain, we introduce a graph-based semi-supervised algorithm together with a data selection scheme to leverage unannotated articles. Both inductive and transductive semi-supervised learning strategies outperform state-of-the-art information extraction performance on the 2017 SemEval Task 10 ScienceIE task. 
007       rubr.cs.CL
007       name.Yi Luan
007       name.Mari Ostendorf
007       name.Hannaneh Hajishirzi
040       2017.08.21 03:33
050       http://arxiv.org/abs/1708.06075
*** 9450
010       Learning Fine-Grained Knowledge about Contingent Relations between   Everyday Events
021       Much of the user-generated content on social media is provided by ordinary people telling stories about their daily lives. 
020       We develop and test a novel method for learning fine-grained common-sense knowledge from these stories about contingent (causal and conditional) relationships between everyday events. This type of knowledge is useful for text and story understanding, information extraction, question answering, and text summarization. We test and compare different methods for learning contingency relation, and compare what is learned from topic-sorted story collections vs. general-domain stories. Our experiments show that using topic-specific datasets enables learning finer-grained knowledge about events and results in significant improvement over the baselines. An evaluation on Amazon Mechanical Turk shows 82% of the relations between events that we learn from topic-sorted stories are judged as contingent. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       name.Elahe Rahimtoroghi
007       name.Ernesto Hernandez
007       name.Marilyn A Walker
040       2017.08.30 20:01
050       http://arxiv.org/abs/1708.09450
*** 9609
010       Identifying Products in Online Cybercrime Marketplaces: A Dataset for   Fine-grained Domain Adaptation
021       One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. 
020       In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects. We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate data from four different forums. Each of these forums constitutes its own "fine-grained domain" in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 1,938 annotated posts from across the four forums. 
007       rubr.cs.CL
007       name.Greg Durrett
007       name.Jonathan K. Kummerfeld
007       name.Taylor Berg-Kirkpatrick
007       name.Rebecca S. Portnoff
007       name.Sadia Afroz
007       name.Damon McCoy
007       name.Kirill Levchenko
007       name.Vern Paxson
040       2017.08.31 08:18
050       http://arxiv.org/abs/1708.09609
*** 2205
010       Semi-Supervised Recurrent Neural Network for Adverse Drug Reaction   Mention Extraction
021       Social media is an useful platform to share health-related information due to its vast reach. 
020       This makes it a good candidate for public-health monitoring tasks, specifically for pharmacovigilance. We study the problem of extraction of Adverse-Drug-Reaction (ADR) mentions from social media, particularly from twitter. Medical information extraction from social media is challenging, mainly due to short and highly information nature of text, as compared to more technical and formal medical reports. <br />Current methods in ADR mention extraction relies on supervised learning methods, which suffers from labeled data scarcity problem. The State-of-the-art method uses deep neural networks, specifically a class of Recurrent Neural Network (RNN) which are Long-Short-Term-Memory networks (LSTMs) \cite{hochreiter1997long}. Deep neural networks, due to their large number of free parameters relies heavily on large annotated corpora for learning the end task. But in real-world, it is hard to get large labeled data, mainly due to heavy cost associated with manual annotation. Towards this end, we propose a novel semi-supervised learning based RNN model, which can leverage unlabeled data also present in abundance on social media. Through experiments we demonstrate the effectiveness of our method, achieving state-of-the-art performance in ADR mention extraction. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       name.Shashank Gupta
007       name.Sachin Pawar
007       name.Nitin Ramrakhiyani
007       name.Girish Palshikar
007       name.Vasudeva Varma
040       2017.09.06 06:42
050       http://arxiv.org/abs/1709.01687
*** 3429
010       Semi-Supervised Instance Population of an Ontology using Word Vector   Embeddings
021       In many modern day systems such as information extraction and knowledge management agents, ontologies play a vital role in maintaining the concept hierarchies of the selected domain. 
020       However, ontology population has become a problematic process due to its nature of heavy coupling with manual human intervention. With the use of word embeddings in the field of natural language processing, it became a popular topic due to its ability to cope up with semantic sensitivity. Hence, in this study, we propose a novel way of semi-supervised ontology population through word embeddings as the basis. We built several models including traditional benchmark models and new types of models which are based on word embeddings. Finally, we ensemble them together to come up with a synergistic model with better accuracy. We demonstrate that our ensemble model can outperform the individual models. 
007       rubr.cs.CL
007       name.Vindula Jayawardana
007       name.Dimuthu Lakmal
007       name.Nisansa de Silva
007       name.Amal Shehan Perera
007       name.Keet Sugathadasa
007       name.Buddhi Ayesha
007       name.Madhavi Perera
040       2017.09.09 05:04
050       http://arxiv.org/abs/1709.02911
*** 3582
010       AppTechMiner: Mining Applications and Techniques from Scientific   Articles
021       This paper presents AppTechMiner, a rule-based information extraction framework that automatically constructs a knowledge base of all application areas and problem solving techniques. 
020       Techniques include tools, methods, datasets or evaluation metrics. We also categorize individual research articles based on their application areas and the techniques proposed/improved in the article. Our system achieves high average precision (~82%) and recall (~84%) in knowledge base creation. It also performs well in application and technique assignment to an individual article (average accuracy ~66%). In the end, we further present two use cases presenting a trivial information retrieval system and an extensive temporal analysis of the usage of techniques and application areas. At present, we demonstrate the framework for the domain of computational linguistics but this can be easily generalized to any other field of research. 
007       rubr.cs.CL
007       name.Mayank Singh
007       name.Soham Dan
007       name.Sanyam Agarwal
007       name.Pawan Goyal
007       name.Animesh Mukherjee
040       2017.11.10 06:13
050       http://arxiv.org/abs/1709.03064
*** 5592
010       A Deep Generative Framework for Paraphrase Generation
021       Paraphrase generation is an important problem in NLP, especially in question answering, information retrieval, information extraction, conversation systems, to name a few. 
020       In this paper, we address the problem of generating paraphrases automatically. Our proposed method is based on a combination of deep generative models (VAE) with sequence-to-sequence models (LSTM) to generate paraphrases, given an input sentence. Traditional VAEs when combined with recurrent neural networks can generate free text but they are not suitable for paraphrase generation for a given sentence. We address this problem by conditioning the both, encoder and decoder sides of VAE, on the original sentence, so that it can generate the given sentence's paraphrases. Unlike most existing models, our model is simple, modular and can generate multiple paraphrases, for a given sentence. Quantitative evaluation of the proposed method on a benchmark paraphrase dataset demonstrates its efficacy, and its performance improvement over the state-of-the-art methods by a significant margin, whereas qualitative human evaluation indicate that the generated paraphrases are well-formed, grammatically correct, and are relevant to the input sentence. Furthermore, we evaluate our method on a newly released question paraphrase dataset, and establish a new baseline for future research. 
007       rubr.cs.CL
007       name.Ankush Gupta
007       name.Arvind Agarwal
007       name.Prawaan Singh
007       name.Piyush Rai
040       2017.09.15 06:58
050       http://arxiv.org/abs/1709.05074
*** 8300
010       Mining User Queries with Information Extraction Methods and Linked Data
021       Purpose: Advanced usage of Web Analytics tools allows to capture the content of user queries. 
020       Despite their relevant nature, the manual analysis of large volumes of user queries is problematic. This paper demonstrates the potential of using information extraction techniques and Linked Data to gather a better understanding of the nature of user queries in an automated manner. <br />Design/methodology/approach: The paper presents a large-scale case-study conducted at the Royal Library of Belgium consisting of a data set of 83 854 queries resulting from 29 812 visits over a 12 month period of the historical newspapers platform BelgicaPress. By making use of information extraction methods, knowledge bases and various authority files, this paper presents the possibilities and limits to identify what percentage of end users are looking for person and place names. <br />Findings: Based on a quantitative assessment, our method can successfully identify the majority of person and place names from user queries. Due to the specific character of user queries and the nature of the knowledge bases used, a limited amount of queries remained too ambiguous to be treated in an automated manner. <br />Originality/value: This paper demonstrates in an empirical manner both the possibilities and limits of gaining more insights from user queries extracted from a Web Analytics tool and analysed with the help of information extraction tools and knowledge bases. Methods and tools used are generalisable and can be reused by other collection holders. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.DL
007       name.Anne Chardonnens
007       name.Ettore Rizza
007       name.Mathias Coeckelbergs
007       name.Seth van Hooland
040       2017.09.22 14:35
050       http://arxiv.org/abs/1709.07782
*** 9968
010       A Literature Based Approach to Define the Scope of Biomedical   Ontologies: A Case Study on a Rehabilitation Therapy Ontology
021       In this article, we investigate our early attempts at building an ontology describing rehabilitation therapies following brain injury. 
020       These therapies are wide-ranging, involving interventions of many different kinds. As a result, these therapies are hard to describe. As well as restricting actual practice, this is also a major impediment to evidence-based medicine as it is hard to meaningfully compare two treatment plans. <br />Ontology development requires significant effort from both ontologists and domain experts. Knowledge elicited from domain experts forms the scope of the ontology. The process of knowledge elicitation is expensive, consumes experts' time and might have biases depending on the selection of the experts. Various methodologies and techniques exist for enabling this knowledge elicitation, including community groups and open development practices. A related problem is that of defining scope. By defining the scope, we can decide whether a concept (i.e. term) should be represented in the ontology. This is the opposite of knowledge elicitation, in the sense that it defines what should not be in the ontology. This can be addressed by pre-defining a set of competency questions. <br />These approaches are, however, expensive and time-consuming. Here, we describe our work toward an alternative approach, bootstrapping the ontology from an initially small corpus of literature that will define the scope of the ontology, expanding this to a set covering the domain, then using information extraction to define an initial terminology to provide the basis and the competencies for the ontology. Here, we discuss four approaches to building a suitable corpus that is both sufficiently covering and precise. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.DL
007       name.Mohammad K. Halawani
007       name.Rob Forsyth
007       name.Phillip Lord
040       2017.09.27 11:11
050       http://arxiv.org/abs/1709.09450
*** 2130
010       Named Entity Recognition in Twitter using Images and Text
021       Named Entity Recognition (NER) is an important subtask of information extraction that seeks to locate and recognise named entities. 
020       Despite recent achievements, we still face limitations with correctly detecting and classifying entities, prominently in short and noisy text, such as Twitter. An important negative aspect in most of NER approaches is the high dependency on hand-crafted features and domain-specific knowledge, necessary to achieve state-of-the-art results. Thus, devising models to deal with such linguistically complex contexts is still challenging. In this paper, we propose a novel multi-level architecture that does not rely on any specific linguistic resource or encoded rule. Unlike traditional approaches, we use features extracted from images and text to classify named entities. Experimental tests against state-of-the-art NER for Twitter on the Ritter dataset present competitive results (0.59 F-measure), indicating that this approach may lead towards better NER models. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       name.Diego Esteves
007       name.Rafael Peres
007       name.Jens Lehmann
007       name.Giulio Napolitano
040       2017.10.30 15:56
050       http://arxiv.org/abs/1710.11027
*** 3407
010       Building a Web-Scale Dependency-Parsed Corpus from CommonCrawl
021       We present DepCC, the largest to date linguistically analyzed corpus in English including 365 million documents, composed of 252 billion tokens and 7.5 billion of named entity occurrences in 14.3 billion sentences from a web-scale crawl of the CommonCrawl project. 
020       The sentences are processed with a dependency parser and with a named entity tagger and contain provenance information, enabling various applications ranging from training syntax-based word embeddings based on to open information extraction and question answering. We demonstrate the utility of this corpus on the verb similarity task by showing that a distributional model trained on our corpus yields better results than models trained on smaller corpora, like Wikipedia. This distributional model outperforms the state of art models of verb similarity trained on smaller corpora on the SimVerb3500 dataset. 
007       rubr.cs.CL
007       name.Alexander Panchenko
007       name.Eugen Ruppert
007       name.Stefano Faralli
007       name.Simone Paolo Ponzetto
007       name.Chris Biemann
040       2017.10.04 19:42
050       http://arxiv.org/abs/1710.01779
*** 487
010       Tensor Valued Common and Individual Feature Extraction:   Multi-dimensional Perspective
021       A novel method for common and individual feature analysis from exceedingly large-scale data is proposed, in order to ensure the tractability of both the computation and storage and thus mitigate the curse of dimensionality, a major bottleneck in modern data science. 
020       This is achieved by making use of the inherent redundancy in so-called multi-block data structures, which represent multiple observations of the same phenomenon taken at different times, angles or recording conditions. Upon providing an intrinsic link between the properties of the outer vector product and extracted features in tensor decompositions (TDs), the proposed common and individual information extraction from multi-block data is performed through imposing physical meaning to otherwise unconstrained factorisation approaches. This is shown to dramatically reduce the dimensionality of search spaces for subsequent classification procedures and to yield greatly enhanced accuracy. Simulations on a multi-class classification task of large-scale extraction of individual features from a collection of partially related real-world images demonstrate the advantages of the "blessing of dimensionality" associated with TDs. 
007       rubr.eess.SP
007       rubr.eess
007       rubr.stat
007       rubr.stat.ML
007       name.Ilia Kisil
007       name.Giuseppe G. Calvi
007       name.Danilo P. Mandic
040       2017.11.01 18:03
050       http://arxiv.org/abs/1711.00487
*** 529
010       Text Annotation Graphs: Annotating Complex Natural Language Phenomena
021       This paper introduces a new web-based software tool for annotating text, Text Annotation Graphs, or TAG. 
020       It provides functionality for representing complex relationships between words and word phrases that are not available in other software tools, including the ability to define and visualize relationships between the relationships themselves (semantic hypergraphs). Additionally, we include an approach to representing text annotations in which annotation subgraphs, or semantic summaries, are used to show relationships outside of the sequential context of the text itself. Users can use these subgraphs to quickly find similar structures within the current document or external annotated documents. Initially, TAG was developed to support information extraction tasks on a large database of biomedical articles. However, our software is flexible enough to support a wide range of annotation tasks for any domain. Examples are provided that showcase TAG's capabilities on morphological parsing and event extraction tasks. 
007       rubr.cs.CL
007       name.Angus G. Forbes
007       name.Kristine Lee
007       name.Gus Hahn-Powell
007       name.Marco A. Valenzuela-Esc&#xe1;rcega
007       name.Mihai Surdeanu
040       2017.11.01 20:24
050       http://arxiv.org/abs/1711.00529
*** 551
010       Constructive summation of the (2,2) quasi normal mode from a population   of black holes
021       The quasi normal modes (QNMs) associated with gravitational-wave signals from binary black hole (BBH) mergers can provide deep insight into the remnant's properties. 
020       Once design sensitivity is achieved, present ground-based gravitational wave interferometers could detect potentially hundreds of BBH signals in the coming years. For most, the ringdown phase will have a very weak signal-to-noise ratio (SNR). Signal summation techniques allow information extraction from the weak SNR ringdowns. <br />We propose a method to constructively sum the (2,2) QNM from different BBH signals by synchronizing and rescaling them. The parameter space adopted to test the method is presently limited to mass ratio $q\leq3$, initially non-spinning black holes with face-on orientation. Moreover, since the synchronisation procedure fails for the weakest signals, we select all ringdowns with SNR above 2.6. Under these conditions, we show that for different BBH populations, 40 to 70% of all the potential detections could be used for the summation while still ensuring a summed SNR of $\sim$80% of the maximal achievable SNR (i.e. for ideally synchronized signals). 
007       rubr.gr-qc
007       name.C. F. Da Silva Costa
007       name.S. Tiwari
007       name.S. Klimenko
007       name.F. Salemi
040       2017.11.07 22:50
050       http://arxiv.org/abs/1711.00551
*** 2230
010       Multimodal Attribute Extraction
021       The broad goal of information extraction is to derive structured information from unstructured data. 
020       However, most existing methods focus solely on text, ignoring other types of unstructured data such as images, video and audio which comprise an increasing portion of the information on the web. To address this shortcoming, we propose the task of multimodal attribute extraction. Given a collection of unstructured and semi-structured contextual information about an entity (such as a textual description, or visual depictions) the task is to extract the entity's underlying attributes. In this paper, we provide a dataset containing mixed-media data for over 2 million product items along with 7 million attribute-value pairs describing the items which can be used to train attribute extractors in a weakly supervised manner. We provide a variety of baselines which demonstrate the relative effectiveness of the individual modes of information towards solving the task, as well as study human performance. 
007       rubr.cs.CL
007       name.Robert L. Logan IV
007       name.Samuel Humeau
007       name.Sameer Singh
040       2017.11.29 21:40
050       http://arxiv.org/abs/1711.11118
*** 5634
010       Efficient Representation for Natural Language Processing via Kernelized   Hashcodes
021       Kernel methods have been used widely in a number of tasks, but have had limited success in Natural Language Processing (NLP) due to high cost of computing kernel similarities between discrete natural language structures. 
020       A recently proposed technique, Kernelized Locality Sensitive Hashing (KLSH), can significantly reduce the computational cost, but is only applicable to classifiers operating on kNN graphs. Here we propose to use random subspaces of KLSH codes for efficiently constructing explicit representation of natural language structures suitable for general classification methods. Further, we propose an approach for optimizing a KLSH model for classification problems, by maximizing a variational lower bound on the mutual information between the KLSH codes (feature vectors) and the class labels. We apply the proposed approach to a biomedical information extraction task, and observe robust improvements in accuracy, along with significant speedup compared to conventional kernel methods. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.IR
007       rubr.cs.LG
007       name.Sahil Garg
007       name.Aram Galstyan
007       name.Greg Ver Steeg
007       name.Irina Rish
007       name.Guillermo Cecchi
007       name.Shuyang Gao
040       2018.02.01 23:10
050       http://arxiv.org/abs/1711.04044
*** 6024
010       Faithful to the Original: Fact Aware Neural Abstractive Summarization
021       Unlike extractive summarization, abstractive summarization has to fuse different parts of the source text, which inclines to create fake facts. 
020       Our preliminary study reveals nearly 30% of the outputs from a state-of-the-art neural summarization system suffer from this problem. While previous abstractive summarization approaches usually focus on the improvement of informativeness, we argue that faithfulness is also a vital prerequisite for a practical abstractive summarization system. To avoid generating fake facts in a summary, we leverage open information extraction and dependency parse technologies to extract actual fact descriptions from the source text. The dual-attention sequence-to-sequence framework is then proposed to force the generation conditioned on both the source text and the extracted fact descriptions. Experiments on the Gigaword benchmark dataset demonstrate that our model can greatly reduce fake summaries by 80%. Notably, the fact descriptions also bring significant improvement on informativeness since they often condense the meaning of the source text. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       name.Ziqiang Cao
007       name.Furu Wei
007       name.Wenjie Li
007       name.Sujian Li
040       2017.11.13 06:34
050       http://arxiv.org/abs/1711.04434
*** 7490
010       Using Noisy Extractions to Discover Causal Knowledge
021       Knowledge bases (KB) constructed through information extraction from text play an important role in query answering and reasoning. 
020       In this work, we study a particular reasoning task, the problem of discovering causal relationships between entities, known as causal discovery. There are two contrasting types of approaches to discovering causal knowledge. One approach attempts to identify causal relationships from text using automatic extraction techniques, while the other approach infers causation from observational data. However, extractions alone are often insufficient to capture complex patterns and full observational data is expensive to obtain. We introduce a probabilistic method for fusing noisy extractions with observational data to discover causal knowledge. We propose a principled approach that uses the probabilistic soft logic (PSL) framework to encode well-studied constraints to recover long-range patterns and consistent predictions, while cheaply acquired extractions provide a proxy for unseen observations. We apply our method gene regulatory networks and show the promise of exploiting KB signals in causal discovery, suggesting a critical, new area of research. 
007       rubr.cs.AI
007       name.Dhanya Sridhar
007       name.Jay Pujara
007       name.Lise Getoor
040       2017.11.16 02:57
050       http://arxiv.org/abs/1711.05900
*** 988
010       End-to-End Relation Extraction using Markov Logic Networks
021       The task of end-to-end relation extraction consists of two sub-tasks: i) identifying entity mentions along with their types and ii) recognizing semantic relations among the entity mention pairs. 
020       %Identifying entity mentions along with their types and recognizing semantic relations among the entity mentions, are two very important problems in Information Extraction. It has been shown that for better performance, it is necessary to address these two sub-tasks jointly. We propose an approach for simultaneous extraction of entity mentions and relations in a sentence, by using inference in Markov Logic Networks (MLN). We learn three different classifiers : i) local entity classifier, ii) local relation classifier and iii) "pipeline" relation classifier which uses predictions of the local entity classifier. Predictions of these classifiers may be inconsistent with each other. We represent these predictions along with some domain knowledge using weighted first-order logic rules in an MLN and perform joint inference over the MLN to obtain a global output with minimum inconsistencies. Experiments on the ACE (Automatic Content Extraction) 2004 dataset demonstrate that our approach of joint extraction using MLNs outperforms the baselines of individual classifiers. Our end-to-end relation extraction performance is better than 2 out of 3 previous results reported on the ACE 2004 dataset. 
007       rubr.cs.AI
007       name.Sachin Pawar
007       name.Pushpak Bhattacharya
007       name.Girish K. Palshikar
040       2017.12.04 10:26
050       http://arxiv.org/abs/1712.00988
*** 1546
010       An Encoder-Decoder Model for ICD-10 Coding of Death Certificates
021       Information extraction from textual documents such as hospital records and healthrelated user discussions has become a topic of intense interest. 
020       The task of medical concept coding is to map a variable length text to medical concepts and corresponding classification codes in some external system or ontology. In this work, we utilize recurrent neural networks to automatically assign ICD-10 codes to fragments of death certificates written in English. We develop end-to-end neural architectures directly tailored to the task, including basic encoder-decoder architecture for statistical translation. In order to incorporate prior knowledge, we concatenate cosine similarities vector among the text and dictionary entry to the encoded state. Being applied to a standard benchmark from CLEF eHealth 2017 challenge, our model achieved F-measure of 85.01% on a full test set with significant improvement as compared to the average score of 62.2% for all official participants approaches. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.CY
007       name.Elena Tutubalina
007       name.Zulfat Miftahutdinov
040       2017.12.04 17:39
050       http://arxiv.org/abs/1712.01213
*** 5523
010       Relation Extraction : A Survey
021       With the advent of the Internet, large amount of digital text is generated everyday in the form of news articles, research publications, blogs, question answering forums and social media. 
020       It is important to develop techniques for extracting information automatically from these documents, as lot of important information is hidden within them. This extracted information can be used to improve access and management of knowledge hidden in large text corpora. Several applications such as Question Answering, Information Retrieval would benefit from this information. Entities like persons and organizations, form the most basic unit of the information. Occurrences of entities in a sentence are often linked through well-defined relations; e.g., occurrences of person and organization in a sentence may be linked through relations such as employed at. The task of Relation Extraction (RE) is to identify such relations automatically. In this paper, we survey several important supervised, semi-supervised and unsupervised RE techniques. We also cover the paradigms of Open Information Extraction (OIE) and Distant Supervision. Finally, we describe some of the recent trends in the RE techniques and possible future research directions. This survey would be useful for three kinds of readers - i) Newcomers in the field who want to quickly learn about RE; ii) Researchers who want to know how the various RE techniques evolved over time and what are possible future research directions and iii) Practitioners who just need to know which RE technique works best in various settings. 
007       rubr.cs.CL
007       rubr.cs
007       rubr.cs.AI
007       rubr.cs.IR
007       name.Sachin Pawar
007       name.Girish K. Palshikar
007       name.Pushpak Bhattacharyya
040       2017.12.14 12:04
050       http://arxiv.org/abs/1712.05191
*** 6230
010       NegBio: a high-performance tool for negation and uncertainty detection   in radiology reports
021       Negative and uncertain medical findings are frequent in radiology reports, but discriminating them from positive findings remains challenging for information extraction. 
020       Here, we propose a new algorithm, NegBio, to detect negative and uncertain findings in radiology reports. Unlike previous rule-based methods, NegBio utilizes patterns on universal dependencies to identify the scope of triggers that are indicative of negation or uncertainty. We evaluated NegBio on four datasets, including two public benchmarking corpora of radiology reports, a new radiology corpus that we annotated for this work, and a public corpus of general clinical texts. Evaluation on these datasets demonstrates that NegBio is highly accurate for detecting negative and uncertain findings and compares favorably to a widely-used state-of-the-art system NegEx (an average of 9.5% improvement in precision and 5.1% in F1-score). 
007       rubr.cs.CL
007       name.Yifan Peng
007       name.Xiaosong Wang
007       name.Le Lu
007       name.Mohammadhadi Bagheri
007       name.Ronald Summers
007       name.Zhiyong Lu
040       2017.12.27 03:44
050       http://arxiv.org/abs/1712.05898
*** 8530
010       Recursive Programs for Document Spanners
021       A document spanner models a program for Information Extraction (IE) as a function that takes as input a text document (string over a finite alphabet) and produces a relation of spans (intervals in the document) over a predefined schema. 
020       A well studied language for expressing spanners is that of the regular spanners: relational algebra over regex formulas, which are obtained by adding capture variables to regular expressions. Equivalently, the regular spanners are the ones expressible in non-recursive Datalog over regex formulas (extracting relations that play the role of EDBs from the input document). In this paper, we investigate the expressive power of recursive Datalog over regex formulas. Our main result is that such programs capture precisely the document spanners computable in polynomial time. Additional results compare recursive programs to known formalisms such as the language of core spanners (that extends regular spanners by allowing to test for string equality) and its closure under difference. Finally, we extend our main result to a recently proposed framework that generalizes both the relational model and document spanners. 
007       rubr.cs.DB
007       name.Liat Peterfreund
007       name.Balder ten Cate
007       name.Ronald Fagin
007       name.Benny Kimelfeld
040       2017.12.21 20:22
050       http://arxiv.org/abs/1712.08198
*** 1316
010       Text Extraction and Retrieval from Smartphone Screenshots: Building a   Repository for Life in Media
021       Daily engagement in life experiences is increasingly interwoven with mobile device use. 
020       Screen capture at the scale of seconds is being used in behavioral studies and to implement "just-in-time" health interventions. The increasing psychological breadth of digital information will continue to make the actual screens that people view a preferred if not required source of data about life experiences. Effective and efficient Information Extraction and Retrieval from digital screenshots is a crucial prerequisite to successful use of screen data. In this paper, we present the experimental workflow we exploited to: (i) pre-process a unique collection of screen captures, (ii) extract unstructured text embedded in the images, (iii) organize image text and metadata based on a structured schema, (iv) index the resulting document collection, and (v) allow for Image Retrieval through a dedicated vertical search engine application. The adopted procedure integrates different open source libraries for traditional image processing, Optical Character Recognition (OCR), and Image Retrieval. Our aim is to assess whether and how state-of-the-art methodologies can be applied to this novel data set. We show how combining OpenCV-based pre-processing modules with a Long short-term memory (LSTM) based release of Tesseract OCR, without ad hoc training, led to a 74% character-level accuracy of the extracted text. Further, we used the processed repository as baseline for a dedicated Image Retrieval system, for the immediate use and application for behavioral and prevention scientists. We discuss issues of Text Information Extraction and Retrieval that are particular to the screenshot image case and suggest important future work. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CV
007       rubr.cs.DL
007       rubr.cs.MM
007       name.Agnese Chiatti
007       name.Mu Jung Cho
007       name.Anupriya Gagneja
007       name.Xiao Yang
007       name.Miriam Brinberg
007       name.Katie Roehrick
007       name.Sagnik Ray Choudhury
007       name.Nilam Ram
007       name.Byron Reeves
007       name.C. Lee Giles
040       2018.01.04 11:51
050       http://arxiv.org/abs/1801.01316
*** 6613
010       Building an Ellipsis-aware Chinese Dependency Treebank for Web Text
021       Web 2.0 has brought with it numerous user-produced data revealing one's thoughts, experiences, and knowledge, which are a great source for many tasks, such as information extraction, and knowledge base construction. 
020       However, the colloquial nature of the texts poses new challenges for current natural language processing techniques, which are more adapt to the formal form of the language. Ellipsis is a common linguistic phenomenon that some words are left out as they are understood from the context, especially in oral utterance, hindering the improvement of dependency parsing, which is of great importance for tasks relied on the meaning of the sentence. In order to promote research in this area, we are releasing a Chinese dependency treebank of 319 weibos, containing 572 sentences with omissions restored and contexts reserved. 
007       rubr.cs.CL
007       name.Xuancheng Ren
007       name.Xu Sun
007       name.Ji Wen
007       name.Bingzhen Wei
007       name.Weidong Zhan
007       name.Zhiyuan Zhang
040       2018.01.23 04:35
050       http://arxiv.org/abs/1801.06613
*** 7414
010       Assertion-based QA with Question-Aware Open Information Extraction
021       We present assertion based question answering (ABQA), an open domain question answering task that takes a question and a passage as inputs, and outputs a semi-structured assertion consisting of a subject, a predicate and a list of arguments. 
020       An assertion conveys more evidences than a short answer span in reading comprehension, and it is more concise than a tedious passage in passage-based QA. These advantages make ABQA more suitable for human-computer interaction scenarios such as voice-controlled speakers. Further progress towards improving ABQA requires richer supervised dataset and powerful models of text understanding. To remedy this, we introduce a new dataset called WebAssertions, which includes hand-annotated QA labels for 358,427 assertions in 55,960 web passages. To address ABQA, we develop both generative and extractive approaches. The backbone of our generative approach is sequence to sequence learning. In order to capture the structure of the output assertion, we introduce a hierarchical decoder that first generates the structure of the assertion and then generates the words of each field. The extractive approach is based on learning to rank. Features at different levels of granularity are designed to measure the semantic relevance between a question and an assertion. Experimental results show that our approaches have the ability to infer question-aware assertions from a passage. We further evaluate our approaches by incorporating the ABQA results as additional features in passage-based QA. Results on two datasets show that ABQA features significantly improve the accuracy on passage-based~QA. 
007       rubr.cs.CL
007       name.Zhao Yan
007       name.Duyu Tang
007       name.Nan Duan
007       name.Shujie Liu
007       name.Wendi Wang
007       name.Daxin Jiang
007       name.Ming Zhou
007       name.Zhoujun Li
040       2018.01.23 07:22
050       http://arxiv.org/abs/1801.07414
*** 7804
010       Vietnamese Open Information Extraction
021       Open information extraction (OIE) is the process to extract relations and their arguments automatically from textual documents without the need to restrict the search to predefined relations. 
020       In recent years, several OIE systems for the English language have been created but there is not any system for the Vietnamese language. In this paper, we propose a method of OIE for Vietnamese using a clause-based approach. Accordingly, we exploit Vietnamese dependency parsing using grammar clauses that strives to consider all possible relations in a sentence. The corresponding clause types are identified by their propositions as extractable relations based on their grammatical functions of constituents. As a result, our system is the first OIE system named vnOIE for the Vietnamese language that can generate open relations and their arguments from Vietnamese text with highly scalable extraction while being domain independent. Experimental results show that our OIE system achieves promising results with a precision of 83.71%. 
007       rubr.cs.CL
007       name.Diem Truong
007       name.Duc-Thuan Vo
007       name.U.T Nguyen
040       2018.01.23 23:03
050       http://arxiv.org/abs/1801.07804
*** 7875
010       Support Vector Machine Active Learning Algorithms with   Query-by-Committee versus Closest-to-Hyperplane Selection
021       This paper investigates and evaluates support vector machine active learning algorithms for use with imbalanced datasets, which commonly arise in many applications such as information extraction applications. 
020       Algorithms based on closest-to-hyperplane selection and query-by-committee selection are combined with methods for addressing imbalance such as positive amplification based on prevalence statistics from initial random samples. Three algorithms (ClosestPA, QBagPA, and QBoostPA) are presented and carefully evaluated on datasets for text classification and relation extraction. The ClosestPA algorithm is shown to consistently outperform the other two in a variety of ways and insights are provided as to why this is the case. 
007       rubr.cs.LG
007       rubr.cs
007       rubr.cs.CL
007       rubr.cs.CL
007       rubr.cs.IR
007       rubr.stat.ML
007       name.Michael Bloodgood
040       2018.01.24 06:38
050       http://arxiv.org/abs/1801.07875
*** 1174
010       A Method for Discovering and Extracting Author Contributions Information   from Scientific Biomedical Publications
021       Creating scientific publications is a complex process, typically composed of a number of different activities, such as designing the experiments, data preparation, programming software and writing and editing the manuscript. 
020       The information about the contributions of individual authors of a paper is important in the context of assessing authors' scientific achievements. Some publications in biomedical disciplines contain a description of authors' roles in the form of a short section written in natural language, typically entitled "Authors' contributions". In this paper, we present an analysis of roles commonly appearing in the content of these sections, and propose an algorithm for automatic extraction of authors' roles from natural language text in scientific publications. During the first part of the study, we used clustering techniques, as well as Open Information Extraction (OpenIE), to semi-automatically discover the most popular roles within a corpus of 2,000 contributions sections obtained from PubMed Central resources. The roles discovered by our approach include: experimenting (1,743 instances, 17% of the entire role set within the corpus), analysis (1,343, 16%), study design (1,132, 13%), interpretation (879, 10%), conceptualization (865, 10%), paper reading (823, 10%), paper writing (724, 8%), paper review (501, 6%), paper drafting (351, 4%), coordination (319, 4%), data collection (76, 1%), paper review (41, 0.5%) and literature review (41, 0.5%). Discovered roles were then used to automatically build a training set for the supervised role extractor, based on Naive Bayes algorithm. According to the evaluation we performed, the proposed role extraction algorithm is able to extract the roles from the text with precision 0.71, recall 0.49 and F1 0.58. 
007       rubr.cs.DL
007       name.Dominika Tkaczyk
007       name.Andrew Collins
007       name.Joeran Beel
040       2018.02.04 18:55
050       http://arxiv.org/abs/1802.01174
*** 5574
010       Open Information Extraction on Scientific Text: An Evaluation
021       Open Information Extraction (OIE) is the task of the unsupervised creation of structured information from text. 
020       OIE is often used as a starting point for a number of downstream tasks including knowledge base construction, relation extraction, and question answering. While OIE methods are targeted at being domain independent, they have been evaluated primarily on newspaper, encyclopedic or general web text. In this article, we evaluate the performance of OIE on scientific texts originating from 10 different disciplines. To do so, we use two state-of-the-art OIE systems applying a crowd-sourcing approach. We find that OIE systems perform significantly worse on scientific text than encyclopedic text. We also provide an error analysis and suggest areas of work to reduce errors. Our corpus of sentences and judgments are made available. 
007       rubr.cs.CL
007       name.Paul Groth
007       name.Michael Lauruhn
007       name.Antony Scerri
007       name.Ron Daniel Jr
040       2018.02.15 14:38
050       http://arxiv.org/abs/1802.05574
*** 10569
010       Simultaneously Self-Attending to All Mentions for Full-Abstract   Biological Relation Extraction
021       Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. 
020       This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives. 
007       rubr.cs.CL
007       name.Patrick Verga
007       name.Emma Strubell
007       name.Andrew McCallum
040       2018.02.28 18:17
050       http://arxiv.org/abs/1802.10569
*** 5277
010       Constant delay algorithms for regular document spanners
021       Regular expressions and automata models with capture variables are core tools in rule-based information extraction. 
020       These formalisms, also called regular document spanners, use regular languages in order to locate the data that a user wants to extract from a text document, and then store this data into variables. Since document spanners can easily generate large outputs, it is important to have good evaluation algorithms that can generate the extracted data in a quick succession, and with relatively little precomputation time. Towards this goal, we present a practical evaluation algorithm that allows constant delay enumeration of a spanner's output after a precomputation phase that is linear in the document. While the algorithm assumes that the spanner is specified in a syntactic variant of variable set automata, we also study how it can be applied when the spanner is specified by general variable set automata, regex formulas, or spanner algebras. Finally, we study the related problem of counting the number of outputs of a document spanner, providing a fine grained analysis of the classes of document spanners that support efficient enumeration of their results. 
007       rubr.cs.DB
007       rubr.cs
007       rubr.cs.FL
007       name.Fernando Florenzano
007       name.Cristian Riveros
007       name.Martin Ugarte
007       name.Stijn Vansummeren
007       name.Domagoj Vrgoc
040       2018.03.14 13:44
050       http://arxiv.org/abs/1803.05277
*** 5667
010       A Study of Recent Contributions on Information Extraction
021       This paper reports on modern approaches in Information Extraction (IE) and its two main sub-tasks of Named Entity Recognition (NER) and Relation Extraction (RE). 
020       Basic concepts and the most recent approaches in this area are reviewed, which mainly include Machine Learning (ML) based approaches and the more recent trend to Deep Learning (DL) based methods. 
007       rubr.cs.IR
007       rubr.cs
007       rubr.cs.CL
007       name.Parisa Naderi Golshan
007       name.HosseinAli Rahmani Dashti
007       name.Shahrzad Azizi
007       name.Leila Safari
040       2018.03.15 10:04
050       http://arxiv.org/abs/1803.05667
*** 6252
010       Joint Recognition of Handwritten Text and Named Entities with a Neural   End-to-end Model
021       When extracting information from handwritten documents, text transcription and named entity recognition are usually faced as separate subsequent tasks. 
020       This has the disadvantage that errors in the first module affect heavily the performance of the second module. In this work we propose to do both tasks jointly, using a single neural network with a common architecture used for plain text recognition. Experimentally, the work has been tested on a collection of historical marriage records. Results of experiments are presented to show the effect on the performance for different configurations: different ways of encoding the information, doing or not transfer learning and processing at text line or multi-line region level. The results are comparable to state of the art reported in the ICDAR 2017 Information Extraction competition, even though the proposed technique does not use any dictionaries, language modeling or post processing. 
007       rubr.cs.CV
007       rubr.cs
007       rubr.cs.CL
007       name.Manuel Carbonell
007       name.Mauricio Villegas
007       name.Alicia Forn&#xe9;s
007       name.Josep Llad&#xf3;s
040       2018.03.22 12:27
050       http://arxiv.org/abs/1803.06252